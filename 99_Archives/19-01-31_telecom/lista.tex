\documentclass[prez_tpt]{subfiles}
\begin{document}

%------------------------------------------------------------------------
\subsection{Motivations}
%------------------------------------------------------------------------

\begin{frame}[t]{Adaptive Optimization for the $Z$-step}

	We have to solve $N$ problems with a common structure $\pmb D$.\\
	\[
		Z^{[n],*} = \argmin_{Z^{[n]}}\| X^{[n]} - \sum_{k=1}^K\pmb D_k*Z_k^{[n]}\|_2^2
				+ \lambda\| Z^{[n]}\|_1
	\]
	{\textbf{Can we use this structure to accelerate the resolution?}\\[1em]}
	\visible<2->{
		Yes, with the Learned ISTA \citeconf{Gregor10}{NeurIPS}
	}

{
	\only<2>{
		\begin{figure}
		\begin{subfigure}[b]{.47\textwidth}
			\centering
			\inputTikZ{.7}{ista_tikz.tex}
			\mbox{\hskip-2em\inputTikZ{.5}{lista_tikz.tex}}
		\end{subfigure}
		\begin{subfigure}[b]{.47\textwidth}
			\includegraphics[width=.8\textwidth]{Gregor10}
		\end{subfigure}\\
		%\caption*{Adapted from \cite{Gregor10}}
		\end{figure}
	}
	\only<3->{
	\vskip1.5em
	\textbf{Why does it work?} \hfill{\color{gray}(non-convolutional setting)}.\\[.3em]
    \begin{itemize}
        \item \citeconf{xin2016maximal}{NeurIPS}: Improved support recovery.
        \item \citeconf{Giryes2016}{IEEE TSP}: Leverage the sparsity pattern in $Z$.
        \item \citeconf{Chen2018}{NeurIPS}: Linear convergence if $Z$ sparse enough.
        \visible<4>{
        \item \citeconf{Moreau2017}{ICLR}: Leverage the structure of $\pmb D$.
        }
    \end{itemize}
	}
}
\end{frame}


\begin{frame}[t]{Vectorized model}
\begin{columns}
    \column{.5\textwidth}
    \begin{itemize}\itemsep1em
        \item $x$ is a vector in $\Rset^T$
        \item $\epsilon$ is a noise vector in $\Rset^T$  
        \item $D$ is a matrix in $\Rset^{T\times LK}$
        \item $z$ is a coding vector in $\Rset^{\widetilde{T}K}$ 
    \end{itemize}
    \column{.5\textwidth}
    {\bf Sparse Linear model:}
    \begin{align*}
    x & = Dz + \epsilon \phantom{ \sum_{k=1}^K }
    \end{align*}
    \vskip.5em
    with \makebox[1em]{$z$} sparse.\\{\color{gray} Few of its coefficients are non-zero.}
\end{columns}
\vskip1em
\scriptsize\centering
\inputTikZ{.4}{vectorial_dictionary}.\\
\end{frame}

\begin{frame}{Notations}
	Consider the sparse coding problem with a dictionary $D$.\\[.5em]
	\[
		z^{*} = \argmin_{z} F(z) = 
				\underbrace{\frac{1}{2}\| x - D z\|_2^2}_{E(z)}
				+ \lambda\| z\|_1
	\]
	We denote $B = D\tran D$ is the Gram matrix of $D$.\\[1em]
	
	\textbf{We introduce a novel class of algorithms -- FacNet -- based on a sparse factorization of $B$.}
	
	\vskip1em
		Quadratic form: $Q_S(u, v) = \frac{1}{2}(u-v)\tran S (u-v) + \lambda \|u\|_1~.$\\[1em]
		
		Note that $F(z) = Q_B(z, D^\dagger x)$ and prox$^S_{\|\cdot\|_1}(v) = \argmin_u Q_S(u, v)$\\[1em]
		If $S$ is diagonal, $\argmin_u Q_S(u, v)$ can be efficiently minimized as
		the problem is separable in each coordinate.
\end{frame}



%----------------------------------------------------------------
\subsection{Adaptive ISTA: FacNet}
%----------------------------------------------------------------


\begin{frame}[t]{Toward an adaptive procedure \citeconfright{Moreau2017}{ICLR}}
	\vskip1em
	Given an estimate $z^{(q)}$ of $z^*$ at iteration $q$, we can write:
	\begin{eqnarray*}
		F(z) & =& E(z) + \lambda\|z\|_1\\
		     & = & E(z^{(q)}) + \left\langle \nabla E(z^{(q)}) , z-z^{(q)} \right\rangle
						+ Q_{\makebox[.7em][c]{$B$}}(\phantom{\bf A}z, \phantom{\bf A}z^{(q)})~,\\
	\end{eqnarray*}%
	\vskip-1em
	\only<1>{\color{lightblue!70}}%
	\uncover<2->{\underline{ISTA:} ~~~~~Replace $B$ by diagonal matrix
           ${\color{red} S} = \|B\|_2\pmb I_K$ \\[.5em]}
	\uncover<3->{\underline{\bf FacNet:} ~~Replace $B$ by ${\color{blue!75}\pmb A\tran_q \pmb S_q \pmb A_q}$ ~~~(${\color{blue!75}\pmb S_q}$ diagonal, ${\color{blue!75}\pmb A_q}$ unitary)}
	\only<-2>{\color{black}}\\
	\vskip-.5em
	\def\surogate{\alt<2>{F_q(z)}{\widetilde F_q(z)}}
	\begin{eqnarray*}
	\only<2->{
		\surogate{} & = & E(z^{(q)}) + \left\langle \nabla E(z^{(q)}) , z-z^{(q)} \right\rangle
						+ Q_{\alt<3->{\color{blue!75}\pmb}{\color{red}} S\visible<3->{_q}}
							(\visible<3->{{\color{blue!75}\pmb A_q}}z, \visible<3->{{\color{blue!75}\pmb A_q}}z^{(q)})~,\\
			   \min_z \surogate{} &\Leftrightarrow&
                \min_z Q_{\alt<3->{\color{blue} \pmb }{\color{red}}S\visible<3->{_q}}
                    \Big(\visible<3->{{\color{blue!75}\pmb A_q}}z,
                         \visible<3->{{\color{blue!75}\pmb A_q}}z^{(q)}
			   	- \text{\makebox[2.5em][c]{
                           $\alt<3->{\color{blue!75}\pmb}{\color{red}}S^{-1}\alt<2>{}{_q\pmb A_q}$}}
                            \nabla E(z^{(q)})\Big)
	}
	\end{eqnarray*}
	\uncover<4>{
	Can we choose $A_q, S_q$ to accelerate the optimization compared to ISTA?}

	
\end{frame}
\begin{frame}{Toward and adaptive procedure}
		\centering
		\hskip2em
		\makebox[.75\textwidth][c]{
		\only<1>{\includegraphics[height=.75\textheight]{ell1}}%
		\only<2>{\includegraphics[height=.75\textheight]{ell2}}%
		\only<3->{\includegraphics[height=.75\textheight]{ell3}}
		}\\

\end{frame}


\begin{frame}{Toward an adaptive procedure}

	The surogate $\widetilde{F_q}$ can be re-written as
	\[
		\widetilde{F_q}(z) = F(z) + (z-z^{(q)})\tran R(z-z^{(q)}) + \delta_A(z)~.
	\]
%\textbf{Proximal resolution :}
%\[z^{(q+1)} = \argmin_z F(z) = E(z^{(q)}) + \langle B(z^{(q)}-y) , z-z^{(q)} \rangle + \frac{1}{2}\| z - z^{(q)} \|_B^2 + G(z)~ ,\]
%$\color{darkblue}\Rightarrow$ Not computationally efficient but quick minimization.\\[1.5em]


	Tradeoff between:\\[.3em]
	\begin{itemize}\itemsep1em
		\item Diagonalization of the gram matrix $B$~,
		\keypoint{Computation}
		\[ R = A\tran SA - B\]
		\item Deformation of the $\ell_1$-norm with the rotation $A$~.
		\keypoint{Accuracy}
		\[ \delta_A(z) = \lambda\left(\|Az\|_1-\|z\|_1\right) \]
	\end{itemize}
\strongpoint{Trade-off between sparse $A$ and good approximation of $B$.}
\end{frame}

%
%\begin{frame}{One step improvement}
%		Suppose that $R = A\tran S A - B \succ 0$ is positive definite, and define 
%		\[z^{(q+1)} = \arg\min_z \widetilde{F_q}(z)~,\]
%		Then
%		\[
%		\begin{split}
%			F(z^{(q+1)}) - F(z^*) \leq \frac{1}{2}(z^{(q)} - z^*)\tran R (z^{(q)} - z^*)~~~~~\\
%									+ \delta_A(z^*) - \delta_A(z^{(q+1)})~.
%\end{split}
%		\]
%	\vskip1em
%	We are interested in factorization $(A, S)$ for which $\|R\|_2$ and $\delta_A$  are small.
%\end{frame}



\begin{frame}{Theoretical results}
    \begin{itemize}\itemsep1em
		\item We showed that FacNet has the same asymptotic convergence rate as ISTA
		in $\mathcal O(\tfrac{1}{q})$.
		\item The constant factors are different and can be improved.
		If the factorization $(A_q, S_q)$ at iteration $q$  verifies
			\[
				\|R_q\|_2 + 2 \frac{L_{A_q}(z^{(q+1)})}{\|z^*-z^{(q)}\|_2} \le \frac{\|B\|_2}{2}
			\]
			and $A_p = \pmb I_K, S_p = \|B\|_2\pmb I_K$ for $p > q$, then the procedure has
			improved convergence rate compared to ISTA.\\[1em]
			{{\usebeamercolor[fg]{block title}$\Rightarrow$}} There is a phase transition when
			 $\|z^{(q)} - z^*\|_2 \to 0$
	\end{itemize}
\end{frame}



%---------------------------------------------------------
\subsection{Understanding LISTA}
%---------------------------------------------------------

\begin{frame}{Learned ISTA \citeconfright{Gregor10}{NeurIPS}}
	\vskip1em
	{\centering
	 \inputTikZ{.8}{lista_tikz.tex}
	   \vskip1.5em
	   With $W_e = \frac{D\tran}{\|B\|_2} $ and $W_g = I - \frac{B}{\|B\|_2}$,
       this network computes ISTA.\\[1em]}
    \textbf{FacNet:} Specialization of LISTA with
    $
    \begin{cases}
        W_e & = S^{-1}A D\tran\\
        W_g & =  A\tran - S^{-1}A BA\tran
        \end{cases}
    $\\[.5em]
    \hskip3em $\Rightarrow$ LISTA can be at least as good as this model.
	
\end{frame}



%\begin{frame}{Generic Dictionary}
%	\vskip1em
%	\begin{itemize}\itemsep1em
%		\item Generic dictionary uniformly sample in unit ball,
%		\[
%			D \sim \mathcal S^{p-1}~,
%		\]
%		\item Sparse code generated with Bernouilli-Gaussian model, \emph{s.t.}
%		\[z_k = b_ka_k, \hskip2em b_k \sim \mathcal B(\rho) \text{ and } a \sim \mathcal N(0, \sigma \pmb I_K)\]
%	\end{itemize}
%	\vskip1em 
%	\emph{Fixed: } $K = 100$, $P = 64$, $\sigma = 10$ and $\lambda = 0.01$
%\end{frame}

\begin{frame}{Generic Dictionary}
		\centering
		\includegraphics[width=\textwidth]{curve_sparse005_seaborn_min}\\[.5em]
%	\only<2>{
%		\includegraphics[width=\textwidth]{curve_sparse02_seaborn_min}\\
%		$\rho = {}^1/_{4}$.\\[1em]
%	}
    $K$ generic atoms (uniform in $\mathcal S^{p-1}$) with Bernouilli-Gaussian activation.\\
	\emph{Params: } $K = 100$, $P = 64$, $\rho = {}^1/_{20}$, $\sigma = 10$ and $\lambda = 0.01$
\end{frame}


\begin{frame}{Adversarial dictionary}
	%\vskip4em
\centering
\includegraphics[width=\textwidth]{curve_adverse_seaborn_min}\\
    Same parameters with adverse dictionary (dense eigen-spaces).
\end{frame}


\begin{frame}{Recap Part I}

	\textbf{Take home message}\\[.5em]
	\begin{itemize}\itemsep.5em
		\item Non asymptotic acceleration of ISTA is possible
		based on the structure of $\pmb D$,
		\item Sufficient analysis to explain LISTA acceleration,
		\item Empirically showed the structure of $D$ is necessary for LISTA. 
	\end{itemize}

	\vskip1.5em
    \textbf{Ahead of us}\\[.5em]
	\begin{itemize}\itemsep.5em
	    \item Improve the factorization formulation
	    for direct optimization,
	    \item Adaptation of the analysis to convolutional sparse coding,
	    \item Explore the link with sparse eigenvectors of the gram matrix.
    \end{itemize}
	
\end{frame}





\biblio{}
\end{document}