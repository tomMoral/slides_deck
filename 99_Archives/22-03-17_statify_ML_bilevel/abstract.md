In recent years, bi-level optimization -- solving an optimization problem that depends on the results of another optimization problem -- has raised much interest in the machine learning community. A core question for such problem is the estimation of the gradient when the inner problem is not solved exactly.  While some fundamental results exist, there is still a gap between what is used in practice and our understanding of the theoretical behavior of such problems.
In this talk, I will review different use cases where this type of problem arise, such as hyper-parameter optimization or dictionary learning. I will also review recent advances on how to solve them efficiently.