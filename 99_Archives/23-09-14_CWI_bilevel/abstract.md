A framework for efficient Bi-level optimization

In recent years, bi-level optimization -- solving an optimization problem that depends on the results of another optimization problem -- has raised much interest in the machine learning community. It can be used in both hyperparameter optimization, automatic data augmentation, implicit deep learning or optimal experimental design. A core question for such problems is the estimation of the gradient when the inner problem is not solved exactly.  While some fundamental results exist, there is still a gap between what is used in practice and our understanding of the theoretical behavior of such problems. In this talk, I will give a view of recent advances on how to solve them efficiently with a framework that allows for easy linear updates.

