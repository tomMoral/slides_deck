\documentclass{beamer}

\usepackage{beamer_tom}
\graphicspath{{./images/}}
\def\TikzLocation{./tikz/}
\def\tkzscl{1}

\def\biblio{
	\nobibliography{../../library}
	\def\biblio{}
}

\institute{INRIA Saclay}
\author{Thomas Moreau}
\title{
	Learning to optimize with\\
    unrolled algorithms
}

\date{
	April 01, 2021
}

\collaborators{
    Pierre Ablin; Mathurin Massias; Alexandre Gramfort;\\
    Hamza Cherkaoui; Jeremias Sulam; Joan Bruna
}

\setbeamertemplate{title page}[frame]
\def\extraLogo{}
\DeclareMathOperator*{\supp}{Supp}

\begin{document}

	\begin{frame}
		\titlepage
		\biblio{}
	\end{frame}

\begin{frame}{Electrophysiology}
\begin{columns}[c]
    \column{.5\textwidth}
    \textbf{Magnetoencephalography}\\
    \includegraphics[width=.8\linewidth]{meg_presentation}
    \column{.5\textwidth}
    \textbf{Electroencephalography}\\
    \includegraphics[width=.8\linewidth]{eeg_presentation}
\end{columns}
\end{frame}

	\begin{frame}{Inverse problems}

        \definecolor{Z}{RGB}{45,162,65}
        \definecolor{D}{RGB}{180,35,35}
        \def\varX{{\color{linkcolor} \pmb x}}
        \def\varZ{{\color{Z} \pmb z}}
        \def\varD{{\color{D} \pmb D}}
        {\centering
        \begin{tikzpicture}
        \tikzset{
            %Define standard arrow tip
            >=stealth',
            %Define style for boxes
            varstyle/.style={
                rectangle,
                rounded corners,
                draw=black,
                text width=1em,
                minimum height=1.5em,
                text centered},
        }
        \node (meg) {
            \alt<3->{%
                \includegraphics[width=12em]{meg_localised_source}
            }{%
                \includegraphics[width=12em]{meg_source}
            }};
        \draw[->, thick] ($(meg.east) - (0, 1.5em)$) -- ++(8em, 0)
        node[midway, align=center] (maxwell) {Maxwell's\\Equations}
        node[right] (topomap) {\includegraphics[width=6em]{topomap_somato}};
        \node[varstyle, below=.5em] at (topomap.south) (varX) {$\varX$};
        \node[below=0em of varX.south] {\bf Observed signal};
%        \draw[->, thick] (topomap.east) -- ++(5em, 0)
%        node[midway, align=center] {\small Probl√®me\\ \small inverse}
        \node[varstyle, below=.2em of meg]
            (varZ) {$\varZ$};
            \node[below=0em of varZ.south] {\bf Electrical activity};
        \node[varstyle, below=3.8em of maxwell.center]
            (varD) {$\varD$};

        \uncover<2->{
             \draw[<-, thick] (meg.east) to[bend left, looseness=1]
                node [midway, above] {Inverse Problem}
                ($(topomap.west) + (0, 1.5em)$);
        }
        \end{tikzpicture}}
        \vskip0em
        {\bf Forward model: }$\varX{} = \varD\varZ$
        \hskip3em
        \uncover<2->{{\bf Inverse problem:} $\varZ = f(\varX)$ (ill-posed)}\\[1em]
        \uncover<3->{\centering
               Optimization with a regularization $\mathcal R$ encoding prior knowledge\\ $\argmin_{\varZ} \|\varX{}
        - \varD{}\varZ{}\|_2^2 + \mathcal R(\varZ{})$\\[1em]
               Example: sparsity with $\mathcal R = \lambda\|\cdot\|_1$}

	\end{frame}

\begin{frame}{Other Inverse Problems}


\begin{columns}[T]

    \column{.5\textwidth} {\bf Ultra sound}\\
    \includegraphics[width=.7\textwidth, trim={6.7em 4em 7em 5em}, clip]{InverseProblem_petroleum}

    \column{.5\textwidth}{\bf fMRI - compress sensing}\\
        \begin{columns}[c]
        \column{.5\textwidth}    \centering
        \includegraphics[width=.8\textwidth]{K_space}\\
        \column{.5\textwidth} \centering
        \includegraphics[width=.8\textwidth]{brain_scan}\\
        \end{columns}
    \end{columns}
    \vskip1em
    {\bf Astrophysic}\\
    \includegraphics[width=\textwidth]{weak_lensing}

\end{frame}

\begin{frame}
    \frametitle{Classical Sparse IP resolution - the Lasso \mycite{Tibshirani1996}}


    Given a forward operator $D \in \Rset^{n\times m}$ and $\lambda > 0$, the Lasso for $x\in \Rset^n$ is \\[.5em]
    \[
    z^{*} = \argmin_{z} F_x(z) =
    \underbrace{\frac{1}{2}\| x - D z\|_2^2}_{f_x(z)}
    + \lambda\| z\|_1
    \]
    \emph{a.k.a.} sparse coding, sparse linear regression, ...\\[1em]
    We are interested in the over-complete case where $m > n$.\\[2em]
    \uncover<2>{\centering
    \begin{block}{\bf Some Properties}
        \begin{itemize}\color{black}\itemsep.5em
            \item The problem is convex in $z$ but not strongly convex in general.
            \item The problem is L-smooth and proximable.
            \item Most of the time, there is a unique solution (but not always).
            % \item $z=0$ is solution if and only if
            %     $\lambda \ge \lambda_{\max} \doteq \|D^\top x\|_\infty$
        \end{itemize}
    \end{block}}
\end{frame}

\begin{frame}
    \frametitle{Solving the Lasso -- classical optimization}

    {\centering
    \begin{block}{\bf Classical Optimizationd}
        \begin{itemize}\color{black}\itemsep.5em
            \item (Fast-) Iterative Shrinkage-Thresholding Algorithm (ISTA).\\ \mycite{Daubechies2004,Beck2009}
            \item Coordinate Descent.\mycite{Friedman2007,Osher2009}
            \item Least-Angle Regression (LARS).\mycite{Efron2004}
        \end{itemize}
    \end{block}}

        \vskip1em
    {\bf Convergence rates -- Worst case analysis:}
    For any $x$,

    \[
        F_x(z^{(t)}) - F_x^* \le \mathcal O\left(\frac{1}{t^2}\right)
    \]
    \vskip1em
    \strongpoint{Guaranteed convergence for any $x$.}

\end{frame}


\begin{frame}
    \frametitle{How to solve efficiently many inverse problems}

    Given multiple inputs $x_i$, we would like to solve efficiently:
    \[
        \min_{z_i} \sum_{i=1}^N F_{x_i}(z_i)
    \]
    \uncover<2->{Here, each problem is independent, so with an infinite budget, there is no point in considering this problem.\\[1em]}
    \uncover<3->{
        However, if your aim is to chose an algorithm $f_L$ for a given computational budget $L$ such that
        \[
            \argmin_{f_L} \frac{1}{N}\sum_{i=1}^N F_{x_i}(f_L(x_i))
        \]
        Can you do better than worst case algorithms?\\[.5em]
    }
    \uncover<4>{
        \emph{Related to average case complexity analysis?}\\ \mycite{Scieur2020,Pedregosa2020}
    }

\end{frame}

\section{Unrolled optimization algorithms}
\parttitleframe{}


\begin{frame}{ISTA: \mycite{Daubechies2004}\\
    Iterative Shrinkage-Thresholding Algorithm\\}
$f_x$ is a L-smooth function with $L = \|D\|_2^2$ and
\[
\nabla f_x(z^{(t)}) = D^\top(D z^{(t)} - x)
\]
The $\ell_1$-norm is proximable with a separable proximal operator
\[
\text{prox}_{\mu\|\cdot\|_1}(x) = \text{sign}(x)\max(0, |x| - \mu)
                              = ST(x, \mu)
\]
\uncover<2>{
We can use the proximal gradient descent algorithm (ISTA)
\[
z^{(t+1)} = \text{ST}\left(z^{(t)}
                         - \rho\underbrace{\nabla f_x(z^{(t)})}_{D^\top(D z^{(t)} - x)},
                         \rho\lambda\right)
\]
Here, $\rho$ play the role of a step size (in $[0, \frac{2}{L}[$).
}
\end{frame}



\frame{
    \frametitle{ISTA is a ``Neural Network'' \mycite{Gregor10}}

    {\bf ISTA}\\[.5em]
    \[
        z^{(t+1)} = \text{ST}\left(z^{(t)} - \rho D^\top(D z^{(t)} - x),
        \rho \lambda\right)
    \]
    \vskip1em
    Let $W_z = I_m - \rho D^\top D$ and $W_x = \rho D^\top$.
    Then
    \[
        z^{(t+1)} = \text{ST}(W_zz^{(t)} + W_xx, \rho \lambda)
    \]
    \alt<2>{
        \begin{columns}
        \column{.3\textwidth}\centering
            RNN equivalent to ISTA
        \column{.7\textwidth}\centering
            \inputTikZ{1}{ista_tikz.tex}\\
        \end{columns}
    }{\centering
        \begin{columns}
        \column{.3\textwidth}\centering
            One step of ISTA
        \column{.7\textwidth}\centering
            \inputTikZ{1}{ista_step_tikz.tex}\\
        \end{columns}
    }

}

\begin{frame}[t]{Learned ISTA \mycite{Gregor10}}
    Recurrence relation of ISTA define a RNN
    \begin{columns}[c]
        \column{.5\textwidth}
        \[
            z^{(t+1)} = \text{ST}\left(z^{(t)} - \frac{1}{L}D^\top(D z^{(t)} - x),
            \frac{\lambda}{L}\right)
        \]
        \column{.5\textwidth}
        {\centering
            \inputTikZ{.8}{ista_tikz.tex}\\}
    \end{columns}

    \vskip1em
    This RNN can be unfolded as a feed-forward network.\\[1em]
    {\centering
        \inputTikZ{.8}{lista_tikz.tex}\\}
    \vskip1em
    Let $\Phi_{\Theta^{(T)}}$ denote a network with $T$ layers parametrized with $\Theta^{(T)}$.\\[1em]

    If $W_x^{(i)} = W_x$ and $W_z^{(i)} = W_z$, then $\Phi_{\Theta^T}(x) = z^{(t)}$.
\end{frame}

\begin{frame}{LISTA -- Training}

    {\bf Empirical risk minimization :}
    We need a training set of $\{x_1, \dots x_N\}$ training sample and our goad is to accelerate ISTA on unseen data $x \sim p$.\\[1em]

    The training solves
    \[
    \tilde{\Theta}^{(T)} \in \arg\min_{\Theta^{(T)}}
        \frac{1}{N}\sum_{i=1}^N \mathcal L_x(\Phi_{\Theta^{(T)}}(x_i))
        \enspace .
    \]
    for a loss $\mathcal L_x~.$\\[2em]

    \strongpoint{Choice of loss $\mathcal L_x$?}
\end{frame}

\begin{frame}{LISTA -- Training}
    \centering
    \setbeamercovered{transparent}
    \setlength{\bodywd}{.9\linewidth}
    \uncover<-3>{\highlight[c=black, wd=\bodywd]{
        \raggedright
        \textcolor{darkred}{\bf Supervised:}
        \normalfont a ground truth $z^*(x)$ is known
        \vskip-.5em
        \[
            \mathcal L_x(z) = \frac{1}{2}\|z - z^*(x)\|
        \]
        \vskip-.2em
        Solving the inverse problem.\\
     }\vskip1em
    \visible<2->{\highlight[c=black, wd=\bodywd]{
         \raggedright
         \textcolor{darkred}{\bf Semi-supervised:}
         \normalfont the solution of the Lasso $z^*(x)$ is known
         \vskip-.5em
         \[
         \mathcal L_x(z) = \frac{1}{2}\|z - z^*(x)\|
         \]
         \vskip-.2em
         Accelerating the resolution of the Lasso.\\
    }}}\vskip1em
\visible<3-4>{
    \highlight[c=black, wd=\bodywd]{
        \raggedright
        \textcolor{darkred}{\bf Unsupervised:}
        \normalfont there is no ground truth
        \vskip-.5em
        \[
        \mathcal L_x(z) = F_x(z) = \frac{1}{2}\|x - Dz\|_2^2 + \lambda \|z\|_1
        \]
        \vskip-.2em
        Solving the Lasso.\\
    }}
\end{frame}


\begin{frame}[t]{LISTA -- Parametrizations}
    \setbeamercovered{transparent}
    \uncover<1>{
        {\bf General LISTA model} \mycite{Gregor10}
        \[
        z^{(t+1)} = \text{ST}\left(\textcolor{linkcolor}{\bf W_e^{(t)}}z^{(t)}
                                 + \textcolor{linkcolor}{\bf W_x^{(t)}}x,
                                   \textcolor{linkcolor}{\bf \theta^{(t)}}\right)
        \]
        The structure of $D$ is lost in the linear transform.\\[2em]
    }

    {\bf Coupled LISTA} \mycite{Chen2018}
    \[
    z^{(t+1)} = \text{ST}\left(z^{(t)} - \textcolor{linkcolor}{\bf\alpha^{(t)} W^{(t)}}(D z^{(t)} -x), \textcolor{linkcolor}{\bf\beta^{(t)}}\right)
    \]
    Can be seen as learning\\[1em]
    \begin{columns}[c]
        \column{.3\textwidth}
        \myitem Pre-conditionner\\
        {\centering $W^{(t)} \in \Rset^{m\times n}$\\}
        \column{.3\textwidth}
        \myitem Step-size\\
        {\centering $\alpha^{(t)} \in \Rset_+$\\}
        \column{.3\textwidth}
        \myitem Threshold\\
        {\centering $\beta^{(t)} \in \Rset_+$\\}
    \end{columns}
    \vskip1.5em
    \visible<2>{\centering {\Large $\Rightarrow$} \large Justified theoretically for (un)supervised  convergence\\}
\end{frame}

\begin{frame}
    \frametitle{Performances}

    {\centering
        \includegraphics[width=.8\textwidth]{comparison_lista}\\
    }
\end{frame}


\section{What is learned in unrolled algorithms?}
\parttitleframe{}


\begin{frame}[t]{Coupled LISTA Parametrizations}


    {\bf Coupled LISTA} \mycite{Chen2018}
    \[
    z^{(t+1)} = \text{ST}\left(z^{(t)} - \textcolor{linkcolor}{\bf\alpha^{(t)} W^{(t)}}(D z^{(t)} -x), \textcolor{linkcolor}{\bf\beta^{(t)}}\right)
    \]
    Can be seen as learning\\[1em]
    \begin{columns}[c]
        \column{.3\textwidth}
        \myitem Pre-conditionner\\
        {\centering $W^{(t)} \in \Rset^{m\times n}$\\}
        \column{.3\textwidth}
        \myitem Step-size\\
        {\centering $\alpha^{(t)} \in \Rset_+$\\}
        \column{.3\textwidth}
        \myitem Threshold\\
        {\centering $\beta^{(t)} \in \Rset_+$\\}
    \end{columns}
\end{frame}


\begin{frame}{What does LISTA learn? \mycite{Ablin2019}}

    {\centering }


    \begin{block}{Theorem -- Asymptotic convergence of the weights}
        Consider a sequence of nested networks $\Phi_{\Theta^{(T)}}$ \emph{s.t.}\\
        $\Phi_{\Theta^{(t)}}(x) = \phi_{\theta^{(t)}}(\Phi_{\Theta^{(t + 1)}}(x), x)~.$
        Assume that
        \begin{enumerate}
            \item\label{hyp:theta_cvg} the sequence of parameters converges \ie{}
            $\theta^{(t)} \xrightarrow[t\to\infty]{} \theta^* = (W^*, \alpha^*, \beta^*) \enspace ,$
            \item\label{hyp:out_cvg} the output of the network converges toward a solution $z^*(x)$ of the Lasso uniformly over the equiregularization set $\mathcal B_\infty~,$ \ie{} $\sup_{x \in \mathcal B_\infty}\|\Phi_{\Theta^{(T)}}(x) - z^*(x)\| \xrightarrow[T \to \infty]{} 0$ \enspace.
        \end{enumerate}
        Then $\frac{\alpha^*}{\beta^*}W^* = D \enspace .$
    \end{block}
    \vskip1em
    \emph{Idea of the proof:} each unit vector needs to be a fixed point of the network.
\end{frame}

\begin{frame}{Numerical verification}
        \centering
    \includegraphics[width=.8\textwidth]{fro_similarity}\\[1em]
    40-layers LISTA network trained on a $10 \times 20$ problem with $\lambda = 0.1$\\
    {\bf The weights $W^{(t)}$ align with $D$ and $\alpha, \beta$ get coupled.}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Understanding LISTA
%

\section{Is there a point in learning step sizes?}
\parttitleframe{}


\begin{frame}{Step LISTA \mycite{Ablin2019}}

    {\bf LISTA with restricted parametrization :}
    Only learn a step-size $\alpha^{(t)}$\\[1em]
    \[
    z^{(t+1)} = \text{ST}\left(z^{(t)}
        - \textcolor{linkcolor}{\bf\alpha^{(t)}} D^\top(D z^{(t)} -x),
        \lambda\textcolor{linkcolor}{\bf\alpha^{(t)}}\right)
    \]
    \vskip1em
    \underline{Fewer parameters:}
    $T$ instead of $( 2 + mn)T~.$\\[1em]
    \begin{columns}[c]
    \column{.5\textwidth}
    \centering $\Rightarrow$ Easier to learn\\
    \column{.5\textwidth}
    \centering $\Rightarrow$ Reduced performances?\\
    \end{columns}
    \vskip2em
    \underline{Goal:} Learn step sizes for ISTA adapted to input distribution.\\[1em]

\end{frame}



\begin{frame}[t]{Performances}

    {\bf Simulated data:} $m=256$ and $n=64$\\[.7em]
    \hskip6ex $D_k \sim \mathcal U(\mathcal S^{n-1})$ and $x = \frac{\widetilde x}{\|D^\top \widetilde x\|_\infty}$ with $\widetilde x_i \sim \mathcal N(0, 1)$\\[1.5em]
    {\centering
    \includegraphics[width=\textwidth]{comparison_networks_simu}\\}
\end{frame}

\begin{frame}{Performance on semi-real datasets}

    {\bf Digits:} $8\times 8$ images from \texttt{scikit-learn}\\[.7em]
    \hskip6ex $D_k $ and $\widetilde x$ sampled uniformly from the digits and $x = \frac{\widetilde x}{\|D^\top \widetilde x\|_\infty}$.\\[1.5em]
    \includegraphics[width=\textwidth]{comparison_networks_images}

\end{frame}

\begin{frame}[t]{ISTA: Majoration-Minimization}
    Taylor expansion of $f_x$ in $z^{(t)}$
    \begin{align*}
        F_x(z) &  = f_x(z^{(t)}) + \nabla f_x(z^{(t)})^\top(z - z^{(t)})
                    + \frac{1}{2}\|D(z-z^{(t)})\|_2^2+ \lambda\|z\|_1\\
               & \le f_x(z^{(t)}) + \nabla f_x(z^{(t)})^\top(z - z^{(t)}) + \frac{L}{2}\|z - z^{(t)}\|_2^2 + \lambda\|z\|_1
    \end{align*}
    $\Rightarrow$ Replace the Hessian $D^\top D$ by $L \textbf{ Id}$.\\[2em]

    Separable function that can be minimized in close form
    \begin{align*}
        \argmin_z \frac{L}{2}\left\|z^{(t)} - \frac{1}{L}\nabla f_x(z^{(t)}) - z\right\|_2^2 + \lambda\|z\|_1
        & = \text{prox}_{\frac{\lambda}{L}}\left(z^{(t)} - \frac{1}{L}\nabla f_x(z^{(t)})\right)\\
        & = \text{ST}\left(z^{(t)} - \frac{1}{L}\nabla f_x(z^{(t)}),
                           \frac{\lambda}{L}\right)
    \end{align*}
\end{frame}

\begin{frame}{ISTA: Majoration for the data-fit}
    \definecolor{darkgreen}{RGB}{0, 148, 0}
    \myitem{} Level sets from $z^\top D^\top D z
                       \only<2>{\le \color{red} L \|z\|_2}
                       \only<3>{\le \color{blue} z^\top A^\top \Lambda A z}
                       \only<4>{\le {\color{darkgreen} L_S \|z\|_2}
                            ~~ \text{ for } Supp(z) \subset S}$
              \only<3>{\mycite{Moreau2017}}\\
    \centering
    \makebox[.75\textwidth][c]{
        \only<1>{\includegraphics[height=.75\textheight]{ell1}}%
        \only<2>{\includegraphics[height=.75\textheight]{ell2}}%
        \only<3>{\includegraphics[height=.75\textheight]{ell3}}%
        \only<4>{\includegraphics[height=.75\textheight]{ell4}}
    }\\
\end{frame}


\begin{frame}[t]{Oracle ISTA: Majoration-Minimization}
    For all $z$ such that $\supp(z) \subset S \doteq \supp(z^{(t)})$,
    \begin{align*}
    F_x(z) & \le f_x(z^{(t)}) + \nabla f_x(z^{(t)})^\top(z - z^{(t)})
           + \frac{\color{linkcolor} L_S}{2}\|z - z^{(t)}\|_2^2 + \lambda\|z\|_1
    \end{align*}
    with $L_S = \|D_{\cdot,S}\|_2^2$.\\[1em]

\centering
\includegraphics[width=.6\textwidth]{surrogate}\\

\end{frame}

\begin{frame}{Better step-sizes for ISTA}

    {\bf \large Oracle ISTA (OISTA):\\[1em]}
    \begin{enumerate}\itemsep1em
        \item Get the Lipschitz constant $L_S$ associated with support $S = \supp(z^{(t)})$.
        \item Compute $y^{(t+1)}$ as a step of ISTA with a step-size of $1/L_S$
        \[
        y^{(t+1)} = \text{ST}\left(z^{(t)}
        - \frac{1}{L_S}D^\top(D z^{(t)} - x),
        \frac{\lambda}{L_S}\right)
        \]
        \item If $\supp(y^{t+1}) \subset S$, accept the update $z^{(t+1)} = y^{(t+1)}$.
        \item Else, $z^{(t+1)}$ is computed with step size $1/L$.
    \end{enumerate}
\end{frame}

\begin{frame}{OISTA: Performances}
    \centering
\includegraphics[width=\textwidth, trim={0 8.5em 0 0}, clip]{comparison_oista_ista}\\
\large \hskip4em Number of iterations
\end{frame}

\begin{frame}{OISTA -- Step-size}
    \centering
    \includegraphics[width=\textwidth]{comparison_oista_ista_steps}\\
\end{frame}


% \begin{frame}{OISTA -- Improved-convergence rates}
%     $S^* = Supp(Z^*)$\\[.5em]
%     $\mu^* = \min \|Dz\|_2^2$ for $\|z\|_2 = 1$
%     and $Supp(z)\subset S^*$.\\[2em]

%     If $\mu^* > 0$, OISTA converges with a linear rate
%     \[
%         F_x(z^{(t)}) - F_x(z^*) \leq
%             (1 - \tfrac{\mu^*}{L_{S^*}})^{t - T^*}(F_x(z^{(T^*)}) - F_x(z^*))\enspace .
%     \]
% \end{frame}

% \begin{frame}{OISTA -- Gaussian setting}
%     \begin{block}{Acceleration quantification with Marchenko-Pastur}
%         Entries in $D \in \Rset^{n \times m}$ are sampled from $\mathcal N(0, 1)$ and $S$ is sampled uniformly with $|S| = k$.
%         %
%         Denote $m/n \rightarrow \gamma, \enspace k /m \rightarrow \zeta \enspace ,$ with $k, m, n \rightarrow +\infty~.$ Then
%         %
%         \begin{equation}
%         \label{eq:ratioL}
%         \frac{L_S}{L} \rightarrow \left(\frac{1 + \sqrt{\zeta\gamma}}{1 + \sqrt{\gamma}} \right)^2 \enspace.
%         \end{equation}
%     \end{block}
%     \begin{columns}
%         \column{.55\textwidth}
%         {\centering\includegraphics[width=\textwidth]{lip_distrib}\\}
%         \column{.25\textwidth}
%         \underline{Empirical law}\\[.5em]
%         $n=200, ~~ m = 600$
%     \end{columns}

% \end{frame}

\begin{frame}{OISTA -- Limitation}

\begin{itemize}
    \item OISTA is not practical, as you need to compute $L_S$ at each iteration and this is costly.\\[1em]
    \item No precomputation possible: there is an exponential number of supports $S$.
\end{itemize}

\end{frame}


\begin{frame}{Link with SLISTA}
    \centering
    \includegraphics[width=.8\textwidth]{learned_steps}\\
    The learned step-sizes are linked to the distribution of $1/L_S$\\[2em]

\end{frame}


\begin{frame}{My take on unrolled algorithms}

\uncover<1->{
        {\bf What unrolled algorithms \emph{can} do:}\\[1em]
        \begin{itemize}
            \item Improve constants in convergence rate.\\\mycite{Moreau2017}
            \item Learn to better optimize for a non-uniform input distribution.\\\mycite{Ablin2019}
            \item Make inverse problem solution differentiable.\\
                \mycite{Ablin2020,Mehmood2020}
        \end{itemize}
}
\vskip1em
\uncover<2->{
    {\bf What unrolled algorithms \emph{can't} do:}\\[1em]
    \begin{itemize}
        \item Faster convergence rates for solvers.
        \item Uniform convergence with modified structure.
    \end{itemize}
}
\uncover<3->{
    \strongpoint{Can we extend these results to other problems?}
}
\end{frame}

\begin{frame}{Conclusion}

    \vskip1em
    \underline{Take home messages:}\\[.5em]
     {\centering\bf First order structure is needed in optimization.\\
       No hope to learn an algorithm better than ISTA.\\
       {\hspace{0pt plus 1 filll}\normalfont\footnotesize (except for step-sizes!)}}\\[1em]
     {\centering\bf Unrolled algorithms are useful to learn to solve optimization problems in average.\\
     {\hspace{0pt plus 1 filll}\normalfont\footnotesize (typical in bi-level optimization?)}}

    \vspace{0pt plus 1 filll}
    Code to reproduce the figures is available online:\\[.5em]

    \includegraphics[height=.8em]{github}~\textbf{adopty} : github.com/tommoral/adopty\\[.5em]
    \includegraphics[height=.8em]{github}~\textbf{carpet} : github.com/hcherkaoui/carpet\\[1em]

    Slides will be on my web page:\\[.5em]
    \hskip5em\includegraphics[height=.8em]{website} tommoral.github.io
    \hskip4em \includegraphics[height=.8em]{twitter} @tomamoral

\end{frame}


\appendix

\begin{frame}[t]{Interlude -- regularization $\lambda$}
    Importance of the parameter $\lambda$
    \[
    \mathcal L_x(z) = \frac{1}{2}\|x - Dz\|_2^2 + \lambda \|z\|_1
    \]
   \[
   z^{(t+1)} = \text{ST}\left(z^{(t)}
   - \textcolor{linkcolor}{\bf\alpha^{(t)}} D^\top(D z^{(t)} -x),
   \lambda\textcolor{linkcolor}{\bf\alpha^{(t)}}\right)
   \]
   Control the distribution of $z^*(x)$ sparsity.\\[.5em]
    \begin{columns}[c]
        \column{.45\textwidth}
            \begin{block}{Maximal value}
                $\lambda_{\max} = \|D^\top x\|_\infty$ is the minimal value of $\lambda$ for which
                \[
                    z^*(x) = 0
                \]
            \end{block}
        \column{.45\textwidth}
        \begin{block}{Equiregularization set}
            Set in $\Rset^n$ for which $\lambda_{\max} = 1$
            \[
                \mathcal B_\infty = \{ x \in \Rset^n ~;~~ \|D^\top x\|_\infty =1\}
            \]
            \vskip1em
        \end{block}
    \end{columns}

    \strongpoint{Training performed with points sampled in $\mathcal B_\infty$}

\end{frame}


\section{TV regularized problems}
\parttitleframe{}

\begin{frame}
    \frametitle{TV regularized problems}


    Given a forward operator $D \in \Rset^{n\times m}$ and $\lambda > 0$, the Lasso for $x\in \Rset^n$ is \\[.5em]
    \[
    z^{*} = \argmin_{z} P_x(z) =
    \underbrace{\frac{1}{2}\| x - D z\|_2^2}_{f_x(z)}
    + \lambda\| z\|_{TV}
    \]
    where $\|z\|_{TV} = \|\nabla z\|_1$, and $\nabla = $
    \scalebox{.5}{
    $
    \begin{bmatrix}
    -1      & 1      & 0      & \dots  & 0      \\
    0      & -1     & 1      & \ddots & \vdots \\
    \vdots & \ddots & \ddots & \ddots & 0      \\
    0      & \dots  & 0      & -1     & 1      \\
    \end{bmatrix}
    $
    } $\in \Rset^{k-1\times k}$ \\[2em]

    {\bf Why consider this?} equivalent formulation with Lasso:
    \[
        \min_{u \in \Rset^{k}} S_x(u) = \frac 12 \|x - D L u\|_2^2 + \lambda\|R u\|_1.
    \]
    where $R$ is diagonal and $L$ is the discrete integration operator.\\[1em]

    \strongpoint{}

\end{frame}

\begin{frame}
    \frametitle{Why not just use the synthesis formulation? \mycite{Cherkaoui2020}}


    \textbf{Convergence rate comparison} \\[.5em]

    Both cvg rates are in $\mathcal O(1/t)$ but scale with $\rho = \|D\|_2^2$ or $\widetilde{\rho} = \|D\nabla\|_2^2$.\\[1.5em]

    \begin{theorem}[Lower bound for the ratio $\frac{\widetilde \rho}{\rho}$ expectation]
        Let $D$ be a random matrix in $\Rset^{m\times k}$ with \emph{iid} normal entries. The expectation of $\widetilde \rho/ \rho$ is asymptotically lower bounded when $k$ tends to $\infty$ by
        \[
            \mathbb E\left[\frac{\widetilde \rho}{\rho}\right] \ge
                \frac{2k + 1}{4\pi^2} + o(1)
        \]
    \end{theorem}

    Empirical evidences also push for a $\mathcal O(k^2)$ scaling.\\[2em]

    Analysis is more efficient in terms of iterations than Synthesis.

\end{frame}



\begin{frame}
    \frametitle{Unrolling iterative algorithms \mycite{Cherkaoui2020}}

    \begin{figure}[tp!]
        \centering
        \scalebox{.7}{\subfile{tikz/tv_tikz}}
        \caption{\textbf{LPGD} - Unfolded network for Learned PGD with $T=3$}
        \label{fig:network_pgd}
    \end{figure}

    {\bf Main blocker:}\\[.5em] { How to compute prox$_{\mu g}$ efficiently and in a differentiable way?}\\[1em]

    \begin{itemize}
        \item Use dedicated solver and compute gradient with implicit function theorem.
        \item Use an unrolled algorithm (LISTA) to solve the prox.
    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Simulation}

    \textbf{Performance investigation}

    Very low dimensional simulation $k, m = 5, 8$ (because of memory issue). \\

    \vskip2em

    \begin{figure}[t]
        \centering
        \includegraphics[width=0.9\columnwidth]{loss_comparison}
        \caption{\textbf{Performance comparison} for different regularisation levels (\emph{left}) $\lambda = 0.1$, (\emph{right}) $\lambda = 0.8$.}
    \end{figure}

\end{frame}


\begin{frame}
    \frametitle{fMRI data deconvolution (UKBB)}

    We retain only $8000$ time-series of $250$ time-frames (3 minute 03 seconds), \\
    Deconvolution for a fixed kernel $h$ and estimate the neural activity signal $z$ for each voxels. \\

    \vskip.5em

    \begin{figure}[hbtp!]
            \includegraphics[width=0.6\columnwidth]{bold_deconv_loss_comp_0_1}
            \caption{\textbf{Performance comparison} $\lambda = 0.1\lambda_{\max}$ between LPGD-Taut and iterative PGD for the analysis formulation for the HRF deconvolution problem with fMRI data.}
    \end{figure}

\end{frame}


\begin{frame}{Weights coupling}

    We denote $\theta=(W, \alpha, \beta)$ the parameters of a given layer $\phi_{\theta}$.
    \[
    \phi_\theta(z, x) = \text{ST}\left(z - \alpha D^\top(D z -x), \lambda\alpha\right)
    \]
    \underline{Assumption 1:}\\
    \hskip3ex $D\in \Rset^{n\times m}$ is a dictionary with non-duplicated unit-normed columns.\\[1em]

    \begin{block}{Lemma~4.3 -- Weight coupling}
            If for all the couples $(z^*(x), x) \in \Rset^m \times \mathcal{B}_{\infty}$ such that $z^*(x)\in \argmin F_x(z)$, it holds $\phi_{\theta}(z^*(x), x) = z^*(x)$. Then, $\frac{\alpha}{\beta} W = D~.$
    \end{block}

    \vskip2em
    The solution of the Lasso is a fixed point of a given layer $\phi_{\theta}$ if and\\
    only if $\phi_{\theta}$ is equivalent to a step of ISTA with a given step-size.
\end{frame}


\frame{

    \frametitle{ISTA -- Convergence}



    \begin{block}{Convergence rates}
       If $f_x$ is $\mu$-strongly convex, \ie{} $\sigma_{\min}(D^TD) \ge \mu > 0$
       \[
            F_x(z^{(t)}) - F_x(z^*) \le \left(1 - \frac{\mu}{L}\right)^t\left(F_x(0) - F_x(z^*)\right)
       \]
       In the general case, $F_x(z^{(t)}) - F_x(z^*) \le \frac{L\|z^*\|_2}{t}$\\
     \end{block}
}


\begin{frame}{OISTA -- Convergence}
    \begin{block}{Proposition 3.1: Convergence}
        When $D$ is such that the solution is unique for all $x$ and $\lambda >0$,\\
        the sequence $(z^{(t)})$ generated by the algorithm converges to $z^* =\argmin F_x~.$

        Further, there exists an iteration $T^*$ such that for $t\geq T^*~,$ $\supp(z^{(t)}) = \supp(z^*) \triangleq S^*$.
    \end{block}

    \begin{block}{Proposition 3.2: Convergence rate}
        For $t > T^*~,$\\
        {\centering
            $F_x(z^{(t)}) - F_x(z^*) \leq L_{S^*} \frac{\|z^{*} - z^{(T^*)}\|^2}{2(t - T^*)}~.$\\[1em]
        }
        If moreover, $\lambda_{\min}(D_{S^*}^\top D_{S^*}) = \mu^* > 0~,$ then\\[1em]
        {\centering
            $F_x(z^{(t)}) - F_x(z^*) \leq
                (1 - \tfrac{\mu^*}{L_{S^*}})^{t - T^*}(F_x(z^{(T^*)}) - F_x(z^*))~.$\\
        }
    \end{block}
\end{frame}


\end{document}
