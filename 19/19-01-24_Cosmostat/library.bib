Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Moreau2018,
address = {Stockohlm, Sweden},
author = {Moreau, Thomas and Oudre, Laurent and Vayatis, Nicolas},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:home/tom/Documents/Mendeley/Moreau, Oudre, Vayatis_2018_DICOD Distributed Convolutional Sparse Coding.pdf:pdf},
pages = {3626--3634},
publisher = {PMLR (80)},
title = {{DICOD: Distributed Convolutional Sparse Coding}},
year = {2018}
}
@inproceedings{Radinsky2004,
abstract = {An improved algorithm for classification of nystagmus was designed allowing the sorting of response segments even in severely non-linear patients and subjects with abnormally large phase shifts. The algorithm employs a model-based approach that was developed by Rey and Galiana [1]. The improved classification algorithm consists of two essential stages. In the first stage the eye velocity response is classified to obtain initial estimates of the slow phase eye velocity intervals. In the second stage, the slow phase estimates are used to identify a response phase shift and non- linearity, and compensate for their effects. Multiple tests on simulated data and experimental data obtained from clinical subjects are presented. The results of the tests demonstrate that the algorithm is able to analyze the patient data with a high accuracy even in the presence of noise, eye- blinks and other artifacts. Keywords—Nystagmus},
address = {San Francisco, CA, USA},
author = {Radinsky, Iliya and Galiana, Henrietta L.},
booktitle = {IEEE Annual International Conference Engineering in Medicine and Biology Society (IEMBS)},
file = {:home/tom/Documents/Mendeley/Radinsky, Galiana_2004_Improved Algorithm for Classification of Ocular Nystagmus.pdf:pdf},
isbn = {0780384393},
keywords = {eye movements,nystagmus analysis,vestibulo-ocular reflex,vor},
pages = {534--537},
title = {{Improved Algorithm for Classification of Ocular Nystagmus}},
year = {2004}
}
@inproceedings{Suzuki2015,
address = {Lille, France},
author = {Suzuki, Taiji},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:home/tom/Documents/Mendeley/Suzuki_2015_Convergence rate of Bayesian tensor estimator and its minimax optimality.pdf:pdf},
pages = {1--9},
title = {{Convergence rate of Bayesian tensor estimator and its minimax optimality}},
year = {2015}
}
@article{Oculo28,
author = {Schwartz, MA and Selhorst, JB and Ochs, AL and Beck, RW and Campbell, WW and HArris, JK and Et, Al.},
journal = {Annals of Neurology},
number = {20},
pages = {677--83},
title = {{Oculomasticatory myorhythmia: a unique movement disorder occurring in Whipple's disease}},
volume = {Dec},
year = {1986}
}
@misc{Robert2015,
address = {Waltham, MA, USA},
author = {Robert, Matthieu and Contal, Emile and Moreau, Thomas and Vayatis, Nicolas and Vidal, Pierre-Paul},
booktitle = {Gordon Research conference on Eye Movement},
howpublished = {Oral Presentation, Gordon Research Conference on Eye Movement},
title = {{The Why and How of Recording Eye Movement from Very Early Childhood}},
year = {2015}
}
@article{Srivastava2014,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:home/tom/Documents/Mendeley/Srivastava et al._2014_Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{pratt1969hadamard,
author = {Pratt, William K. and Kane, Julius and Andrews, Harry C.},
journal = {Proceedings of the IEEE},
number = {1},
pages = {58--68},
publisher = {IEEE},
title = {{Hadamard transform image coding}},
volume = {57},
year = {1969}
}
@inproceedings{Wohlberg2014,
address = {Florence, Italy},
author = {Wohlberg, Brendt},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2014.6854992},
file = {:home/tom/Documents/Mendeley/Wohlberg_2014_Efficient convolutional sparse coding.pdf:pdf},
isbn = {9781479928927},
issn = {15206149},
keywords = {ADMM,Convolutional Sparse Coding,Sparse Coding,Sparse Representation},
pages = {7173--7177},
title = {{Efficient convolutional sparse coding}},
year = {2014}
}
@article{Coles2013,
author = {Coles, Lisa D and Patterson, Edward E and Sheffield, W Douglas and Mavoori, Jaideep and Higgins, Jason and Michael, Bland and Leyde, Kent and Cloyd, James C and Litt, Brian and Vite, Charles},
journal = {Epilepsy research},
number = {3},
pages = {456----460},
title = {{Feasibility study of a caregiver seizure alert system in canine epilepsy}},
volume = {106},
year = {2013}
}
@article{Mikolajczyk2005,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Mikolajczyk, Krystian and Schmid, C. and A, Cordelia Schmid.},
doi = {10.1109/TPAMI.2005.188},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/Mendeley/Mikolajczyk, Schmid, A_2005_A performance evaluation of local descriptors.pdf:pdf},
isbn = {0162-8828},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {10},
pages = {1615--1630},
pmid = {16237996},
title = {{A performance evaluation of local descriptors}},
url = {http://www-dsp.elet.polimi.it/VA-TLC/Articoli/mikolajczyk_pami2004-A performance evaluation of local descriptors.pdf%5Cnhttp://www.google.com/url?sa=t&source=web&cd=1&ved=0CBcQFjAA&url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5507&rep=},
volume = {27},
year = {2005}
}
@inproceedings{Razavian2014,
address = {Colombus, OH, USA},
author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
file = {:home/tom/Documents/Mendeley/Razavian et al._2014_CNN Features off-the-Shelf an Astounding Baseline for Recognition.pdf:pdf},
pages = {806--813},
title = {{CNN Features off-the-Shelf: an Astounding Baseline for Recognition}},
year = {2014}
}
@inproceedings{Eigen2014,
archivePrefix = {arXiv},
arxivId = {1312.1847},
author = {Eigen, David and Rolfe, Jason and Fergus, Rob and LeCun, Yann},
booktitle = {International Conference on Learning Representation (ICLR)},
eprint = {1312.1847},
file = {:home/tom/Documents/Mendeley/Eigen et al._2014_Understanding Deep Architectures using a Recursive Convolutional Network.pdf:pdf},
pages = {1--8},
title = {{Understanding Deep Architectures using a Recursive Convolutional Network}},
url = {http://arxiv.org/abs/1312.1847},
year = {2014}
}
@article{Bottou2008,
author = {Bottou, L{\'{e}}on and Bousquet, Olivier},
file = {:home/tom/Documents/Mendeley/Bottou, Bousquet_2008_Learning using large datasets.pdf:pdf},
journal = {Mining Massive DataSets for Security},
keywords = {large-scale learning,optimization,statistics},
title = {{Learning using large datasets}},
volume = {3},
year = {2008}
}
@article{Ghaoui2012,
archivePrefix = {arXiv},
arxivId = {1009.4219},
author = {{El Ghaoui}, Laurent and Viallon, Vivian and Rabbani, Tarek},
eprint = {1009.4219},
file = {:home/tom/Documents/Mendeley/El Ghaoui, Viallon, Rabbani_2012_Safe feature elimination for the LASSO and sparse supervised learning problems.pdf:pdf},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {feature elimination,lasso,logistic regression,sparse regression,svm},
number = {4},
pages = {667--698},
title = {{Safe feature elimination for the LASSO and sparse supervised learning problems}},
volume = {8},
year = {2012}
}
@inproceedings{Yu2012,
address = {Brussels, Belgium},
author = {Yu, Hsiang Fu and Hsieh, Cho Jui and Si, Si and Dhillon, Inderjit S.},
booktitle = {IEEE International Conference on Data Mining (ICDM)},
file = {:home/tom/Documents/Mendeley/Yu et al._2012_Scalable coordinate descent approaches to parallel matrix factorization for recommender systems.pdf:pdf},
keywords = {Low rank approximation,Matrix factorization,Parallelization,Recommender systems},
pages = {765--774},
title = {{Scalable coordinate descent approaches to parallel matrix factorization for recommender systems}},
year = {2012}
}
@phdthesis{agarwal2012computational,
author = {Agarwal, Alekh},
school = {University of California, Berkeley},
title = {{Computational Trade-offs in Statistical Learning}},
year = {2012}
}
@article{cortes1995support,
author = {Cortes, Corinna and Vapnik, Vladimir},
journal = {Machine learning},
number = {3},
pages = {273--297},
publisher = {Springer},
title = {{Support-vector networks}},
volume = {20},
year = {1995}
}
@article{Jasper1948,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jasper, Herbert H.},
doi = {10.1007/sl0869-007-9037-x},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/Mendeley/Jasper_1948_Charting the Sea of Brain Waves.pdf:pdf},
isbn = {0506006905},
issn = {0506006905},
journal = {Science},
number = {2805},
pages = {343--347},
pmid = {12428256},
title = {{Charting the Sea of Brain Waves}},
volume = {108},
year = {1948}
}
@inproceedings{larochelle2007empirical,
address = {Corvallis, United States},
author = {Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James and Bengio, Yoshua},
booktitle = {International Conference on Machine Learning (ICML)},
organization = {ACM},
pages = {473--480},
title = {{An empirical evaluation of deep architectures on problems with many factors of variation}},
year = {2007}
}
@inproceedings{Wan2012,
address = {Atlanta, GA, USA},
author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Lecun, Yann and Fergus, Rob},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:home/tom/Documents/Mendeley/Wan et al._2012_Regularization of Neural Networks using DropConnect.pdf:pdf},
pages = {1058--1066},
title = {{Regularization of Neural Networks using DropConnect}},
year = {2012}
}
@article{Aharon2008,
author = {Aharon, Michal and Elad, Michael},
file = {:home/tom/Documents/Mendeley/Aharon, Elad_2008_Sparse and Redundant Modeling of Image Content Using an Image-Signature-Dictionary.pdf:pdf},
journal = {SIAM Journal on Imaging Sciences},
keywords = {denoising,dictionary,image-signature,learning,matching pursuit,mod,sparse representation,subject classifications},
number = {3},
pages = {228--247},
title = {{Sparse and Redundant Modeling of Image Content Using an Image-Signature-Dictionary}},
volume = {1},
year = {2008}
}
@article{Dalcin2005,
author = {Dalc{\'{i}}n, Lisandro and Paz, Rodrigo and Storti, Mario},
doi = {10.1016/j.jpdc.2005.03.010},
file = {:home/tom/Documents/Mendeley/Dalc{\'{i}}n, Paz, Storti_2005_MPI for Python.pdf:pdf},
isbn = {0100000150},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
number = {9},
pages = {1108--1115},
pmid = {2317303},
title = {{MPI for Python}},
volume = {65},
year = {2005}
}
@article{Fikret2011,
author = {Fikret, Işık Karahanoğlu and Bayram, İlker and Dimitri, Van De Ville},
file = {:home/tom/Documents/Mendeley/Fikret, Bayram, Dimitri_2011_A Signal Processing Approach to Generalized 1-D Total Variation.pdf:pdf},
journal = {IEEE Transaction on Signal Processing},
number = {11},
pages = {5265--5274},
title = {{A Signal Processing Approach to Generalized 1-D Total Variation}},
volume = {59},
year = {2011}
}
@article{Sokolic2016,
archivePrefix = {arXiv},
arxivId = {1605.08254},
author = {Sokolic, Jure and Giryes, Raja and Sapiro, Guillermo and Rodrigues, Miguel R. D.},
doi = {10.1109/TSP.2017.2708039},
eprint = {1605.08254},
file = {:home/tom/Documents/Mendeley/Sokolic et al._2016_Robust Large Margin Deep Neural Networks.pdf:pdf},
issn = {1053-587X},
journal = {preprint ArXiv},
title = {{Robust Large Margin Deep Neural Networks}},
url = {http://arxiv.org/abs/1605.08254%0Ahttp://dx.doi.org/10.1109/TSP.2017.2708039},
volume = {1605.08254},
year = {2016}
}
@article{Dinh2017,
archivePrefix = {arXiv},
arxivId = {1703.04933},
author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
eprint = {1703.04933},
file = {:home/tom/Documents/Mendeley/Dinh et al._2017_Sharp Minima Can Generalize For Deep Nets.pdf:pdf},
issn = {1938-7228},
journal = {preprint ArXiv},
title = {{Sharp Minima Can Generalize For Deep Nets}},
url = {http://arxiv.org/abs/1703.04933},
volume = {1703.04933},
year = {2017}
}
@article{Papyan2017,
archivePrefix = {arXiv},
arxivId = {1708.08705},
author = {Sulam, Jeremias and Papyan, Vardan and Romano, Yaniv and Elad, Michael},
eprint = {1708.08705},
file = {:home/tom/Documents/Mendeley/Sulam et al._2017_Multi-Layer Convolutional Sparse Modeling Pursuit and Dictionary Learning.pdf:pdf},
journal = {preprint ArXiv},
title = {{Multi-Layer Convolutional Sparse Modeling: Pursuit and Dictionary Learning}},
url = {http://arxiv.org/abs/1708.08705},
volume = {1708.08705},
year = {2017}
}
@article{Starck2005,
author = {Starck, Jean-Luc and Elad, Michael and Donoho, David L},
doi = {10.1109/TIP.2005.852206},
file = {:home/tom/Documents/Mendeley/Starck, Elad, Donoho_2005_Image Decomposition Via the Combination of Sparse Representation and a Variational Approach.pdf:pdf},
isbn = {1057-7149},
issn = {1057-7149},
journal = {IEEE Trans.Image Process.},
keywords = {Basis pursuit denoising (BPDN),curvelet,discrete cosine transform (DCT),local,piecewise smooth,ridgelet,sparse representations,texture,total variation,wavelet},
number = {10},
pages = {1570--1582},
pmid = {16238062},
title = {{Image Decomposition Via the Combination of Sparse Representation and a Variational Approach}},
volume = {14},
year = {2005}
}
@article{Tao2015,
archivePrefix = {arXiv},
arxivId = {1501.02888},
author = {Tao, Shaozhe and Boley, Daniel and Zhang, Shuzhong},
doi = {10.1137/151004549},
eprint = {1501.02888},
file = {:home/tom/Documents/Mendeley//Tao, Boley, Zhang_2015_Local Linear Convergence of ISTA and FISTA on the LASSO Problem.pdf:pdf},
issn = {10526234},
journal = {preprint ArXiv},
title = {{Local Linear Convergence of ISTA and FISTA on the LASSO Problem}},
url = {http://arxiv.org/abs/1501.02888},
volume = {1501.02888},
year = {2015}
}
@article{Oculo6,
author = {Escudero, M and Vidal, Pierre-Paul},
journal = {European Journal of Neuroscience},
number = {8},
pages = {572--580},
title = {{A quantitative study of electroencephalography, eye movements and neck electromyography characterizing the sleep-wake cycle of the guinea-pig}},
volume = {Mar},
year = {1996}
}
@article{Martin2000,
author = {Martin, Richard J.},
doi = {10.1109/78.827549},
file = {:home/tom/Documents/Mendeley/Martin_2000_Metric for ARMA processes.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {autoregression,classification,linear systems},
number = {4},
pages = {1164--1170},
title = {{Metric for ARMA processes}},
volume = {48},
year = {2000}
}
@article{jerri1977shannon,
author = {Jerri, Abdul J},
journal = {Proceedings of the IEEE},
number = {11},
pages = {1565--1596},
publisher = {IEEE},
title = {{The Shannon sampling theorem, Its various extensions and applications: A tutorial review}},
volume = {65},
year = {1977}
}
@article{Oculo32,
author = {Toledano, H and Muhsinoglu, O and Luckman, J and Goldenberg-Cohen, N and Michowiz, S},
journal = {European Journal of Pediatric Neurology},
number = {19},
pages = {694--700},
title = {{Acquired nystagmus as the initial presenting sign of chiasmal glioma in young children}},
volume = {Nov},
year = {2015}
}
@article{Oculo27,
author = {Raudnitz, R. W.},
journal = {Jb Kinderheilkd},
pages = {145},
title = {{Zur Lehre von Spasmus Nutans}},
volume = {45},
year = {1897}
}
@article{Mairal2012,
archivePrefix = {arXiv},
arxivId = {1009.5358},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean},
doi = {10.1109/TPAMI.2011.156},
eprint = {1009.5358},
file = {:home/tom/Documents/Mendeley/Mairal, Bach, Ponce_2012_Task-driven dictionary learning.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Basis pursuit,Lasso,compressed sensing,dictionary learning,matrix factorization,semi-supervised learning},
number = {4},
pages = {791--804},
pmid = {21808090},
title = {{Task-driven dictionary learning}},
volume = {34},
year = {2012}
}
@inproceedings{Chan2005,
address = {San Diego, CA, USA},
author = {Chan, Antoni B. and Vasconcelos, Nuno},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2005.279},
file = {:home/tom/Documents/Mendeley/Chan, Vasconcelos_2005_Probabilistic kernels for the classification of auto-regressive visual processes.pdf:pdf},
isbn = {0769523722},
issn = {1063-6919},
pages = {846--851},
title = {{Probabilistic kernels for the classification of auto-regressive visual processes}},
year = {2005}
}
@article{Zou2005,
author = {Zou, Hui and Hastie, Trevor},
file = {:home/tom/Documents/Mendeley/Zou, Hastie_2005_Regularization and variable selection via the elastic-net.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (statistical methodology)},
keywords = {grouping effect,lars algorithm,lasso,penalization,variable selection},
number = {2},
pages = {301--320},
title = {{Regularization and variable selection via the elastic-net}},
volume = {67},
year = {2005}
}
@article{Deng2017,
abstract = {{\textcopyright} 2017 Hong Deng et al. Total variation (TV) is a well-known image model with extensive applications in various images and vision tasks, for example, denoising, deblurring, superresolution, inpainting, and compressed sensing. In this paper, we systematically study the coordinate descent (CoD) method for solving general total variation (TV) minimization problems. Based on multidirectional gradients representation, the proposed CoD method provides a unified solution for both anisotropic and isotropic TV-based denoising (CoDenoise). With sequential sweeping and small random perturbations, CoDenoise is efficient in denoising and empirically converges to optimal solution. Moreover, CoDenoise also delivers new perspective on understanding recursive weighted median filtering. By incorporating with the Augmented Lagrangian Method (ALM), CoD was further extended to TV-based image deblurring (ALMCD). The results on denoising and deblurring  validate the efficiency and effectiveness of the CoD-based methods.},
author = {Deng, H. and Ren, D. and Xiao, G. and Zhang, D. and Zuo, W.},
doi = {10.1155/2017/3012910},
file = {:home/tom/Documents/Mendeley/Deng et al._2017_A Coordinate Descent Method for Total Variation Minimization.pdf:pdf},
issn = {15635147},
journal = {Mathematical Problems in Engineering},
number = {1},
title = {{A Coordinate Descent Method for Total Variation Minimization}},
volume = {2017},
year = {2017}
}
@article{Goldfarb2013,
archivePrefix = {arXiv},
arxivId = {0912.4571},
author = {Goldfarb, Donald and Ma, Shiqian and Scheinberg, Katya},
doi = {10.1007/s10107-012-0530-2},
eprint = {0912.4571},
file = {:home/tom/Documents/Mendeley/Goldfarb, Ma, Scheinberg_2013_Fast alternating linearization methods for minimizing the sum of two convex functions.pdf:pdf},
isbn = {0025-5610},
issn = {00255610},
journal = {Mathematical Programming},
keywords = {Alternating direction method,Alternating linearization method,Augmented Lagrangian method,Convex optimization,Gauss-Seidel method,Optimal gradient method,Peaceman-Rachford method,Variable splitting},
number = {1-2},
pages = {349--382},
title = {{Fast alternating linearization methods for minimizing the sum of two convex functions}},
volume = {141},
year = {2013}
}
@inproceedings{Wang2012,
address = {Portland, USA},
author = {Wang, Dong and Technologies, Language and Tejedor, Javier and Computer, Human},
booktitle = {Annual Conference of the International Speech Communication Association},
file = {:home/tom/Documents/Mendeley/Wang et al._2012_Heterogeneous Convolutive Non-Negative Sparse Coding.pdf:pdf},
pages = {2150--2153},
title = {{Heterogeneous Convolutive Non-Negative Sparse Coding}},
year = {2012}
}
@article{Golyandina2014,
archivePrefix = {arXiv},
arxivId = {1206.6910},
author = {Golyandina, Nina and Korobeynikov, Anton},
doi = {10.1016/j.csda.2013.04.009},
eprint = {1206.6910},
file = {:home/tom/Documents/Mendeley/Golyandina, Korobeynikov_2014_Basic Singular Spectrum Analysis and Forecasting with R.pdf:pdf},
journal = {Computational Statistics \& Data Analysis},
keywords = {2000 msc,62m10,62m20,65c60,forecasting,frequency estimation,r package,singular spectrum analysis,time series,time series analysis},
pages = {934----954},
title = {{Basic Singular Spectrum Analysis and Forecasting with R}},
url = {http://arxiv.org/abs/1206.6910%0Ahttp://dx.doi.org/10.1016/j.csda.2013.04.009},
volume = {71},
year = {2014}
}
@inproceedings{cho2009kernel,
address = {Vancouver, Canada},
author = {Cho, Youngmin and Saul, Lawrence K},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Documents/Mendeley//Cho, Saul_2009_Kernel Methods for Deep Learning.pdf:pdf},
pages = {342--350},
title = {{Kernel Methods for Deep Learning}},
year = {2009}
}
@inproceedings{Kavukcuoglu2013,
address = {Vancouver, Canada},
author = {Kavukcuoglu, Koray and Sermanet, Pierre and Boureau, Y-lan and Gregor, Karol and Lecun, Yann},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Documents/Mendeley/Kavukcuoglu et al._2010_Learning Convolutional Feature Hierarchies for Visual Recognition.pdf:pdf},
keywords = {convolution,dictionary learning},
pages = {1090--1098},
title = {{Learning Convolutional Feature Hierarchies for Visual Recognition}},
year = {2010}
}
@article{Gilles2013,
author = {Gilles, Jerome},
doi = {10.1109/TSP.2013.2265222},
file = {:home/tom/Documents/Mendeley/Gilles_2013_Empirical wavelet transform.pdf:pdf},
isbn = {1053-587X VO - 61},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Adaptive filtering,empirical mode decomposition,wavelet},
number = {16},
pages = {3999--4010},
title = {{Empirical wavelet transform}},
volume = {61},
year = {2013}
}
@inproceedings{oner2012towards,
address = {San Diego, CA, USA},
author = {Oner, M and Pulcifer-Stump, J and Seeling, P and Kaya, T},
booktitle = {International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
pages = {1980--1983},
title = {{Towards the run and walk activity classification through Step detection-An Android application}},
year = {2012}
}
@techreport{Saux2015,
author = {Saux, Patrick},
file = {:home/tom/Documents/Mendeley/Saux_2015_Rough Paths Theory A Signature-Based Approach to Data Analysis.pdf:pdf},
pages = {1----60},
title = {{Rough Paths Theory A Signature-Based Approach to Data Analysis}},
year = {2015}
}
@article{Scaman2017,
archivePrefix = {arXiv},
arxivId = {1702.08704},
author = {Scaman, Kevin and Bach, Francis and Bubeck, S{\'{e}}bastien and Lee, Yin Tat and Massouli{\'{e}}, Laurent},
eprint = {1702.08704},
file = {:home/tom/Documents/Mendeley/Scaman et al._2017_Optimal algorithms for smooth and strongly convex distributed optimization in networks.pdf:pdf},
journal = {preprint ArXiv},
title = {{Optimal algorithms for smooth and strongly convex distributed optimization in networks}},
url = {http://arxiv.org/abs/1702.08704},
volume = {1702.08704},
year = {2017}
}
@inproceedings{Bengio2006,
address = {Vancouver, Canada},
author = {Bengio, Yoshua and Roux, Nicolas Le and Vincent, Pascal and Delalleau, Olivier and Marcotte, Patrice and Branch, Downtown},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Documents/Mendeley/Bengio et al._2006_Convex Neural Networks.pdf:pdf},
pages = {123--130},
title = {{Convex Neural Networks}},
year = {2006}
}
@article{Mazzoni2015,
abstract = {Leaky integrate-and-fire (LIF) network models are commonly used to study how the spiking dynamics of neural networks changes with stimuli, tasks or dynamic network states. However, neurophysiological studies in vivo often rather measure the mass activity of neuronal microcircuits with the local field potential (LFP). Given that LFPs are generated by spatially separated currents across the neuronal membrane, they cannot be computed directly from quantities defined in models of point-like LIF neurons. Here, we explore the best approximation for predicting the LFP based on standard output from point-neuron LIF networks. To search for this best "LFP proxy", we compared LFP predictions from candidate proxies based on LIF network output (e.g, firing rates, membrane potentials, synaptic currents) with "ground-truth" LFP obtained when the LIF network synaptic input currents were injected into an analogous three-dimensional (3D) network model of multi-compartmental neurons with realistic morphology, spatial distributions of somata and synapses. We found that a specific fixed linear combination of the LIF synaptic currents provided an accurate LFP proxy, accounting for most of the variance of the LFP time course observed in the 3D network for all recording locations. This proxy performed well over a broad set of conditions, including substantial variations of the neuronal morphologies. Our results provide a simple formula for estimating the time course of the LFP from LIF network simulations in cases where a single pyramidal population dominates the LFP generation, and thereby facilitate quantitative comparison between computational models and experimental LFP recordings in vivo.},
author = {Mazzoni, Alberto and Lind{\'{e}}n, Henrik and Cuntz, Hermann and Lansner, Anders and Panzeri, Stefano and Einevoll, Gaute T.},
doi = {10.1371/journal.pcbi.1004584},
file = {:home/tom/Documents/Mendeley/Mazzoni et al._2015_Computing the Local Field Potential (LFP) from Integrate-and-Fire Network Models.pdf:pdf},
isbn = {1553-734x},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {12},
pages = {1--38},
pmid = {26657024},
title = {{Computing the Local Field Potential (LFP) from Integrate-and-Fire Network Models}},
volume = {11},
year = {2015}
}
@inproceedings{Lee2016,
abstract = {We show that gradient descent converges to a local minimizer, almost surely with random initialization. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.},
address = {New-York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1602.04915},
author = {Lee, Jason D. and Simchowitz, Max and Jordan, Michael I. and Recht, Benjamin},
booktitle = {Conference on Learning Theory (COLT)},
eprint = {1602.04915},
file = {:home/tom/Documents/Mendeley/Lee et al._2016_Gradient Descent Converges to Minimizers.pdf:pdf},
keywords = {dynamical systems,gradient descent,local minimum,saddle points,smooth optimization},
pages = {1246--1257},
title = {{Gradient Descent Converges to Minimizers}},
url = {http://arxiv.org/abs/1602.04915},
year = {2016}
}
@inproceedings{Moghadam2005,
abstract = {Sparse PCA seeks approximate sparse “eigenvectors” whose projections capture the maximal variance of data. As a cardinality-constrained and non-convex optimization problem, it is NP-hard and is encountered in a wide range of applied fields, from bio-informatics to finance. Recent progress has focused mainly on continuous approximation and convex relaxation of the hard cardinality constraint. In contrast, we consider an alternative discrete spectral formulation based on variational eigenvalue bounds and provide an effective greedy strategy as well as provably optimal solutions using branch-and-bound search. Moreover, the exact methodology used reveals a simple renormalization step that improves approximate solutions obtained by any continuous method. The resulting performance gain of discrete algorithms is demonstrated on real-world benchmark data and in extensive Monte Carlo evaluation trials.},
address = {Vancouver, Canada},
annote = {From Duplicate 1 (Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms - Moghadam, Baback; Weiss, Yair; Avidan, Shai)

* Propose a 2-step procedure that evaluate the support of the sparse-eigenvector and then solve a reduced kxk problem to get exact value.
* Describe a combinatorial algorithm to compute the SPCA using growing matrices},
author = {Moghadam, Baback and Weiss, Yair and Avidan, Shai},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1145/1143844.1143925},
file = {:home/tom/Documents/Mendeley//Moghadam, Weiss, Avidan_2006_Spectral Bounds for Sparse PCA Exact and Greedy Algorithms.pdf:pdf},
isbn = {9780262232531},
issn = {1049-5258},
pages = {915--922},
title = {{Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms}},
year = {2006}
}
@inproceedings{Kurtek2011,
address = {Grenada, Spain},
author = {Kurtek, Sebastian and Srivastava, Anuj and Wu, Wei},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Documents/Mendeley/Kurtek, Srivastava, Wu_2011_Signal Estimation Under Random Time-Warpings and Nonlinear Signal Alignment.pdf:pdf},
isbn = {9781618395993},
pages = {575--683},
title = {{Signal Estimation Under Random Time-Warpings and Nonlinear Signal Alignment}},
year = {2011}
}
@inproceedings{Zinkevich2010,
abstract = {With the increase in available data parallel machine learning has become an in- creasingly pressing problem. In this paper we present the first parallel stochastic gradient descent algorithm including a detailed analysis and experimental evi- dence. Unlike prior work on parallel optimization algorithms [5, 7] our variant comes with parallel acceleration guarantees and it poses no overly tight latency constraints, which might only be available in the multicore setting. Our analy- sis introduces a novel proof technique — contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits. As a side effect this answers the question of how quickly stochastic gradient descent algorithms reach the asymptotically normal regime [1, 8]. 1},
address = {Vancouver, Canada},
author = {Zinkevich, Martin a and Smola, Alex and Weimer, Markus},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1088/0741-3335/38/11/011},
file = {:home/tom/Documents/Mendeley/Zinkevich, Smola, Weimer_2010_Parallelized Stochastic Gradient Descent.pdf:pdf},
issn = {07413335},
keywords = {Distributed,Optimization,Parallel algorithm,Stochastic Gradient Descent},
mendeley-tags = {Distributed,Optimization},
pages = {2595--2603},
pmid = {12484348},
title = {{Parallelized Stochastic Gradient Descent}},
url = {http://martin.zinkevich.org/publications/nips2010.pdf},
year = {2010}
}
@inproceedings{Zheng2015,
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1503.05479v1},
author = {Zheng, Qinqing and Tomioka, Ryota},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1503.05479v1},
pages = {3106--3113},
title = {{Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm}},
year = {2015}
}
@book{Hastie2009,
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
booktitle = {Springer},
doi = {10.1007/b94608},
file = {:home/tom/Documents/Mendeley/Hastie, Tibshirani, Friedman_2009_The Elements of Statistical Learning.pdf:pdf},
isbn = {9780387848570},
issn = {03436993},
pages = {1----694},
pmid = {15512507},
title = {{The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/10.1007/b94608},
year = {2009}
}
@article{Xiong2017,
abstract = {In this work, we generalize the detrended fluctuation analysis (DFA) to the multivariate case, named multivariate DFA (MVDFA). The validity of the proposed MVDFA is illustrated by numerical simulations on synthetic multivariate processes, where the cases that initial data are generated independently from the same system and from different systems as well as the correlated variate from one system are considered. Moreover, the proposed MVDFA works well when applied to the multi-scale analysis of the returns of stock indices in Chinese and US stock markets. Generally, connections between the multivariate system and the individual variate are uncovered, showing the solid performances of MVDFA and the multi-scale MVDFA.},
author = {Xiong, Hui and Shang, P.},
doi = {10.1016/j.cnsns.2016.04.035},
file = {:home/tom/Documents/Mendeley/Xiong, Shang_2017_Detrended fluctuation analysis of multivariate time series.pdf:pdf},
issn = {10075704},
journal = {Communications in Nonlinear Science and Numerical Simulation},
keywords = {Detrended fluctuation analysis,Financial time series,Multi-scale,Multivariate},
pages = {12--21},
publisher = {Elsevier B.V.},
title = {{Detrended fluctuation analysis of multivariate time series}},
volume = {42},
year = {2017}
}
@article{Vu2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1211.0373v4},
author = {Vu, Vincent Q and Lei, Jing},
doi = {10.1214/13-AOS1151},
eprint = {arXiv:1211.0373v4},
file = {:home/tom/Documents/Mendeley/Vu, Lei_2014_Minimax sparse principal subspace estimation in high dimensions.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {62C20,62H12,62H25, 62H12, 62C20, Principal components analysis,Principal components analysis},
number = {6},
pages = {2905--2947},
title = {{Minimax sparse principal subspace estimation in high dimensions}},
volume = {41},
year = {2014}
}
@inproceedings{Zhong2014,
abstract = {We consider the class of optimization problems arising from computationally intensive L1-regularized M-estimators, where the function or gradient values are very expensive to compute. A particular instance of interest is the L1-regularized MLE for learning Conditional Random Fields (CRFs), which are a popular class of statistical models for varied structured prediction problems such as sequence labeling, alignment, and classification with label taxonomy. L1-regularized MLEs for CRFs are particularly expensive to optimize since computing the gradient values requires an expensive inference step. In this work, we propose the use of a carefully constructed proximal quasi-Newton algorithm for such computationally intensive M-estimation problems, where we employ an aggressive active set selection technique. In a key contribution of the paper, we show that the proximal quasi-Newton method is provably super-linearly convergent, even in the absence of strong convexity, by leveraging a restricted variant of strong convexity. In our experiments, the proposed algorithm converges considerably faster than current state-of-the-art on the problems of sequence labeling and hierarchical classification.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.7321v1},
author = {Zhong, Kai and Yen, Ian E.H. and Dhillon, Inderjit S. and Ravikumar, Pradeep},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {arXiv:1406.7321v1},
file = {:home/tom/Documents/Mendeley//Zhong et al._2014_Proximal Quasi-Newton for Computationally Intensive $ell_1$-regularizedM-estimators.pdf:pdf},
issn = {10495258},
pages = {2375--2383},
title = {{Proximal Quasi-Newton for Computationally Intensive $\ell_1$-regularizedM-estimators}},
year = {2014}
}
@article{Oymak2015,
archivePrefix = {arXiv},
arxivId = {1507.04793},
author = {Oymak, Samet and Recht, Benjamin and Soltanolkotabi, Mahdi},
eprint = {1507.04793},
file = {:home/tom/Documents/Mendeley//Oymak, Recht, Soltanolkotabi_2015_Sharp Time-Data Tradeoffs for Linear Inverse Problems.pdf:pdf},
journal = {preprint ArXiv},
title = {{Sharp Time-Data Tradeoffs for Linear Inverse Problems}},
url = {http://arxiv.org/abs/1507.04793},
volume = {1507.04793},
year = {2015}
}
@book{Golyandina2002,
author = {Golyandina, Nina and Nekrutkin, Vladimir and Zhigljavsky, Anatoly A},
file = {:home/tom/Documents/Mendeley/Golyandina, Nekrutkin, Zhigljavsky_2001_Analysis of Time Series Structure SSA and Related Techniques.pdf:pdf},
publisher = {CRC Press},
title = {{Analysis of Time Series Structure: SSA and Related Techniques}},
year = {2001}
}
@article{Oculo24,
author = {Newman, SA and Hedges, TR and Wall, M and Sedwick, LA},
journal = {Survey of Ophthalmology},
number = {34},
pages = {453--456},
title = {{Spasmus nutans-- or is it?}},
volume = {May-Jun},
year = {1990}
}
@inproceedings{Papagerogakis2017,
author = {Papagerogakis, Christos and Hitziger, Sebastian and Papadopoulo, Th\'eodore},
booktitle = {Groupe de Recherche et d'Etudes en Traitement du Signal et des Images (GRETSI)},
file = {:home/tom/Documents/Mendeley/Papagerogakis, Hitziger, Papadopoulo_2017_Dictionary Learning for multidimensional data.pdf:pdf},
pages = {1--4},
title = {{Dictionary Learning for multidimensional data}},
year = {2017}
}
@inproceedings{Schuldt2004,
abstract = {Local space-time features capture local events in video and can be adapted to the size, the frequency and the veloc-ity of moving patterns. In this paper we demonstrate how such features can be used for recognizing complex motion patterns. We construct video representations in terms of lo-cal space-time features and integrate such representations with SVM classification schemes for recognition. For the purpose of evaluation we introduce a new video database containing 2391 sequences of six human actions performed by 25 people in four different scenarios. The presented re-sults of action recognition justify the proposed method and demonstrate its advantage compared to other relative ap-proaches for action recognition.},
address = {Cambridge, UK},
archivePrefix = {arXiv},
arxivId = {1505.04868},
author = {Sch{\"{u}}ldt, Christian and Laptev, Ivan and Caputo, Barbara},
booktitle = {International Conference on Pattern Recognition (ICPR)},
doi = {10.1109/ICPR.2004.1334462},
eprint = {1505.04868},
file = {:home/tom/Documents/Mendeley/Sch{\"{u}}ldt, Laptev, Caputo_2004_Recognizing human actions A local SVM approach.pdf:pdf},
isbn = {0769521282},
issn = {10514651},
pages = {32--36},
pmid = {12171414},
title = {{Recognizing human actions: A local SVM approach}},
year = {2004}
}
@inproceedings{Telgarsky2016,
abstract = {For any positive integer $k$, there exist neural networks with $\Theta(k^3)$ layers, $\Theta(1)$ nodes per layer, and $\Theta(1)$ distinct parameters which can not be approximated by networks with $\mathcal{O}(k)$ layers unless they are exponentially large --- they must possess $\Omega(2^k)$ nodes. This result is proved here for a class of nodes termed "semi-algebraic gates" which includes the common choices of ReLU, maximum, indicator, and piecewise polynomial functions, therefore establishing benefits of depth against not just standard networks with ReLU gates, but also convolutional networks with ReLU and maximization gates, sum-product networks, and boosted decision trees (in this last case with a stronger separation: $\Omega(2^{k^3})$ total tree nodes are required).},
address = {New-York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1602.04485},
author = {Telgarsky, Matus},
booktitle = {Conference on Learning Theory (COLT)},
eprint = {1602.04485},
file = {:home/tom/Documents/Mendeley/Telgarsky_2016_Benefits of depth in neural networks.pdf:pdf},
keywords = {1,a model of real-valued,a neural network is,approximation,as follows,computation defined by a,connected directed graph,depth hierarchy,neural networks,nodes await real numbers,on their incoming edges,representation,setting and main results,thereafter computing a function},
pages = {1--23},
title = {{Benefits of depth in neural networks}},
url = {http://arxiv.org/abs/1602.04485},
year = {2016}
}
@article{Cole2017,
abstract = {Oscillations are a prevalent feature of brain recordings. They are believed to play key roles in neural communication and computation. Current analysis methods for studying neural oscillations often implicitly assume that the oscillations are sinusoidal. While these approaches have proven fruitful, we show here that there are numerous instances in which neural oscillations are nonsinusoidal. We highlight approaches to characterize nonsinusoidal features and account for them in traditional spectral analysis. Instead of being a nuisance, we discuss how these nonsinusoidal features may provide crucial and so far overlooked physiological information related to neural communication, computation, and cognition.},
author = {Cole, Scott R. and Voytek, Bradley},
doi = {10.1016/j.tics.2016.12.008},
file = {:home/tom/Documents/Mendeley/Cole, Voytek_2017_Brain Oscillations and the Importance of Waveform Shape.pdf:pdf},
isbn = {1879-307X (Electronic) 1364-6613 (Linking)},
issn = {1879307X},
journal = {Trends in Cognitive Sciences},
keywords = {nonsinusoidal,oscillation,phase–amplitude coupling,shape,waveform},
number = {2},
pages = {137--149},
pmid = {28063662},
publisher = {Elsevier Ltd},
title = {{Brain Oscillations and the Importance of Waveform Shape}},
url = {http://dx.doi.org/10.1016/j.tics.2016.12.008},
volume = {21},
year = {2017}
}
@article{erhan2010does,
author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
journal = {Journal of Machine Learning Research (JMLR)},
number = {Feb},
pages = {625--660},
title = {{Why does unsupervised pre-training help deep learning?}},
volume = {11},
year = {2010}
}
@article{cadieu2012learning,
author = {Cadieu, Charles F. and Olshausen, Bruno A.},
file = {:home/tom/Documents/Mendeley//Cadieu, Olshausen_2012_Learning intermediate-level representations of form and motion from natural movies.pdf:pdf},
journal = {Neural computation},
number = {4},
pages = {827--866},
publisher = {MIT Press},
title = {{Learning intermediate-level representations of form and motion from natural movies}},
volume = {24},
year = {2012}
}
@article{Oculo16,
author = {Hung, GK and Semmlow, JL and Ciuffreda, KJ},
journal = {IEEE Transaction on Biomedical Engineering},
number = {33},
pages = {1021--1028},
title = {{A dual-mode dynamic model of the vergence eye movement system}},
volume = {Nov},
year = {1986}
}
@article{Wiatowski2015,
abstract = {Deep convolutional neural networks have led to breakthrough results in numerous practical machine learning tasks such as classification of images in the ImageNet data set, control-policy-learning to play Atari games or the board game Go, and image captioning. Many of these applications first perform feature extraction and then feed the results thereof into a trainable classifier. The mathematical analysis of deep convolutional neural networks for feature extraction was initiated by Mallat, 2012. Specifically, Mallat considered so-called scattering networks based on a wavelet transform followed by the modulus non-linearity in each network layer, and proved translation invariance (asymptotically in the wavelet scale parameter) and deformation stability of the corresponding feature extractor. This paper complements Mallat's results by developing a theory of deep convolutional neural networks for feature extraction encompassing general convolutional transforms, or in more technical parlance, general semi-discrete frames (including Weyl-Heisenberg, curvelet, shearlet, ridgelet, and wavelet frames), general Lipschitz-continuous non-linearities (e.g., rectified linear units, shifted logistic sigmoids, hyperbolic tangents, and modulus functions), and general Lipschitz-continuous pooling operators emulating sub-sampling and averaging. In addition, all of these elements can be different in different network layers. For the resulting feature extractor we prove a translation invariance result which is of vertical nature in the sense of the network depth determining the amount of invariance, and we establish deformation sensitivity bounds that apply to signal classes with inherent deformation insensitivity such as, e.g., band-limited functions.},
archivePrefix = {arXiv},
arxivId = {1512.06293},
author = {Wiatowski, Thomas and B{\"{o}}lcskei, Helmut},
eprint = {1512.06293},
file = {:home/tom/Documents/Mendeley/Wiatowski, B{\"{o}}lcskei_2018_A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
title = {{A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction}},
url = {http://arxiv.org/abs/1512.06293},
volume = {to appear},
year = {2018}
}
@article{Wazen2014,
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Larson-prior, L. J. and Oostenveld, R. and Penna, S. D. and Michalareas, G. and Prior, F. and Babajani-Feremi, A. and Schoffelen, J-M. and Marzetti, L. and de Pasquale, F. and {Di Pompeo}, F. and Stout, J. and Woolrich, M. and Luo, Q. and Bucholz, R. and Fries, P. and Pizzella, V. and Romani, G.L. and Corbetta, M. and Snyder, A. Z. and {HCP Consortium}, WU-Minn},
doi = {10.2217/nnm.12.167.Gene},
eprint = {NIHMS150003},
file = {:home/tom/Documents/Mendeley/Larson-prior et al._2013_Adding dynamics to the Human Connectome Project with MEG.pdf:pdf},
isbn = {3149778794},
issn = {08966273},
journal = {NeuroImage},
keywords = {1,2,and ensure clinical performance,and stability,bone,development of biomaterials with,dna microarray,histomorphometry,implant research is the,nanotopography,surface,surfaces that,the current trend in,titanium alloy,will improve tissue integration},
number = {9},
pages = {190--201},
pmid = {1000000221},
title = {{Adding dynamics to the Human Connectome Project with MEG}},
volume = {80},
year = {2013}
}
@article{Fercoq2016,
author = {Fercoq, Olivier and Richtarik, Peter},
file = {:home/tom/Documents/Mendeley/Fercoq, Richtarik_2016_Optimization in High Dimensions via Accelerated , Parallel , and Proximal Coordinate Descent.pdf:pdf},
journal = {SIAM Journal on Optimization},
number = {4},
pages = {1997----2023},
title = {{Optimization in High Dimensions via Accelerated , Parallel , and Proximal Coordinate Descent}},
volume = {25},
year = {2016}
}
@article{Hornik1991,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt},
doi = {10.1016/0893-6080(91)90009-T},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/Mendeley/Hornik_1991_Approximation capabilities of multilayer feedforward networks.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Activation function,Input environment measure,Lp(??) approximation,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities},
number = {2},
pages = {251--257},
pmid = {25246403},
title = {{Approximation capabilities of multilayer feedforward networks}},
volume = {4},
year = {1991}
}
@inproceedings{Jenatton2009,
address = {Clearwater Beach, FL, USA},
archivePrefix = {arXiv},
arxivId = {0909.1440},
author = {Jenatton, Rodolphe and Obozinski, Guillaume and Bach, Francis},
booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
doi = {1553374},
eprint = {0909.1440},
file = {:home/tom/Documents/Mendeley/Jenatton, Obozinski, Bach_2009_Structured Sparse Principal Component Analysis.pdf:pdf},
issn = {15324435},
pages = {366--373},
title = {{Structured Sparse Principal Component Analysis}},
url = {http://arxiv.org/abs/0909.1440},
year = {2009}
}
@book{Mallat2008,
author = {Mallat, St\'ephane},
file = {:home/tom/Documents/Mendeley/Mallat_2008_A Wavelet Tour of Signal Processing.pdf:pdf},
publisher = {Academic press},
title = {{A Wavelet Tour of Signal Processing}},
year = {2008}
}
@article{auvinet2002reference,
author = {Auvinet, B and Berrut, G and Touzard, C and Moutel, L and Collet, N and Chaleil, D and Barrey, E},
journal = {Gait \& posture},
number = {2},
pages = {124--134},
publisher = {Elsevier},
title = {{Reference data for normal subjects obtained with an accelerometric device}},
volume = {16},
year = {2002}
}
@article{Candes2011,
archivePrefix = {arXiv},
arxivId = {0912.3599},
author = {Candes, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
eprint = {0912.3599},
file = {:home/tom/Documents/Mendeley/Candes et al._2011_Robust Principal Component Analysis.pdf:pdf},
isbn = {0780362780},
issn = {0899-7667},
journal = {Journal of the Association for Computing Machinery (JACM)},
keywords = {duality,low-rank matrices,minimization,nuclear-norm minimization,principal components,robustness vis-a-vis outliers,sparsity,video surveillance},
number = {3},
pages = {11},
title = {{Robust Principal Component Analysis?}},
url = {http://arxiv.org/abs/0912.3599},
volume = {58},
year = {2011}
}
@article{Blumensath2012,
author = {Blumensath, Thomas},
doi = {10.1016/j.sigpro.2011.09.017},
file = {:home/tom/Documents/Mendeley/Blumensath_2012_Accelerated iterative hard thresholding.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Compressed sensing,Iterative hard thresholding},
number = {3},
pages = {752--756},
publisher = {Elsevier},
title = {{Accelerated iterative hard thresholding}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0165168411003197},
volume = {92},
year = {2012}
}
@article{Sherman2016,
author = {Sherman, Maxwell A. and Lee, Shane and Law, Robert and Haegens, Saskia and Thorn, Catherine A. and H{\"{a}}m{\"{a}}l{\"{a}}inen, Matti S. and Moore, Christopher I. and Jones, Stephanie R.},
doi = {10.1073/pnas.1604135113},
file = {:home/tom/Documents/Mendeley/Sherman et al._2016_Neural mechanisms of transient neocortical beta rhythms Converging evidence from humans, computational modeling, mon.pdf:pdf},
isbn = {1091-6490 (Electronic)\r0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {33},
pages = {E4885--E4894},
pmid = {27469163},
title = {{Neural mechanisms of transient neocortical beta rhythms: Converging evidence from humans, computational modeling, monkeys, and mice}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1604135113},
volume = {113},
year = {2016}
}
@inproceedings{coates2011importance,
address = {Bellevue, WA, USA},
author = {Coates, Adam and Ng, Andrew Y},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:home/tom/Documents/Mendeley//Coates, Ng_2011_The importance of encoding versus training with sparse coding and vector quantization.pdf:pdf},
pages = {921--928},
title = {{The importance of encoding versus training with sparse coding and vector quantization}},
year = {2011}
}
@article{Krishnaprasad1983,
author = {Krishnaprasad, P. S.},
journal = {International Journal of Control,},
number = {5},
pages = {1055--1079},
title = {{On families of systems and deformations}},
volume = {38},
year = {1983}
}
@inproceedings{Richard2012,
abstract = {The paper introduces a penalized matrix estimation procedure aiming at solutions which are sparse and low-rank at the same time. Such structures arise in the context of social networks or protein interactions where underlying graphs have adjacency matrices which are block-diagonal in the appropriate basis. We introduce a convex mixed penalty which involves $\ell_1$-norm and trace norm simultaneously. We obtain an oracle inequality which indicates how the two effects interact according to the nature of the target matrix. We bound generalization error in the link prediction problem. We also develop proximal descent strategies to solve the optimization problem efficiently and evaluate performance on synthetic and real data sets.},
address = {Edinburgh, Great Britain},
archivePrefix = {arXiv},
arxivId = {1206.6474},
author = {Richard, Emile and Paris, Ecole Centrale and Vayatis, Nicolas and Savalle, Pierre-Andre},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1206.6474},
file = {:home/tom/Documents/Mendeley/Richard et al._2012_Estimation of Simultaneously Sparse and Low Rank Matrices.pdf:pdf},
isbn = {978-1-4503-1285-1},
pages = {1351--1358},
title = {{Estimation of Simultaneously Sparse and Low Rank Matrices}},
year = {2012}
}
@article{Mallat1993,
author = {Mallat, St\'ephane and Zhang, Zhifeng},
file = {:home/tom/Documents/Mendeley/Mallat, Zhang_1993_Matching Pursuits With Time-Frequency Dictionaries.pdf:pdf},
journal = {IEEE Transactions on Signal Processing},
number = {12},
pages = {3397--3415},
title = {{Matching Pursuits With Time-Frequency Dictionaries}},
volume = {41},
year = {1993}
}
@article{Bartlett2003,
author = {Bartlett, Peter L and Maass, Wolfgang},
file = {:home/tom/Documents/Mendeley/Bartlett, Maass_2003_Vapnik-Chervonenkis Dimension of Neural Nets.pdf:pdf},
journal = {The handbook of brain theory and neural networks},
pages = {1188--1192},
title = {{Vapnik-Chervonenkis Dimension of Neural Nets}},
year = {2003}
}
@article{Oculo26,
author = {Orekhova, EV and Stroganova, TA and Posikera, IN and Elam, M},
journal = {Clinical Neurophysiology},
number = {17},
pages = {1047--62},
title = {{EEG theta rhythm in infants and preschool children}},
volume = {MAy},
year = {2006}
}
@article{Macqueen1967,
abstract = {This paper describes a number of applications of the 'k-means', a procedure for classifying a random sample of points in E sub N. The procedure consists of starting with k groups which each consist of a single random point, and thereafter adding the points one after another to the group whose mean each point is nearest. After a point is added to a group, the mean of that group is adjusted so as to take account of the new point. Thus at each stage there are in fact k means, one for each group. After the sample is processed in this way, the points are classified on the basis of nearness to the final means. The portions which result tend to be fficient in the sense of having low within class variance. Applications are suggested for the problems of non-linear prediction, efficient communication, non-parametric tests of independence, similarity grouping, and automatic file construction. The extension of the methods to general metric spaces is indicated.},
author = {Macqueen, J},
doi = {citeulike-article-id:6083430},
file = {:home/tom/Documents/Mendeley/Macqueen_1967_Some methods for classification and analysis of multivariate observations.pdf:pdf},
isbn = {1595931619},
issn = {00970433},
journal = {Berkeley Symposium on Mathematical Statistics and Probability},
number = {233},
pages = {281--297},
pmid = {17121457},
title = {{Some methods for classification and analysis of multivariate observations}},
volume = {1},
year = {1967}
}
@inproceedings{Bo2012,
author = {Bo, Liefeng and Sminchisescu, Cristian},
booktitle = {preprint ArXiv},
file = {:home/tom/Documents/Mendeley/Bo, Sminchisescu_2012_Greedy Block Coordinate Descent for Large Scale Gaussian Process Regression.pdf:pdf},
isbn = {0-9749039-4-9},
title = {{Greedy Block Coordinate Descent for Large Scale Gaussian Process Regression}},
volume = {1206.3238},
year = {2012}
}
@phdthesis{thuer2008step,
address = {Belgium},
author = {Th{\"{u}}er, G and Verwimp, T},
booktitle = {E-Lab Masters Thesis, from the Artesis University College of Antwerp, Antwerp, Belgium},
school = {Artesis University College of Antwerp},
title = {{Step detection algorithms for accelerometers}},
year = {2008}
}
@article{Giavalisco2004,
archivePrefix = {arXiv},
arxivId = {astro-ph/0309105},
author = {Giavalisco, M. and {the GOODS Teams}},
doi = {10.1086/379232},
eprint = {0309105},
file = {:home/tom/Documents/Mendeley/Giavalisco, the GOODS Teams_2004_The Great Observatories Origins Deep Survey Initial Results From Optical and Near-Infrared Imaging.pdf:pdf},
journal = {The Astrophysical Journal Letters},
number = {2},
pages = {L93},
primaryClass = {astro-ph},
title = {{The Great Observatories Origins Deep Survey: Initial Results From Optical and Near-Infrared Imaging}},
url = {http://arxiv.org/abs/astro-ph/0309105%0Ahttp://dx.doi.org/10.1086/379232},
volume = {600},
year = {2004}
}
@article{Hall2014,
author = {Hall, Peter and Xue, Jing Hao},
doi = {10.1016/j.csda.2012.10.010},
file = {:home/tom/Documents/Mendeley/Hall, Xue_2014_On selecting interacting features from high-dimensional data.pdf:pdf},
isbn = {01679473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Classification,Correlation,Feature ranking,Generalised correlation},
pages = {694--708},
publisher = {Elsevier B.V.},
title = {{On selecting interacting features from high-dimensional data}},
url = {http://dx.doi.org/10.1016/j.csda.2012.10.010},
volume = {71},
year = {2014}
}
@inproceedings{Moghadam2005a,
abstract = {Sparse PCA seeks approximate sparse “eigenvectors” whose projections capture the maximal variance of data. As a cardinality-constrained and non-convex optimization problem, it is NP-hard and is encountered in a wide range of applied fields, from bio-informatics to finance. Recent progress has focused mainly on continuous approximation and convex relaxation of the hard cardinality constraint. In contrast, we consider an alternative discrete spectral formulation based on variational eigenvalue bounds and provide an effective greedy strategy as well as provably optimal solutions using branch-and-bound search. Moreover, the exact methodology used reveals a simple renormalization step that improves approximate solutions obtained by any continuous method. The resulting performance gain of discrete algorithms is demonstrated on real-world benchmark data and in extensive Monte Carlo evaluation trials.},
author = {Moghadam, Baback and Weiss, Yair and Avidan, Shai},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1145/1143844.1143925},
file = {:home/tom/Documents/Mendeley//Moghadam, Weiss, Avidan_2006_Spectral Bounds for Sparse PCA Exact and Greedy Algorithms.pdf:pdf},
isbn = {9780262232531},
issn = {1049-5258},
pages = {915--922},
title = {{Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms}},
year = {2006}
}
@inproceedings{Bristow2013,
address = {Portland, OR, USA},
author = {Bristow, Hilton and Eriksson, Anders and Lucey, Simon},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Documents/Mendeley/Bristow, Eriksson, Lucey_2013_Fast convolutional sparse coding.pdf:pdf},
keywords = {ADMM,convolution,deep learning,fourier,sparse coding},
pages = {391--398},
title = {{Fast convolutional sparse coding}},
year = {2013}
}
@inproceedings{alaoui15fast,
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1411.0306},
author = {{El Alaoui}, Ahmed and Mahoney, Michael W.},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1411.0306},
file = {:home/tom/Documents/Mendeley/El Alaoui, Mahoney_2015_Fast Randomized Kernel Ridge Regression with Statistical Guarantees.pdf:pdf},
issn = {10495258},
pages = {775--783},
title = {{Fast Randomized Kernel Ridge Regression with Statistical Guarantees}},
year = {2015}
}
@article{Oculo13,
author = {Gresty, M. and Ell, J. and Findley, L.},
journal = {Journal of Neurology, Neurosurgery and Psychiatry},
number = {45},
pages = {431--439},
title = {{Acquired pendular nystagmus: its characteristics, localising value and pathophysiology}},
volume = {May},
year = {1982}
}
@article{DAspremont2007,
archivePrefix = {arXiv},
arxivId = {cs.CE/0406021},
author = {D'Aspremont, Alexandre and {El Ghaoui}, Laurent and Jordan, Michael I. and Lanckriet, Gert R. G.},
doi = {10.1137/050645506},
eprint = {0406021},
file = {:home/tom/Documents/Mendeley/d'Aspremont et al._2007_A Direct Formulation for Sparse PCA Using Semidefinite Programming.pdf:pdf},
isbn = {0036-1445},
issn = {0036-1445},
journal = {SIAM Review},
keywords = {Karhunen–Lo{\`{e}}ve transform,Moreau–Yosida regularization,factor analysis,principal component analysis,semidefinite programming,semidefinite relaxation},
number = {3},
pages = {434--448},
primaryClass = {cs.CE},
title = {{A Direct Formulation for Sparse PCA Using Semidefinite Programming}},
url = {http://epubs.siam.org/doi/abs/10.1137/050645506},
volume = {49},
year = {2007}
}
@inproceedings{zhuang2011two,
address = {Ft. Lauderdale, FL, USA},
author = {Zhuang, Jinfeng and Tsang, Ivor W and Hoi, Steven C H},
booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {909--917},
title = {{Two-Layer Multiple Kernel Learning.}},
year = {2011}
}
@article{Garcia-Cardona2017,
archivePrefix = {arXiv},
arxivId = {1709.02893},
author = {Garcia-Cardona, Cristina and Wohlberg, Brendt},
eprint = {1709.02893},
file = {:home/tom/Documents/Mendeley/Garcia-Cardona, Wohlberg_2017_Convolutional Dictionary Learning.pdf:pdf},
journal = {preprint ArXiv},
title = {{Convolutional Dictionary Learning}},
url = {http://arxiv.org/abs/1709.02893},
volume = {1709.00106},
year = {2017}
}
@article{Li2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1607.05428v1},
author = {Li, Xudong and Sun, Defeng and Toh, Kim-Chuan},
eprint = {arXiv:1607.05428v1},
file = {:home/tom/Documents/Mendeley/Li, Sun, Toh_2016_An efficient linearly convergent semismooth Netwon-CG augmented Lagrangian method for Lasso problems.pdf:pdf},
journal = {preprint ArXiv},
keywords = {()},
title = {{An efficient linearly convergent semismooth Netwon-CG augmented Lagrangian method for Lasso problems}},
volume = {1607.05428},
year = {2016}
}
@article{Oculo19,
author = {Lavery, MA and O'Neill, JF and Chu, FC and Martyn, LJ},
journal = {Ophthalmology},
number = {91},
pages = {425--453},
title = {{Acquired nystagmus in early childhood: a presenting sign of intracranial tumor.}},
volume = {May},
year = {1984}
}
@inproceedings{Hyvarinen2016,
address = {Barcelona, Spain},
archivePrefix = {arXiv},
arxivId = {1605.06336},
author = {Hyvarinen, Aapo and Morioka, Hiroshi},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1605.06336},
file = {:home/tom/Documents/Mendeley/Hyvarinen, Morioka_2016_Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA.pdf:pdf},
issn = {10495258},
pages = {3765--3773},
title = {{Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA}},
url = {http://arxiv.org/abs/1605.06336},
year = {2016}
}
@article{Osborne2000,
author = {Osborne, M and Presnell, B and Turlach, B},
file = {:home/tom/Documents/Mendeley/Osborne, Presnell, Turlach_2000_A new approach to variable selection in least squares problems.pdf:pdf},
journal = {IMA J. Numerical Analysis},
number = {3},
pages = {389--404},
title = {{A new approach to variable selection in least squares problems}},
volume = {20},
year = {2000}
}
@article{hari2006action,
author = {Hari, R},
journal = {Progress in brain research},
pages = {253--260},
publisher = {Elsevier},
title = {{Action--perception connection and the cortical mu rhythm}},
volume = {159},
year = {2006}
}
@article{Zhang2015,
archivePrefix = {arXiv},
arxivId = {1602.07017},
author = {Zhang, Zheng and Xu, Yong and Yang, Jian and Li, Xuelong and Zhang, David},
doi = {10.1109/ACCESS.2015.2430359},
eprint = {1602.07017},
file = {:home/tom/Documents/Mendeley/Zhang et al._2015_A Survey of Sparse Representation Algorithms and Applications.pdf:pdf},
isbn = {2014041717},
issn = {21693536},
journal = {IEEE Access},
keywords = {Sparse representation,compressive sensing,constrained optimization,dictionary learning,greedy algorithm,homotopy algorithm,proximal algorithm},
pages = {490--530},
title = {{A Survey of Sparse Representation: Algorithms and Applications}},
volume = {3},
year = {2015}
}
@article{Moreau2019,
author = {Moreau, Thomas and Gramfort, Alexandre},
journal = {preprint ArXiv (to be submitted)},
title = {{Distributed Convolutional Dictionary Learning (DiCoDiLe): Pattern Discovery in Large Images and Signals}},
year = {2019}
}
@article{Haeffele2015,
archivePrefix = {arXiv},
arxivId = {1506.07540},
author = {Haeffele, Benjamin D. and Vidal, Ren{\'{e}}},
doi = {10.1088/0953-8984/28/3/035305},
eprint = {1506.07540},
file = {:home/tom/Documents/Mendeley/Haeffele, Vidal_2015_Global Optimality in Tensor Factorization, Deep Learning, and Beyond.pdf:pdf},
isbn = {1506.07540},
issn = {1361648X},
journal = {preprint ArXiv},
pmid = {473150},
title = {{Global Optimality in Tensor Factorization, Deep Learning, and Beyond}},
url = {http://arxiv.org/abs/1506.07540},
volume = {1506.07540},
year = {2015}
}
@article{Agarwal2014a,
abstract = {Although neuronal spikes can be readily detected from extracellular recordings, synaptic and subthreshold activity remains undifferentiated within the local field potential (LFP). In the hippocampus, neurons discharge selectively when the rat is at certain locations, while LFPs at single anatomical sites exhibit no such place-tuning. Nonetheless, because the representation of position is sparse and distributed, we hypothesized that spatial information can be recovered from multiple-site LFP recordings. Using high-density sampling of LFP and computational methods, we show that the spatiotemporal structure of the theta rhythm can encode position as robustly as neuronal spiking populations. Because our approach exploits the rhythmicity and sparse structure of neural activity, features found in many brain regions, it is useful as a general tool for discovering distributed LFP codes.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Agarwal, Gautam and Stevenson, Ian H. and Bere{\'{n}}yi, Antal and Mizuseki, Kenji and Buzs{\'{a}}ki, Gy{\"{o}}rgy and Sommer, Friedrich T.},
doi = {10.1126/science.1250444},
eprint = {15334406},
file = {:home/tom/Documents/Mendeley/Agarwal et al._2014_Spatially distributed local fields in the hippocampus encode rat position.pdf:pdf},
isbn = {1095-9203 (Electronic)\n0036-8075 (Linking)},
issn = {10959203},
journal = {Science},
number = {6184},
pages = {626--630},
pmid = {24812401},
title = {{Spatially distributed local fields in the hippocampus encode rat position}},
volume = {344},
year = {2014}
}
@article{Hasson2018,
abstract = {We explore a combinatorial framework which efficiently quantifies the asymmetries between minima and maxima in local fluctuations of time series. We firstly showcase its performance by applying it to a battery of synthetic cases. We find rigorous results on some canonical dynamical models (stochastic processes with and without correlations, chaotic processes) complemented by extensive numerical simulations for a range of processes which indicate that the methodology correctly distinguishes different complex dynamics and outperforms state of the art metrics in several cases. Subsequently, we apply this methodology to real-world problems emerging across several disciplines including cases in neurobiology, finance and climate science. We conclude that differences between the statistics of local maxima and local minima in time series are highly informative of the complex underlying dynamics and a graph-theoretic extraction procedure allows to use these features for statistical learning purposes.},
archivePrefix = {arXiv},
arxivId = {1710.04947},
author = {Hasson, Uri and Iacovacci, Jacopo and Davis, Ben and Flanagan, Ryan and Tagliazucchi, Enzo and Laufs, Helmut and Lacasa, Lucas},
doi = {10.1038/s41598-018-21785-0},
eprint = {1710.04947},
file = {:home/tom/Documents/Mendeley/Hasson et al._2018_A combinatorial framework to quantify peakpit asymmetries in complex dynamics.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--17},
pmid = {29476077},
publisher = {Springer US},
title = {{A combinatorial framework to quantify peak/pit asymmetries in complex dynamics}},
url = {http://dx.doi.org/10.1038/s41598-018-21785-0},
volume = {8},
year = {2018}
}
@article{chandrasekaran2013computational,
author = {Chandrasekaran, Venkat and Jordan, Michael I.},
journal = {Proceedings of the National Academy of Sciences},
number = {13},
pages = {E1181----E1190},
publisher = {National Acad Sciences},
title = {{Computational and statistical tradeoffs via convex relaxation}},
volume = {110},
year = {2013}
}
@article{Giryes2016b,
archivePrefix = {arXiv},
arxivId = {1712.04741},
author = {Vidal, Ren{\'{e}} and Bruna, Joan and Giryes, Raja and Soatto, Stefano},
doi = {10.1007/978-3-540-31299-4},
eprint = {1712.04741},
file = {:home/tom/Documents/Mendeley/Vidal et al._2016_Mathematics of Deep Learning.pdf:pdf},
isbn = {978-3-540-21992-7},
issn = {13534858},
journal = {preprint ArXiv},
title = {{Mathematics of Deep Learning}},
volume = {1712.04741},
year = {2016}
}
@inproceedings{DelAguilaPla2018a,
annote = {Source localization with non-negative CSC + group constraints},
author = {{del Aguila Pla}, Pol and Jalden, Joakim},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2018.8462235},
file = {:home/tom/Documents/Mendeley/del Aguila Pla, Jalden_2018_Convolutional Group-Sparse Coding and Source Localization(2).pdf:pdf},
isbn = {978-1-5386-4658-8},
pages = {2776--2780},
title = {{Convolutional Group-Sparse Coding and Source Localization}},
url = {https://ieeexplore.ieee.org/document/8462235/},
year = {2018}
}
@article{hotelling1933analysis,
author = {Hotelling, Harold},
journal = {Journal of educational psychology},
number = {6},
pages = {417},
publisher = {Warwick \& York},
title = {{Analysis of a complex of statistical variables into principal components.}},
volume = {24},
year = {1933}
}
@article{Barthelemy2013,
abstract = {This article addresses the issue of representing electroencephalographic (EEG) signals in an efficient way. While classical approaches use a fixed Gabor dictionary to analyze EEG signals, this article proposes a data-driven method to obtain an adapted dictionary. To reach an efficient dictionary learning, appropriate spatial and temporal modeling is required. Inter-channels links are taken into account in the spatial multivariate model, and shift-invariance is used for the temporal model. Multivariate learned kernels are informative (a few atoms code plentiful energy) and interpretable (the atoms can have a physiological meaning). Using real EEG data, the proposed method is shown to outperform the classical multichannel matching pursuit used with a Gabor dictionary, as measured by the representative power of the learned dictionary and its spatial flexibility. Moreover, dictionary learning can capture interpretable patterns: this ability is illustrated on real data, learning a P300 evoked potential. {\textcopyright} 2013 Elsevier B.V.},
archivePrefix = {arXiv},
arxivId = {1303.0742},
author = {Barth{\'{e}}lemy, Q. and Gouy-Pailler, C. and Isaac, Y. and Souloumiac, A. and Larue, A. and Mars, J. I.},
doi = {10.1016/j.jneumeth.2013.02.001},
eprint = {1303.0742},
file = {:home/tom/Documents/Mendeley/Barth{\'{e}}lemy et al._2013_Multivariate temporal dictionary learning for EEG.pdf:pdf},
isbn = {0165-0270},
issn = {01650270},
journal = {Journal of Neuroscience Methods},
keywords = {Dictionary learning,EEG,Evoked potentials,Multivariate,Orthogonal matching pursuit,P300,Shift-invariance},
number = {1},
pages = {19--28},
pmid = {23428648},
title = {{Multivariate temporal dictionary learning for EEG}},
volume = {215},
year = {2013}
}
@misc{Neyshabur2017a,
abstract = {In an attempt to better understand generalization in deep learning, we study several possible explanations. We show that implicit regularization induced by the optimization method is playing a key role in generalization and success of deep learning models. Motivated by this view, we study how different complexity measures can ensure generalization and explain how optimization algorithms can implicitly regularize complexity measures. We empirically investigate the ability of these measures to explain different observed phenomena in deep learning. We further study the invariances in neural networks, suggest complexity measures and optimization algorithms that have similar invariances to those in neural networks and evaluate them on a number of learning tasks.},
archivePrefix = {arXiv},
arxivId = {1709.01953},
author = {Neyshabur, Behnam},
eprint = {1709.01953},
file = {:home/tom/Documents/Mendeley/Neyshabur_2017_Implicit Regularization in Deep Learning.pdf:pdf},
howpublished = {PhD Thesis},
title = {{Implicit Regularization in Deep Learning}},
url = {http://arxiv.org/abs/1709.01953},
year = {2017}
}
@inproceedings{Sprechmann2013a,
author = {Sprechmann, Pablo and Litman, Roee and Yakar, TB},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Documents/Mendeley/Sprechmann, Litman, Yakar_2013_Efficient Supervised Sparse Analysis and Synthesis Operators.pdf:pdf},
issn = {10495258},
pages = {908--916},
title = {{Efficient Supervised Sparse Analysis and Synthesis Operators}},
url = {https://www.tau.ac.il/$\sim$roeelitm/SprLitBenBroSapNIPS13.pdf},
year = {2013}
}
@inproceedings{Nutini,
address = {Lille, France},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.00552v1},
author = {Nutini, Julie and Schmidt, Mark and Laradji, Issam H and Friedlander, Michael P. and Koepke, Hoyt},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {arXiv:1506.00552v1},
file = {:home/tom/Documents/Mendeley/Nutini et al._2015_Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection.pdf:pdf},
pages = {1632--1641},
title = {{Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection}},
year = {2015}
}
@inproceedings{Engan1999,
address = {Phoenix, AZ, USA},
author = {Engan, Kjersti and Aase, Sven Ole and Hus{\o}y, John H{\aa}kon},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:home/tom/Documents/Mendeley/Engan, Aase, Hus{\o}y_1999_Method of Optimal Directions for Frame Design.pdf:pdf},
pages = {2443----2446},
title = {{Method of Optimal Directions for Frame Design}},
year = {1999}
}
@misc{ChenYanpingandKeoghEamonnandHuBingandBegumNurjahanandBagnallAnthonyandMueenAbdullahandBatista,
author = {Chen, Yanping and Keogh, Eamonn and Hu, Bing and Begum, Nurjahan and Bagnall, Anthony and Mueen, Abdullah and Batista, Gustavo},
title = {{UCR Time Series Classification/Clustering}},
url = {http://www.cs.ucr.edu/$\sim$eamonn/time_series_data/},
urldate = {2015-09-14},
year = {2015}
}
@inproceedings{Pascanu2014,
abstract = {We study the complexity of functions computable by deep feedforward neural networks with piece-wise linear activations in terms of the number of regions of linearity that they have. Deep networks are able to sequentially map portions of each layer's input space to the same output. In this way, deep models compute functions with a compositional structure that is able to re-use pieces of computation exponentially often in terms of their depth. This note investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piece-wise linear activation functions.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1869v1},
author = {Mont{\'{u}}far, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1007/978-1-4471-5779-3_4},
eprint = {arXiv:1402.1869v1},
file = {:home/tom/Documents/Mendeley//Mont'ufar et al._2014_On the Number of Linear Regions of Deep Neural Networks.pdf:pdf},
isbn = {978-1-4471-5779-3; 978-1-4471-5778-6},
issn = {10495258},
keywords = {deep learning,input space partition,maxout,neural network,rectifier},
pages = {2924--2932},
title = {{On the Number of Linear Regions of Deep Neural Networks}},
year = {2014}
}
@inproceedings{Wang2015a,
archivePrefix = {arXiv},
arxivId = {1509.00153},
author = {Wang, Zhangyang and Ling, Qing and Huang, Thomas S.},
booktitle = {AAAI Conference on Artificial Intelligence},
eprint = {1509.00153},
file = {:home/tom/Documents/Mendeley/Wang, Ling, Huang_2016_Learning Deep $ell_0$ Encoders.pdf:pdf},
pages = {2194--2200},
title = {{Learning Deep $\ell_0$ Encoders}},
url = {http://arxiv.org/abs/1509.00153},
year = {2016}
}
@article{Agarwal2013,
archivePrefix = {arXiv},
arxivId = {1309.1952},
author = {Agarwal, Alekh and Anandkumar, Animashree and Netrapalli, Praneeth},
eprint = {1309.1952},
file = {:home/tom/Documents/Mendeley/Agarwal, Anandkumar, Netrapalli_2013_A Clustering Approach to Learn Sparsely-Used Overcomplete Dictionaries.pdf:pdf},
journal = {preprint ArXiv},
keywords = {dictionary learning,incoherence,lasso,overcomplete dictionaries,sparse coding},
title = {{A Clustering Approach to Learn Sparsely-Used Overcomplete Dictionaries}},
url = {http://arxiv.org/abs/1309.1952},
volume = {1309.1952},
year = {2013}
}
@inproceedings{Sprechmann2012,
address = {Edinburgh, Great Britain},
archivePrefix = {arXiv},
arxivId = {1206.4649},
author = {Sprechmann, Pablo and Bronstein, Alex and Sapiro, Guillermo},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1206.4649},
file = {:home/tom/Documents/Mendeley//Sprechmann, Bronstein, Sapiro_2012_Learning Efficient Structured Sparse Models.pdf:pdf},
isbn = {978-1-4503-1285-1},
pages = {615--622},
title = {{Learning Efficient Structured Sparse Models}},
year = {2012}
}
@article{Richtarik2012,
archivePrefix = {arXiv},
arxivId = {1212.0873},
author = {Richt{\'{a}}rik, Peter and Tak{\'{a}}{\v{c}}, Martin},
doi = {10.1007/s10107-015-0901-6},
eprint = {1212.0873},
file = {:home/tom/Documents/Mendeley/Richt{\'{a}}rik, Tak{\'{a}}{\v{c}}_2012_Parallel coordinate descent methods for big data optimization.pdf:pdf},
isbn = {1010701509},
issn = {14364646},
journal = {preprint ArXiv},
keywords = {big data optimization,composite ob-,convex optimization,expected separable over-approximation,huge-,iteration complexity,jective,lasso,parallel coordinate descent,partial separability,scale optimization},
title = {{Parallel coordinate descent methods for big data optimization}},
url = {http://arxiv.org/abs/1212.0873},
volume = {1212.0873},
year = {2012}
}
@article{Mairal2009,
abstract = {Sparse coding�that is, modelling data vectors as\n\nsparse linear combinations of basis elements�is\n\nwidely used in machine learning, neuroscience,\n\nsignal processing, and statistics. This paper focuses\n\non learning the basis set, also called dictionary,\n\nto adapt it to specific data, an approach\n\nthat has recently proven to be very effective for\n\nsignal reconstruction and classification in the audio\n\nand image processing domains. This paper\n\nproposes a new online optimization algorithm\n\nfor dictionary learning, based on stochastic approximations,\n\nwhich scales up gracefully to large\n\ndatasets with millions of training samples. A\n\nproof of convergence is presented, along with\n\nexperiments with natural images demonstrating\n\nthat it leads to faster performance and better dictionaries\n\nthan classical batch algorithms for both\n\nsmall and large datasets.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {0908.0050},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
doi = {http://doi.acm.org/10.1145/1553374.1553463},
eprint = {0908.0050},
file = {:home/tom/Documents/Mendeley/Mairal et al._2009_Online dictionary learning for sparse coding.pdf:pdf},
isbn = {978-1-60558-516-1},
issn = {0016450X},
journal = {International Conference on Machine Learning (ICML)},
keywords = {Learning,Machine Learning,Optimization and Control},
number = {September},
pages = {689--696},
pmid = {710806},
title = {{Online dictionary learning for sparse coding}},
url = {http://arxiv.org/abs/0908.0050},
year = {2009}
}
@article{Trench2003,
author = {Trench, William F},
file = {:home/tom/Documents/Mendeley/Trench_2003_Absolute equal distribution of families of finite sets.pdf:pdf},
journal = {Linear algebra and its applications},
keywords = {absolutely equally distributed,e-mail,edu,eigenvalues,equally distributed,singular values,trinity,wtrench},
pages = {131--146},
title = {{Absolute equal distribution of families of finite sets}},
volume = {367},
year = {2003}
}
@article{Papyan2016a,
archivePrefix = {arXiv},
arxivId = {1607.02005},
author = {Papyan, Vardan and Sulam, Jeremias and Elad, Michael},
eprint = {1607.02005},
file = {:home/tom/Documents/Mendeley//Papyan, Sulam, Elad_2016_Working Locally Thinking Globally - Part II Theoretical Guarantees for Convolutional Sparse Coding.pdf:pdf},
journal = {preprint ArXiv},
title = {{Working Locally Thinking Globally - Part II: Theoretical Guarantees for Convolutional Sparse Coding}},
volume = {1607.02009},
year = {2016}
}
@article{Adler2017,
archivePrefix = {arXiv},
arxivId = {1710.10898},
author = {Adler, Jonas and Ringh, Axel and {\"{O}}ktem, Ozan and Karlsson, Johan},
eprint = {1710.10898},
file = {:home/tom/Documents/Mendeley/Adler et al._2017_Learning to solve inverse problems using Wasserstein loss.pdf:pdf},
journal = {preprint ArXiv},
title = {{Learning to solve inverse problems using Wasserstein loss}},
url = {http://arxiv.org/abs/1710.10898},
volume = {1710.10898},
year = {2017}
}
@inproceedings{Yang2011,
address = {Barcelona, Spain},
author = {Yang, Meng and Zhang, L},
booktitle = {International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2011.6126286},
file = {:home/tom/Documents/Mendeley/Yang, Zhang_2011_Fisher discrimination dictionary learning for sparse representation.pdf:pdf},
isbn = {978-1-4577-1102-2},
pages = {543--550},
publisher = {IEEE},
title = {{Fisher discrimination dictionary learning for sparse representation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6126286%5Cnhttp://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6126286},
year = {2011}
}
@article{Group2001,
author = {Group, CW},
journal = {The National Eye Institute Publications},
title = {{A national eye institute sponsored workshop and publication on the classification of eye movement abnormalities and strabismus (CEMAS)}},
year = {2001}
}
@article{Combettes2008,
archivePrefix = {arXiv},
arxivId = {0807.2617},
author = {Combettes, Patrick L and Pesquet, Jean-Christophe},
doi = {10.1088/0266-5611/24/6/065014},
eprint = {0807.2617},
file = {:home/tom/Documents/Mendeley//Combettes, Pesquet_2008_A Proximal Decomposition Method for Solving Convex Variational Inverse Problems.pdf:pdf},
issn = {0266-5611, 1361-6420},
journal = {Inverse Problems},
keywords = {Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
number = {6},
pages = {65014},
title = {{A Proximal Decomposition Method for Solving Convex Variational Inverse Problems}},
url = {http://arxiv.org/abs/0807.2617},
volume = {24},
year = {2008}
}
@article{Duval2013,
archivePrefix = {arXiv},
arxivId = {1306.6909},
author = {Duval, Vincent and Peyr{\'{e}}, Gabriel},
eprint = {1306.6909},
file = {:home/tom/Documents/Mendeley/Duval, Peyr{\'{e}}_2013_Exact support recovery for sparse spikes deconvolution.pdf:pdf},
issn = {00405736},
journal = {preprint ArXiv},
keywords = {Deconvolution,transport},
mendeley-tags = {Deconvolution,transport},
title = {{Exact support recovery for sparse spikes deconvolution}},
volume = {1306.6909},
year = {2013}
}
@inproceedings{Caruana2001,
address = {Vancouver, Canada},
author = {Caruana, Rich and Lawrence, Steve and Giles, Lee},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Documents/Mendeley/Caruana, Lawrence, Giles_2001_Overfitting in Neural Nets Backpropagation, Conjugate Gradient, and Early Stopping.pdf:pdf},
pages = {402--408},
title = {{Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping}},
year = {2001}
}
@article{Scheinberg2013,
archivePrefix = {arXiv},
arxivId = {1311.6547},
author = {Scheinberg, Katya and Tang, Xiaocheng},
eprint = {1311.6547},
file = {:home/tom/Documents/Mendeley/Scheinberg, Tang_2013_Practical Inexact Proximal Quasi-Newton Method with Global Complexity Analysis.pdf:pdf;:home/tom/Documents/Mendeley//Scheinberg, Tang_2013_Practical Inexact Proximal Quasi-Newton Method with Global Complexity Analysis.pdf:pdf},
journal = {preprint ArXiv},
title = {{Practical Inexact Proximal Quasi-Newton Method with Global Complexity Analysis}},
volume = {1311.6547},
year = {2013}
}
@article{Mairal2010,
archivePrefix = {arXiv},
arxivId = {0908.0050},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
eprint = {0908.0050},
file = {:home/tom/Documents/Mendeley//Mairal et al._2010_Online Learning for Matrix Factorization and Sparse Coding.pdf:pdf},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {Deconvolution,basis pursuit,dictionary learning,factorization,ing,matrix factorization,negative matrix factorization,non,online,online learning,sparse cod,sparse coding,sparse principal component analysis,stochastic approximations,stochastic optimization,transport},
mendeley-tags = {Deconvolution,transport},
number = {1},
pages = {19--60},
title = {{Online Learning for Matrix Factorization and Sparse Coding}},
volume = {11},
year = {2010}
}
@article{Richtarik2011,
abstract = {In this paper we develop a randomized block-coordinate descent method for minimizing the sum of a smooth and a simple nonsmooth block-separable convex function and prove that it obtains an $\epsilon$-accurate solution with probability at least $1-\rho$ in at most $O(\tfrac{n}{\epsilon} \log \tfrac{1}{\rho})$ iterations, where $n$ is the number of blocks. For strongly convex functions the method converges linearly. This extends recent results of Nesterov [Efficiency of coordinate descent methods on huge-scale optimization problems, CORE Discussion Paper #2010/2], which cover the smooth case, to composite minimization, while at the same time improving the complexity by the factor of 4 and removing $\epsilon$ from the logarithmic term. More importantly, in contrast with the aforementioned work in which the author achieves the results by applying the method to a regularized version of the objective function with an unknown scaling factor, we show that this is not necessary, thus achieving true iteration complexity bounds. In the smooth case we also allow for arbitrary probability vectors and non-Euclidean norms. Finally, we demonstrate numerically that the algorithm is able to solve huge-scale $\ell_1$-regularized least squares and support vector machine problems with a billion variables.},
archivePrefix = {arXiv},
arxivId = {1107.2848},
author = {Richt{\'{a}}rik, Peter and Tak{\'{a}}{\v{c}}, Martin},
eprint = {1107.2848},
file = {:home/tom/Documents/Mendeley/Richt{\'{a}}rik, Tak{\'{a}}{\v{c}}_2014_Iteration Complexity of Randomized Block-Coordinate Descent Methods for Minimizing a Composite Function.pdf:pdf},
journal = {Mathematical Programming},
keywords = {alternating minimization,block coordinate descent,composite minimization,convex optimization,coor-,dinate relaxation,iteration complexity,l1-regularization,large scale,support vector machines},
number = {1-2},
pages = {1--38},
title = {{Iteration Complexity of Randomized Block-Coordinate Descent Methods for Minimizing a Composite Function}},
url = {http://arxiv.org/abs/1107.2848},
volume = {144},
year = {2014}
}
@inproceedings{Lee2007,
abstract = {Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that cap- ture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimiza- tion problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field sur- round suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons. 1},
address = {Vancouver, Canada},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03733v1},
author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew Y},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1.1.69.2112},
eprint = {arXiv:1506.03733v1},
file = {:home/tom/Documents/Mendeley/Lee et al._2007_Efficient Sparse coding algorithms.pdf:pdf},
isbn = {0262195682},
issn = {10495258},
keywords = {Stanford University},
pages = {801--808},
pmid = {17051527},
title = {{Efficient Sparse coding algorithms}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.2112&rep=rep1&type=pdf%5Cnhttp://books.nips.cc/papers/txt/nips19/NIPS2006_0878.txt},
year = {2007}
}
@article{Sakoe1978,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sakoe, Hiroaki and Chiba, Seibi},
doi = {10.1109/TASSP.1978.1163055},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/Mendeley/Sakoe, Chiba_1978_Dynamic Programming Algorithm Optimization for Spoken Word Recognition.pdf:pdf},
isbn = {9788578110796},
issn = {00963518},
journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
number = {1},
pages = {43--49},
pmid = {25246403},
title = {{Dynamic Programming Algorithm Optimization for Spoken Word Recognition}},
volume = {26},
year = {1978}
}
@inproceedings{Freeman2017,
address = {Toulon, France},
archivePrefix = {arXiv},
arxivId = {1611.01540},
author = {Freeman, C. Daniel and Bruna, Joan},
booktitle = {International Conference on Learning Representation (ICLR)},
eprint = {1611.01540},
file = {:home/tom/Documents/Mendeley/Freeman, Bruna_2017_Topology and Geometry of Deep Rectified Network Optimization Landscapes.pdf:pdf},
title = {{Topology and Geometry of Deep Rectified Network Optimization Landscapes}},
url = {http://arxiv.org/abs/1611.01540},
year = {2017}
}
@article{Cherfaoui2018,
archivePrefix = {arXiv},
arxivId = {1812.07201},
author = {Cherfaoui, Farah and Emiya, Valentin and Ralaivola, Liva and Anthoine, Sandrine},
eprint = {1812.07201},
file = {:home/tom/Documents/Mendeley/Cherfaoui et al._2018_Frank-Wolfe Algorithm for the Exact Sparse Problem.pdf:pdf},
journal = {preprint ArXiv},
title = {{Frank-Wolfe Algorithm for the Exact Sparse Problem}},
url = {https://hal-amu.archives-ouvertes.fr/hal-01881329},
volume = {1812.07201},
year = {2018}
}
@inproceedings{marschollek2008performance,
address = {Vancouver, Canada},
author = {Marschollek, M and Goevercin, M and Wolf, K.-H. and Song, B and Gietzelt, M and Haux, R and Steinhagen-Thiessen, E},
booktitle = {Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBS)},
pages = {1319--1322},
title = {{A performance comparison of accelerometry-based step detection algorithms on a large, non-laboratory sample of healthy and mobility-impaired persons}},
year = {2008}
}
@inproceedings{Zhu2010,
abstract = {ABSTRACT A novel audio fingerprinting method that is highly robust to Time Scale Modification (TSM) and pitch shifting is pro- posed. Instead of simply employing spectral or tempo-related features, our system is based on computer-vision techniques. We transform each 1-D ...},
address = {Singapore, Singapore},
author = {Zhu, Bilei and Li, Wei and Wang, Zhurong and Xue, Xiangyang},
booktitle = {International Conference on Multimedia (MM)},
doi = {10.1145/1873951.1874130},
file = {:home/tom/Documents/Mendeley/Zhu et al._2010_A novel audio fingerprinting method robust to time scale modification and pitch shifting.pdf:pdf},
isbn = {9781605589336},
keywords = {audio fingerprinting,pitch shifting,robustness,time scale modification},
pages = {987},
title = {{A novel audio fingerprinting method robust to time scale modification and pitch shifting}},
url = {http://dl.acm.org/citation.cfm?doid=1873951.1874130},
year = {2010}
}
@inproceedings{Gregor10,
author = {Gregor, Karol and {Le Cun}, Yann},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:home/tom/Documents/Mendeley//Gregor, Le Cun_2010_Learning Fast Approximations of Sparse Coding.pdf:pdf},
pages = {399--406},
title = {{Learning Fast Approximations of Sparse Coding}},
year = {2010}
}
@inproceedings{Adler2013,
address = {Southampton, United Kingdom},
author = {Adler, Amir and Elad, Michael and Hel-Or, Yacov and Rivlin, Ehud},
booktitle = {IEEE International Workshop on Machine Learning for Signal Processing (MLSP)},
pages = {22 -- 25},
title = {{Sparse Coding with Anomaly Detection}},
year = {2013}
}
@inproceedings{Oudre2015,
address = {Lyon, France},
author = {Oudre, Laurent and Moreau, Thomas and Truong, Charles and Barrois-M{\"{u}}ller, R{\'{e}}mi and Dadashi, Robert and Gr{\'{e}}gory, Thomas},
booktitle = {Groupe de Recherche et d'Etudes en Traitement du Signal et des Images (GRETSI)},
file = {:home/tom/Documents/Mendeley/Oudre et al._2015_D{\'{e}}tection de pas {\`{a}} partir de donn{\'{e}}es d'acc{\'{e}}l{\'{e}}rom{\'{e}}trie.pdf:pdf},
title = {{D{\'{e}}tection de pas {\`{a}} partir de donn{\'{e}}es d'acc{\'{e}}l{\'{e}}rom{\'{e}}trie}},
year = {2015}
}
@article{Gordon1988,
author = {Gordon, Y.},
doi = {10.1007/BFb0081732},
file = {:home/tom/Documents/Mendeley/Gordon_1988_On Milman's inequality and random subspaces which escape through a mesh in Rn.pdf:pdf},
isbn = {978-3-540-19353-1},
journal = {Geometric Aspects of Functional Analysis},
keywords = {Mathematics and Statistics},
pages = {84--106},
title = {{On Milman's inequality and random subspaces which escape through a mesh in Rn}},
url = {http://www.springerlink.com/content/8248580l241p33vt/},
volume = {1317},
year = {1988}
}
@article{Bozzo2010,
author = {Bozzo, Enrico and Carniel, Roberto and Fasino, Dario},
doi = {10.1016/j.camwa.2010.05.028},
file = {:home/tom/Documents/Mendeley/Bozzo, Carniel, Fasino_2010_Relationship between Singular Spectrum Analysis and Fourier analysis Theory and application to the monitorin.pdf:pdf},
issn = {08981221},
journal = {Computers and Mathematics with Applications},
keywords = {Fourier analysis,Singular Spectrum Analysis,Time series,Toeplitz matrices,Volcanic tremor},
number = {3},
pages = {812--820},
publisher = {Elsevier Ltd},
title = {{Relationship between Singular Spectrum Analysis and Fourier analysis: Theory and application to the monitoring of volcanic activity}},
url = {http://dx.doi.org/10.1016/j.camwa.2010.05.028},
volume = {60},
year = {2010}
}
@article{Rozell2008,
abstract = {While evidence indicates that neural systems may be employing sparse approximations to represent sensed stimuli, the mechanisms underlying this ability are not understood. We describe a locally competitive algorithm (LCA) that solves a collection of sparse coding principles minimizing a weighted combination of mean-squared error and a coefficient cost function. LCAs are designed to be implemented in a dynamical system composed of many neuron-like elements operating in parallel. These algorithms use thresholding functions to induce local (usually one-way) inhibitory competitions between nodes to produce sparse representations. LCAs produce coefficients with sparsity levels comparable to the most popular centralized sparse coding algorithms while being readily suited for neural implementation. Additionally, LCA coefficients for video sequences demonstrate inertial properties that are both qualitatively and quantitatively more regular (i.e., smoother and more predictable) than the coefficients produced by greedy algorithms.},
author = {Rozell, Christopher J and Johnson, Don H and Baraniuk, Richard G and Olshausen, Bruno A.},
doi = {10.1162/neco.2008.03-07-486},
file = {:home/tom/Documents/Mendeley/Rozell et al._2008_Sparse coding via thresholding and local competition in neural circuits.pdf:pdf},
isbn = {9781424495290},
issn = {08997667},
journal = {Neural Computation},
keywords = {algorithms,models,neurological,neurons,neurons physiology},
number = {10},
pages = {2526--63},
pmid = {18439138},
title = {{Sparse coding via thresholding and local competition in neural circuits.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18439138%5Cnhttp://redwood.berkeley.edu/bruno/papers/rozell-sparse-coding-nc08.pdf},
volume = {20},
year = {2008}
}
@article{Cho2015,
archivePrefix = {arXiv},
arxivId = {1507.01053},
author = {Cho, Kyunghyun and Courville, Aaron and Bengio, Yoshua},
doi = {10.1109/TMM.2015.2477044},
eprint = {1507.01053},
file = {:home/tom/Documents/Mendeley/Cho, Courville, Bengio_2015_Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks.pdf:pdf},
isbn = {978-1-60558-907-7},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Attention mechanism,deep learning,recurrent neural networks},
number = {11},
pages = {1875--1886},
title = {{Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks}},
volume = {17},
year = {2015}
}
@article{Galiana1991,
author = {Galiana, Henrietta L.},
doi = {10.1109/10.81578},
file = {:home/tom/Documents/Mendeley/Galiana_1991_A nystagmus strategy to linearize the vestibulo-ocular reflex.pdf:pdf},
isbn = {0018929419},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
number = {6},
pages = {532--543},
pmid = {1879842},
title = {{A nystagmus strategy to linearize the vestibulo-ocular reflex}},
volume = {38},
year = {1991}
}
@article{Chaudhari2017a,
archivePrefix = {arXiv},
arxivId = {arXiv:1704.04932v2},
author = {Chaudhari, Pratik and Oberman, Adam and Osher, Stanley and Soatto, Stefano and Carlier, Guillaume},
eprint = {arXiv:1704.04932v2},
file = {:home/tom/Documents/Mendeley/Chaudhari et al._2017_Deep relaxation partial differential equations for optimizing deep neural networks.pdf:pdf},
journal = {preprint ArXiv},
keywords = {deep learning,entropy,inf-convolution,local,local convexity,neural networks,non-convex optimization,optimal control,partial differential equations,proximal,regularization,smoothing,stochastic gradient descent,viscous burgers},
title = {{Deep relaxation: partial differential equations for optimizing deep neural networks}},
volume = {1704.04932},
year = {2017}
}
@article{Cadieu2005,
author = {Cadieu, Charles F. and Olshausen, Bruno A.},
file = {:home/tom/Documents/Mendeley//Cadieu, Olshausen_2005_Learning Invariant and Variant Components of Time Varying Natural Images Using a Sparse , Multiplicative Model.pdf:pdf},
journal = {Journal of Vision},
number = {June},
pages = {1247--1283},
title = {{Learning Invariant and Variant Components of Time Varying Natural Images Using a Sparse , Multiplicative Model}},
volume = {12},
year = {2005}
}
@article{Bouchard2017,
abstract = {The concept of sparsity has proven useful to understanding elementary neural computations in sensory systems. However, the role of sparsity in motor regions is poorly understood. Here, we investigated the functional properties of sparse structure in neural activity collected with high-density electrocorticography (ECoG) from speech sensorimotor cortex (vSMC) in neurosurgical patients. Using independent components analysis (ICA), we found individual components corresponding to individual major oral articulators (i.e., Coronal Tongue, Dorsal Tongue, Lips), which were selectively activated during utterances that engaged that articulator on single trials. Some of the components corresponded to spatially sparse activations. Components with similar properties were also extracted using convolutional sparse coding (CSC), and required less data pre-processing. Finally, individual utterances could be accurately decoded from vSMC ECoG recordings using linear classifiers trained on the high-dimensional sparse codes generated by CSC. Together, these results suggest that sparse coding may be an important framework and tool for understanding sensory-motor activity generating complex behaviors, and may be useful for brain-machine interfaces.},
author = {Bouchard, Kristofer E. and Bujan, Alejandro F. and Chang, Edward F. and Sommer, Friedrich T.},
doi = {10.1109/EMBC.2017.8037645},
file = {:home/tom/Documents/Mendeley/Bouchard et al._2017_Sparse coding of ECoG signals identifies interpretable components for speech control in human sensorimotor cortex.pdf:pdf},
isbn = {9781509028092},
issn = {1557170X},
journal = {Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
keywords = {ICA,Neural signals - Blind source separation (PCA},
pages = {3636--3639},
pmid = {29060686},
title = {{Sparse coding of ECoG signals identifies interpretable components for speech control in human sensorimotor cortex}},
year = {2017}
}
@article{Barron1993,
author = {Barron, Andrew},
file = {:home/tom/Documents/Mendeley/Barron_1993_Universal Approximation Bounds for Superpositions of a Sigmoidal Function.pdf:pdf},
journal = {IEEE Transaction on Information Theory},
number = {3},
pages = {930--945},
title = {{Universal Approximation Bounds for Superpositions of a Sigmoidal Function}},
volume = {39},
year = {1993}
}
@article{andreao2007combining,
author = {Andre{\~{a}}o, R V and Boudy, J},
journal = {EURASIP Journal on Applied Signal Processing},
number = {1},
pages = {95--103},
title = {{Combining wavelet transform and hidden Markov models for ECG segmentation}},
volume = {2007},
year = {2007}
}
@article{Oculo15,
author = {Huang, MY and CC, Chen and Huber-Reggi, SP and Neuhauss, SC and Straumann, D.},
journal = {Annals of the New-York Academy of Science},
number = {1233},
pages = {285--291},
title = {{Comparison of infantile nystagmus syndrome in achiasmatic zebrafish and humans}},
volume = {Sep},
year = {2011}
}
@misc{Werbos1982,
author = {Werbos, PaulJ.},
booktitle = {System Modeling and Optimization},
doi = {10.1007/BFb0006203},
file = {:home/tom/Documents/Mendeley/Werbos_1982_Applications of advances in nonlinear sensitivity analysis.pdf:pdf},
isbn = {978-3-540-11691-2},
pages = {762--770},
title = {{Applications of advances in nonlinear sensitivity analysis}},
volume = {38},
year = {1982}
}
@article{Cavazza2017,
archivePrefix = {arXiv},
arxivId = {1710.05092},
author = {Cavazza, Jacopo and Morerio, Pietro and Haeffele, Benjamin and Lane, Connor and Murino, Vittorio and Vidal, Ren{\'{e}}},
eprint = {1710.05092},
file = {:home/tom/Documents/Mendeley/Cavazza et al._2017_Dropout as a Low-Rank Regularizer for Matrix Factorization.pdf:pdf},
number = {2},
title = {{Dropout as a Low-Rank Regularizer for Matrix Factorization}},
url = {http://arxiv.org/abs/1710.05092},
year = {2017}
}
@inproceedings{Bengio2007,
address = {Vancouver, Canada},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {citeulike-article-id:4640046},
eprint = {0500581},
file = {:home/tom/Documents/Mendeley/Bengio et al._2007_Greedy Layer-Wise Training of Deep Networks.pdf:pdf},
isbn = {0262195682},
issn = {01628828},
pages = {153--160},
pmid = {19018704},
primaryClass = {submit},
title = {{Greedy Layer-Wise Training of Deep Networks}},
url = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
year = {2007}
}
@article{hinton2006reducing,
author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
journal = {Science},
number = {5786},
pages = {504--507},
publisher = {American Association for the Advancement of Science},
title = {{Reducing the dimensionality of data with neural networks}},
volume = {313},
year = {2006}
}
@inproceedings{Dauphin2014,
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1406.2572},
author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1406.2572},
file = {:home/tom/Documents/Mendeley/Dauphin et al._2014_Identifying and attacking the saddle point problem in high-dimensional non-convex optimization.pdf:pdf},
isbn = {1406.2572},
issn = {10495258},
pages = {2933--2941},
title = {{Identifying and attacking the saddle point problem in high-dimensional non-convex optimization}},
url = {http://arxiv.org/abs/1406.2572},
year = {2014}
}
@inproceedings{Dantas2018,
address = {Calgary, AB, Canada},
author = {Dantas, Cassio Fraga and Gribonval, R{\'{e}}mi},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:home/tom/Documents/Mendeley/Dantas, Gribonval_2018_Faster and still safe combining screening techniques and strucutred dictionaries to accelerate the Lasso.pdf:pdf},
isbn = {9781538646588},
pages = {4069--4073},
title = {{Faster and still safe: combining screening techniques and strucutred dictionaries to accelerate the Lasso}},
year = {2018}
}
@article{Su2016,
abstract = {We derive a second-order ordinary differential equation (ODE) which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.},
archivePrefix = {arXiv},
arxivId = {1503.01243},
author = {Su, Weijie and Boyd, Stephen and Candes, Emmanuel J.},
doi = {10.1017/S1446788715000774},
eprint = {1503.01243},
file = {:home/tom/Documents/Mendeley/Su, Boyd, Candes_2016_A Differential Equation for Modeling Nesterov's Accelerated Gradient Method Theory and Insights.pdf:pdf},
issn = {10495258},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {convex optimization,differential equation,first-order methods,nesterov,restarting,s accelerated scheme},
number = {153},
pages = {1--43},
title = {{A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights}},
url = {http://arxiv.org/abs/1503.01243},
volume = {17},
year = {2016}
}
@article{Bellman1952,
author = {Bellman, Richard},
doi = {10.1073/pnas.38.8.716},
file = {:home/tom/Documents/Mendeley/Bellman_1952_On the Theory of Dynamic Programming.pdf:pdf},
isbn = {00278424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {8},
pages = {716--719},
pmid = {16589166},
title = {{On the Theory of Dynamic Programming}},
volume = {38},
year = {1952}
}
@article{Haeffele2017a,
archivePrefix = {arXiv},
arxivId = {1708.07850},
author = {Haeffele, Benjamin D. and Vidal, Ren{\'{e}}},
eprint = {1708.07850},
file = {:home/tom/Documents/Mendeley/Haeffele, Vidal_2017_Structured Low-Rank Matrix Factorization Global Optimality, Algorithms, and Applications.pdf:pdf},
journal = {preprint ArXiv},
title = {{Structured Low-Rank Matrix Factorization: Global Optimality, Algorithms, and Applications}},
url = {http://arxiv.org/abs/1708.07850},
volume = {1708.07850},
year = {2017}
}
@article{Donoho2002,
author = {Donoho, David L and Elad, Michael},
file = {:home/tom/Documents/Mendeley/Donoho, Elad_2002_Maximal Sparsity Representation via l 1 Minimization.pdf:pdf},
journal = {submitted to IEEE Transactions on Information Theory},
keywords = {atomic decomposition,basic pursuit,convex optimization,linear,matching pursuit,programming,sparse representation},
pages = {1--28},
title = {{Maximal Sparsity Representation via l 1 Minimization}},
year = {2002}
}
@article{Coles2013,
author = {Coles, Lisa D and Patterson, Edward E and Sheffield, W Douglas and Mavoori, Jaideep and Higgins, Jason and Michael, Bland and Leyde, Kent and Cloyd, James C and Litt, Brian and Vite, Charles},
journal = {Epilepsy research},
number = {3},
pages = {456--460},
title = {{Feasibility study of a caregiver seizure alert system in canine epilepsy}},
volume = {106},
year = {2013}
}
@article{Gorodnitsky1997,
author = {Gorodnitsky, Irina F. and Rao, Bhaskar D.},
doi = {10.1109/78.558475},
file = {:home/tom/Documents/Mendeley/Gorodnitsky, Rao_1997_Sparse signal reconstruction from limited data using FOCUSS A re-weighted minimum norm algorithm.pdf:pdf},
isbn = {1053-587X},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
number = {3},
pages = {600--616},
pmid = {1000198971},
title = {{Sparse signal reconstruction from limited data using FOCUSS: A re-weighted minimum norm algorithm}},
volume = {45},
year = {1997}
}
@article{Efron2004,
abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0406456v2},
author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
doi = {10.1214/009053604000000067},
eprint = {0406456v2},
file = {:home/tom/Documents/Mendeley/Efron et al._2004_Least angle regression.pdf:pdf},
isbn = {0090-5364},
issn = {1095-9572},
journal = {The Annals of statistics},
keywords = {and phrases,boosting,coefficient paths,lasso,linear regression,variable selection},
number = {2},
pages = {407--499},
pmid = {20472078},
primaryClass = {arXiv:math},
title = {{Least angle regression}},
url = {http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1083178935%5Cnhttp://projecteuclid.org/DPubS?verb=Display&version=1.0&service=UI&handle=euclid.aos/1083178935&page=record%5Cnhttp://projecteuclid.org/euclid.aos/108317893},
volume = {32},
year = {2004}
}
@inproceedings{Rudi2015,
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1507.04717},
author = {Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1507.04717},
file = {:home/tom/Documents/Mendeley//Rudi, Camoriano, Rosasco_2015_Less is More Nystr{\"{o}}m Computational Regularization.pdf:pdf},
pages = {1657--1665},
title = {{Less is More: Nystr{\"{o}}m Computational Regularization}},
url = {http://arxiv.org/abs/1507.04717},
year = {2015}
}
@article{Friedlander2016,
archivePrefix = {arXiv},
arxivId = {1603.05719},
author = {Friedlander, Michael P and Goh, Gabriel},
eprint = {1603.05719},
journal = {preprint ArXiv},
title = {{Efficient evaluation of scaled proximal operators}},
volume = {1603.05719},
year = {2016}
}
@article{Zou2006,
abstract = {Principal component analysis (PCA) is widely used in data processing and dimension- ality reduction.However,PCAsuffers from the fact that each principal component is a linear combination of all the original variables, thus it is often difficult to interpret the results.We introduce a newmethod called sparse principal component analysis (SPCA) using the lasso (elastic net) to produce modified principal components with sparse loadings.We first show that PCA can be formulated as a regression-type optimization problem; sparse loadings are then obtained by imposing the lasso (elastic net) constraint on the regression coefficients. Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data and gene expression arrays. We also give a new formula to compute the total variance of modified principal components. As illustrations, SPCA is applied to real and simulated data with encouraging results. Key},
archivePrefix = {arXiv},
arxivId = {1205.0121v2},
author = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1198/106186006X113430},
eprint = {1205.0121v2},
file = {:home/tom/Documents/Mendeley/Zou, Hastie, Tibshirani_2006_Sparse Principal Component Analysis.pdf:pdf},
isbn = {106186006X},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {arrays,elastic net,gene expression,lasso,multivariate analysis,singular,thresholding,value decomposition},
number = {2},
pages = {265--286},
pmid = {21811560},
title = {{Sparse Principal Component Analysis}},
volume = {15},
year = {2006}
}
@article{Daubechies2004,
archivePrefix = {arXiv},
arxivId = {math/0307152},
author = {Daubechies, Ingrid and Defrise, Michel and {De Mol}, Christine},
doi = {10.1002/cpa.20042},
eprint = {0307152},
file = {:home/tom/Documents/Mendeley/Daubechies, Defrise, De Mol_2004_An iterative thresholding algorithm for linear inverse problems with a sparsity constraint.pdf:pdf},
isbn = {0010-3640},
issn = {00103640},
journal = {Communications on Pure and Applied Mathematics},
number = {11},
pages = {1413--1457},
primaryClass = {math},
title = {{An iterative thresholding algorithm for linear inverse problems with a sparsity constraint}},
volume = {57},
year = {2004}
}
@article{Oculo31,
author = {Thompson, DA and Liasis, A},
journal = {Journal of Pediatric Ophthalmology and Strabismus},
pages = {55--62},
title = {{Visual electrophysiology: how it can help you and your patient}},
volume = {4},
year = {2012}
}
@inproceedings{Chaudhari2017,
address = {Toulon, France},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.01838v5},
author = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and Lecun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
booktitle = {International Conference on Learning Representation (ICLR)},
eprint = {arXiv:1611.01838v5},
file = {:home/tom/Documents/Mendeley/Chaudhari et al._2017_Entropy-SGD Biasing Gradient Descent into Wide Valleys.pdf:pdf},
title = {{Entropy-SGD: Biasing Gradient Descent into Wide Valleys}},
year = {2017}
}
@inproceedings{Bradley2011,
address = {Bellevue, WA, USA},
archivePrefix = {arXiv},
arxivId = {1105.5379},
author = {Bradley, Joseph K. and Kyrola, Aapo and Bickson, Danny and Guestrin, Carlos},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1105.5379},
file = {:home/tom/Documents/Mendeley/Bradley et al._2011_Parallel Coordinate Descent for $ell_1$-Regularized Loss Minimization.pdf:pdf},
pages = {321--328},
title = {{Parallel Coordinate Descent for $\ell_1$-Regularized Loss Minimization}},
year = {2011}
}
@article{Tibshirani2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.2234v2},
author = {Tibshirani, Robert and Bien, Jacob and Friedman, Jerome and Hastie, Trevor and Simon, Noah and Taylor, Jonathan and Tibshirani, Ryan},
doi = {10.2307/41430939},
eprint = {arXiv:1011.2234v2},
file = {:home/tom/Documents/Mendeley/Tibshirani et al._2012_Strong rules for discarding predictors in Lasso- type problems.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (statistical methodology)},
number = {2},
pages = {245--266},
title = {{Strong rules for discarding predictors in Lasso- type problems}},
volume = {74},
year = {2012}
}
@article{Jolliffe2003,
abstract = {In many multivariate statistical techniques, a set of linear functions of the original p variables is produced. One of the more dif cult aspects of these techniques is the inter-pretation of the linear functions, as these functions usually have nonzero coeff cients on all p variables. A common approach is to effectively ignore (treat as zero) any coef cients less than some threshold value, so that the function becomes simple and the interpretation becomes easier for the users. Such a procedure can be misleading. There are alternatives to principal component analysis which restrict the coef cients to a smaller number of possible values in the derivationof the linear functions,or replace the principalcomponentsby " prin-cipal variables. " This article introduces a new technique, borrowing an idea proposed by Tibshirani in the context of multiple regression where similar problems arise in interpreting regression equations. This approach is the so-called LASSO, the " least absolute shrinkage and selection operator, " in which a bound is introduced on the sum of the absolute values of the coef cients, and in which some coef cients consequently become zero. We explore some of the propertiesof the new technique,both theoreticallyand using simulation studies, and apply it to an example.},
annote = {Founding paper for SPCA using LASSO

* True sparse PCA - with orthogonality constraints},
author = {Jolliffe, Ian T and Trendafilov, Nickolay T and Uddin, Mudassir},
doi = {10.1198/1061860032148},
file = {:home/tom/Documents/Mendeley/Jolliffe, Trendafilov, Uddin_2003_A Modified Principal Component Technique Based on the LASSO.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Interpretation,Principal component analysis,Simplii cation},
number = {3},
pages = {531--547},
title = {{A Modified Principal Component Technique Based on the LASSO}},
url = {http://amstat.tandfonline.com/loi/ucgs20%5Cnhttp://dx.doi.org/10.1198/1061860032148%5Cnhttp://amstat.tandfonline.com/page/terms-and-conditions},
volume = {12},
year = {2003}
}
@inproceedings{Keogh2003,
abstract = {On pr{\'{e}}sente les biais qui peuvent apparaitre avec une m{\'{e}}thode de temporal data mining du fait des donn{\'{e}}es},
address = {Washington, United States},
author = {Keogh, Eamonn and Kasetty, Shruti},
booktitle = {SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1023/A:1024988512476},
file = {:home/tom/Documents/Mendeley/Keogh, Kasetty_2003_On the Need for Time Series Data Mining Benchmarks A Survey and Empirical Demonstration.pdf:pdf},
isbn = {158113567X},
issn = {13845810},
keywords = {Data mining,Experimental evaluation,Time series},
number = {4},
pages = {349--371},
publisher = {ACM},
title = {{On the Need for Time Series Data Mining Benchmarks: A Survey and Empirical Demonstration}},
volume = {7},
year = {2003}
}
@article{Robert2016,
author = {Robert, Matthieu P. and Grill, Jacques and Moreau, Thomas and Grevent, David and Zambrowsky, Olivia and Varlet, Pascale and Contal, Emile and Martin, Gilles and Br{\'{e}}mond-Gignac, Dominique and Ingster-Moati, Isabelle and Dufour, Christelle and Brugi{\`{e}}res, Laurence and Vayatis, Nicolas and Boddaert, Nathalie and Sainte-Rose, Christian and Blauwblomme, Thomas and Puget, St{\'{e}}phanie and Vidal, Pierre-Paul},
journal = {submitted to Brain},
title = {{Optic pathway gliomas-associated nystagmus}},
year = {2016}
}
@inproceedings{han2006gait,
address = {Ioannina, Greece},
author = {Han, J and Jeon, H S and Jeon, B S and Park, K S},
booktitle = {International Special Topic Conference on Information Technology in Biomedicine (ITAB)},
title = {{Gait detection from three dimensional acceleration signals of ankles for the patients with Parkinson's disease}},
year = {2006}
}
@article{Parikh2014,
abstract = {Thismonograph is about a class of optimization algorithms called prox- imal algorithms.Much like Newton's method is a standard tool for solv- ing unconstrained smooth optimization problems of modest size, proxi- mal algorithms can be viewed as an analogous tool for nonsmooth, con- strained, large-scale, or distributed versions of these problems. They are very generally applicable, but are especially well-suited to problems of substantial recent interest involving large or high-dimensional datasets. Proximal methods sit at a higher level of abstraction than classical al- gorithms like Newton's method: the base operation is evaluating the proximal operator of a function, which itself involves solving a small convex optimization problem. These subproblems, which generalize the problem of projecting a point onto a convex set, often admit closed- form solutions or can be solved very quickly with standard or simple specialized methods. Here, we discuss the many different interpreta- tions of proximal operators and algorithms, describe their connections to many other topics in optimization and applied mathematics, survey some popular algorithms, and provide a large number of examples of proximal operators that commonly arise in practice.},
author = {Parikh, Neal and Boyd, Stephen},
doi = {10.1561/2400000003},
file = {:home/tom/Documents/Mendeley/Parikh, Boyd_2014_Proximal Algorithms.pdf:pdf},
isbn = {9781601987167},
issn = {2167-3888},
journal = {Foundations and Trends in Optimization},
number = {3},
pages = {123--231},
title = {{Proximal Algorithms}},
volume = {1},
year = {2014}
}
@article{Parekh2015,
abstract = {Background: This paper addresses the problem of detecting sleep spindles and K-complexes in human sleep EEG. Sleep spindles and K-complexes aid in classifying stage 2 NREM human sleep. New method: We propose a non-linear model for the EEG, consisting of a transient, low-frequency, and an oscillatory component. The transient component captures the non-oscillatory transients in the EEG. The oscillatory component admits a sparse time-frequency representation. Using a convex objective function, this paper presents a fast non-linear optimization algorithm to estimate the components in the proposed signal model. The low-frequency and oscillatory components are used to detect K-complexes and sleep spindles respectively. Results and comparison with other methods: The performance of the proposed method is evaluated using an online EEG database. The F1 scores for the spindle detection averaged 0.70 ± 0.03 and the F1 scores for the K-complex detection averaged 0.57 ± 0.02. The Matthews Correlation Coefficient and Cohen's Kappa values were in a range similar to the F1 scores for both the sleep spindle and K-complex detection. The F1 scores for the proposed method are higher than existing detection algorithms. Conclusions: Comparable run-times and better detection results than traditional detection algorithms suggests that the proposed method is promising for the practical detection of sleep spindles and K-complexes.},
author = {Parekh, Ankit and Selesnick, Ivan W. and Rapoport, David M. and Ayappa, Indu},
doi = {10.1016/j.jneumeth.2015.04.006},
file = {:home/tom/Documents/Mendeley/Parekh et al._2015_Detection of K-complexes and sleep spindles (DETOKS) using sparse optimization.pdf:pdf},
isbn = {0165-0270},
issn = {1872678X},
journal = {Journal of Neuroscience Methods},
keywords = {Convex optimization,K-complex detection,Sleep spindle detection,Sparse signal},
pages = {37--46},
pmid = {25956566},
title = {{Detection of K-complexes and sleep spindles (DETOKS) using sparse optimization}},
volume = {251},
year = {2015}
}
@article{Fawaz2018,
archivePrefix = {arXiv},
arxivId = {1809.04356},
author = {Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
doi = {arXiv:1809.04356v2},
eprint = {1809.04356},
file = {:home/tom/Documents/Mendeley/Fawaz et al._2018_Deep learning for time series classification a review.pdf:pdf},
journal = {preprint ArXiv},
title = {{Deep learning for time series classification: a review}},
url = {http://arxiv.org/abs/1809.04356},
volume = {1809.04356},
year = {2018}
}
@article{Boyd2010,
author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
doi = {10.1561/2200000016},
file = {:home/tom/Documents/Mendeley/Boyd et al._2010_Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.pdf:pdf},
isbn = {1935823719358},
issn = {1935-8237},
journal = {Foundations and Trends in Machine Learning},
number = {1},
pages = {1--122},
title = {{Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers}},
url = {http://www.nowpublishers.com/product.aspx?product=MAL&doi=2200000016%5Cnpapers3://publication/uuid/4BDE54D0-4DE4-4136-BF94-8E1201C74798},
volume = {3},
year = {2010}
}
@techreport{July2013,
author = {Wotao, Yin},
file = {:home/tom/Documents/Mendeley/Wotao_2013_Sparse Optimization Lecture Parallel and Distributed Sparse Optimization.pdf:pdf},
number = {July},
title = {{Sparse Optimization Lecture : Parallel and Distributed Sparse Optimization}},
year = {2013}
}
@article{Shalev-Shwartz2017,
archivePrefix = {arXiv},
arxivId = {1703.07950},
author = {Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
eprint = {1703.07950},
file = {:home/tom/Documents/Mendeley/Shalev-Shwartz, Shamir, Shammah_2017_Failures of Deep Learning.pdf:pdf},
journal = {preprint ArXiv},
title = {{Failures of Deep Learning}},
url = {http://arxiv.org/abs/1703.07950},
volume = {1703.07950},
year = {2017}
}
@article{Low2012,
abstract = {While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.},
archivePrefix = {arXiv},
arxivId = {1204.6078},
author = {Low, Yucheng and Bickson, Danny and Gonzalez, Joseph and Guestrin, Carlos and Kyrola, Aapo and Hellerstein, Joseph M},
doi = {10.14778/2212351.2212354},
eprint = {1204.6078},
file = {:home/tom/Documents/Mendeley/Low et al._2012_Distributed GraphLab a framework for machine learning and data mining in the cloud.pdf:pdf},
issn = {2150-8097},
journal = {VLDB Endowment},
keywords = {a framework for machine,and data mining in,learning,the cloud,tributed graphlab},
number = {8},
pages = {716--727},
title = {{Distributed GraphLab: a framework for machine learning and data mining in the cloud}},
url = {http://dl.acm.org/citation.cfm?id=2212354},
volume = {5},
year = {2012}
}
@article{Hahnloser2003,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hahnloser, Richard H R and Seung, H Sebastian and Slotine, Jean-Jacques},
doi = {10.1162/089976603321192103},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/Mendeley/Hahnloser, Seung, Slotine_2003_Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks.pdf:pdf},
isbn = {0899-7667 (Print)\r0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural Computation},
number = {3},
pages = {621--638},
pmid = {12620160},
title = {{Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks}},
url = {http://www.mitpressjournals.org/doi/10.1162/089976603321192103},
volume = {15},
year = {2003}
}
@article{Boley2013,
author = {Boley, Daniel},
file = {:home/tom/Documents/Mendeley/Boley_2013_On the linear convergence of the alternating direction method of multipliers.pdf:pdf},
journal = {SIAM Journal on Optimization},
keywords = {1,10,1137,120878951,65k05,90c05,90c20,admm,ams subject classifications,doi,introduction,linear programming,problems arise in many,quadratic programming,very large-scale convex optimization},
number = {4},
pages = {2183--2207},
title = {{On the linear convergence of the alternating direction method of multipliers}},
volume = {23},
year = {2013}
}
@article{pearson2004quantification,
author = {Pearson, O R and Busse, M E and {Van Deursen}, R W M and Wiles, C M},
journal = {QJM},
number = {8},
pages = {463--475},
publisher = {Oxford Univ Press},
title = {{Quantification of walking mobility in neurological disorders}},
volume = {97},
year = {2004}
}
@inproceedings{Chaudhry2013,
address = {Melbourne, Australia},
author = {Chaudhry, Rizwan and Vidal, Ren{\'{e}}},
booktitle = {IEEE Conference on Decision and Control (CDC)},
doi = {10.1109/CDC.2013.6760735},
file = {:home/tom/Documents/Mendeley/Chaudhry, Vidal_2013_Initial-state invariant binet-cauchy kernels for the comparison of linear dynamical systems.pdf:pdf},
isbn = {9781467357173},
issn = {01912216},
pages = {5377--5384},
title = {{Initial-state invariant binet-cauchy kernels for the comparison of linear dynamical systems}},
year = {2013}
}
@inproceedings{Kowalski2011,
address = {Grenada, Spain},
author = {Kowalski, Matthieu and Gramfort, Alexandre and Weiss, Pierre and Anthoine, Sandrine},
booktitle = {NIPS Workshop on Optimization for Machine Learning (OPT)},
file = {:home/tom/Documents/Mendeley/Kowalski et al._2011_Accelerating ISTA with an active set strategy.pdf:pdf},
keywords = {active set strategy,elerating ista with an},
pages = {1--6},
title = {{Accelerating ISTA with an active set strategy}},
year = {2011}
}
@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
address = {San Diego, CA, USA},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
booktitle = {International Conference on Learning Representation (ICLR)},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
file = {:home/tom/Documents/Mendeley/Kingma, Ba_2015_Adam A Method for Stochastic Optimization.pdf:pdf},
isbn = {9781450300728},
issn = {09252312},
pages = {1--10},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2015}
}
@inproceedings{Zhou2014,
address = {Colombus, OH, USA},
author = {Zhou, Yin and Chang, Hang and Barner, Kenneth and Spellman, Paul and Parvin, Bahram and Division, Life Sciences and Berkeley, Lawrence},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Documents/Mendeley/Zhou et al._2014_Classification of Histology Sections via Multispectral Convolutional Sparse Coding.pdf:pdf},
keywords = {Convolution,dictionary learning,sparse coding},
pages = {3081----3088},
title = {{Classification of Histology Sections via Multispectral Convolutional Sparse Coding}},
year = {2014}
}
@book{Wright1999,
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Wright, S. and Nocedal, J.},
doi = {10.1007/BF01068601},
eprint = {NIHMS150003},
file = {:home/tom/Documents/Mendeley/Wright, Nocedal_1999_Numerical optimization.pdf:pdf},
isbn = {0387987932},
issn = {0011-4235},
pmid = {21384397},
publisher = {Science Springer},
title = {{Numerical optimization}},
year = {1999}
}
@inproceedings{Fercoq2015,
address = {Lille, France},
archivePrefix = {arXiv},
arxivId = {1505.03410},
author = {Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1505.03410},
file = {:home/tom/Documents/Mendeley//Fercoq, Gramfort, Salmon_2015_Mind the duality gap safer rules for the Lasso.pdf:pdf},
pages = {333--342},
title = {{Mind the duality gap : safer rules for the Lasso}},
year = {2015}
}
@inproceedings{Listgarten2005,
address = {Vancouver, Canada},
author = {Listgarten, Jennifer and Neal, Radford M and Roweis, Sam T and Emili, Andrew},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Documents/Mendeley/Listgarten et al._2004_Multiple alignment of continuous time series.pdf:pdf},
pages = {817--824},
title = {{Multiple alignment of continuous time series}},
year = {2004}
}
@article{Blumensath2009,
archivePrefix = {arXiv},
arxivId = {0805.0510},
author = {Blumensath, Thomas and Davies, Mike E.},
doi = {10.1016/j.acha.2009.04.002},
eprint = {0805.0510},
file = {:home/tom/Documents/Mendeley/Blumensath, Davies_2009_Iterative hard thresholding for compressed sensing.pdf:pdf},
isbn = {1063-5203},
issn = {10635203},
journal = {Applied and Computational Harmonic Analysis},
keywords = {Algorithms,Compressed sensing,Iterative hard thresholding,Signal recovery,Sparse inverse problem},
number = {3},
pages = {265--274},
publisher = {Elsevier Inc.},
title = {{Iterative hard thresholding for compressed sensing}},
url = {http://dx.doi.org/10.1016/j.acha.2009.04.002},
volume = {27},
year = {2009}
}
@article{Naik2011,
abstract = {Independent Component Analysis (ICA), a computationally efficient blind source separation technique, has been an area of interest for researchers for many practical applications in various fields of science and engineering. This paper attempts to cover the fundamental concepts involved in ICA techniques and review its applications. A thorough discussion of the applications and ambiguities problems of ICA has been carried out.Different ICA methods and their applications in various disciplines of science and engineering have been reviewed. In this paper, we present ICA methods from the basics to their potential applications to serve as a comprehensive single source for an inquisitive researcher to carry out research in this field. Povzetek: Podan je pregled tehnike ICA (Independent Component Analysis).},
author = {Naik, Ganesh R and Kumar, Dinesh K},
file = {:home/tom/Documents/Mendeley/Naik, Kumar_2011_An Overview of Independent Component Analysis and Its Applications.pdf:pdf},
isbn = {9780123747266},
issn = {1854-3871},
journal = {Informatica},
keywords = {blind source separation,independent component analysis,multi run ICA,non-gaussianity,overcomplete ICA,undercomplete ICA},
pages = {63--81},
title = {{An Overview of Independent Component Analysis and Its Applications}},
volume = {35},
year = {2011}
}
@inproceedings{Scherrer2012,
abstract = {Large-scale `1-regularized loss minimization problems arise in high-dimensional applications such as compressed sensing and high-dimensional supervised learn- ing, including classification and regression problems. High-performance algo- rithms and implementations are critical to efficiently solving these problems. Building upon previous work on coordinate descent algorithms for `1-regularized problems, we introduce a novel family of algorithms called block-greedy coor- dinate descent that includes, as special cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and Thread-Greedy. We give a unified convergence analysis for the family of block-greedy algorithms. The analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clus- tered so that the maximum inner product between features in different blocks is small. Our theoretical convergence analysis is supported with experimental re- sults using data from diverse real-world applications. We hope that algorithmic approaches and convergence analysis we provide will not only advance the field, but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale `1-regularization problems.},
address = {South Lake Tahoe, United States},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.4174v1},
author = {Scherrer, Chad and Tewari, Ambuj and Halappanavar, Mahantesh and Haglin, David J.},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {arXiv:1212.4174v1},
file = {:home/tom/Documents/Mendeley/Scherrer et al._2012_Feature Clustering for Accelerating Parallel Coordinate Descent.pdf:pdf},
pages = {28--36},
title = {{Feature Clustering for Accelerating Parallel Coordinate Descent}},
year = {2012}
}
@article{Beckouche2013,
archivePrefix = {arXiv},
arxivId = {1304.3573},
author = {Beckouche, Simon and Starck, Jean-Luc and Fadili, Jalal M.},
doi = {10.1051/0004-6361/201220752},
eprint = {1304.3573},
file = {:home/tom/Documents/Mendeley/Beckouche, Starck, Fadili_2013_Astronomical Image Denoising Using Dictionary Learning.pdf:pdf},
isbn = {1099-1557 (Electronic) 1053-8569 (Linking)},
issn = {0004-6361},
journal = {Astronomy \& Astrophysics},
number = {A132},
pmid = {26152658},
title = {{Astronomical Image Denoising Using Dictionary Learning}},
url = {http://arxiv.org/abs/1304.3573%0Ahttp://dx.doi.org/10.1051/0004-6361/201220752},
volume = {556},
year = {2013}
}
@book{Starck2006,
author = {Starck, Jean-Luc and Murtagh, Fionn},
booktitle = {Springer},
doi = {10.1007/978-3-642-56042-2},
file = {:home/tom/Documents/Mendeley/Starck, Murtagh_2006_Astronomical Image and Data Analysis.pdf:pdf},
isbn = {9783642012891},
issn = {0941-7834},
title = {{Astronomical Image and Data Analysis}},
year = {2006}
}
@inproceedings{Mairal2008,
address = {Anchorage, AK, USA},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo and Zisserman, Andrew},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Documents/Mendeley/Mairal et al._2008_Discriminative Learned Dictionaries for Local Image Analysis.pdf:pdf},
pages = {1--8},
title = {{Discriminative Learned Dictionaries for Local Image Analysis}},
year = {2008}
}
@article{Chaudhari2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1511.06485v5},
author = {Chaudhari, Pratik and Soatto, Stefano},
eprint = {arXiv:1511.06485v5},
file = {:home/tom/Documents/Mendeley/Chaudhari, Soatto_2015_On the energy landscape of deep networks.pdf:pdf},
journal = {preprint ArXiv},
title = {{On the energy landscape of deep networks}},
volume = {1511.06485},
year = {2015}
}
@inproceedings{Frasconi1993,
address = {San Francisco, CA, USA},
author = {Frasconi, Paolo and Gori, Marco and Tesi, Alberto},
booktitle = {International Conference on Neural Networks (ICNN)},
file = {:home/tom/Documents/Mendeley/Frasconi, Gori, Tesi_1993_Backpropagation for Lineraly-Separable Patterns a Detailed Analysis.pdf:pdf},
pages = {1818--1822},
title = {{Backpropagation for Lineraly-Separable Patterns: a Detailed Analysis}},
year = {1993}
}
@article{Zhou2013,
abstract = {Classical regression methods treat covariates as a vector and estimate a corresponding vector of regression coefficients. Modern applications in medical imaging generate covariates of more complex form such as multidimensional arrays (tensors). Traditional statistical and computational methods are proving insufficient for analysis of these high-throughput data due to their ultrahigh dimensionality as well as complex structure. In this article, we propose a new family of tensor regression models that efficiently exploit the special structure of tensor covariates. Under this framework, ultrahigh dimensionality is reduced to a manageable level, resulting in efficient estimation and prediction. A fast and highly scalable estimation algorithm is proposed for maximum likelihood estimation and its associated asymptotic properties are studied. Effectiveness of the new methods is demonstrated on both synthetic and real MRI imaging data.},
archivePrefix = {arXiv},
arxivId = {1203.3209},
author = {Zhou, Hua and Li, Lexin and Zhu, Hongtu},
doi = {10.1080/01621459.2013.776499},
eprint = {1203.3209},
file = {:home/tom/Documents/Mendeley/Zhou, Li, Zhu_2013_Tensor regression with applications in neuroimaging data analysis.pdf:pdf},
isbn = {978-1-909493-43-8},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Brain imaging,Dimension reduction,Generalized linear model,Magnetic resonance imaging,Multidimensional array,Tensor regression},
number = {502},
pages = {540--552},
pmid = {24791032},
title = {{Tensor regression with applications in neuroimaging data analysis}},
volume = {108},
year = {2013}
}
@article{Oculo34,
author = {Yang, S and Jeong, J and Kim, JG and Yoon, YH},
journal = {Ophthalmic surgery Lasers Imaging},
number = {37},
pages = {230--3},
title = {{Progressive venous stasis retinopathy and open-angle glaucoma associated with primary pulmonary hypertension}},
volume = {May-Jun},
year = {2006}
}
@article{zaremba2014recurrent,
author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
journal = {preprint ArXiv},
title = {{Recurrent neural network regularization}},
volume = {1409.2329},
year = {2014}
}
@article{Dempster1977,
author = {Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
file = {:home/tom/Documents/Mendeley/Dempster, Laird, Rubin_1977_Dempster, Arthur P and Laird, Nan M and Rubin, Donald B.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (statistical methodology)},
number = {1},
pages = {1--38},
title = {{Dempster, Arthur P and Laird, Nan M and Rubin, Donald B}},
volume = {39},
year = {1977}
}
@article{Qu2016,
abstract = {We study the problem of minimizing the sum of a smooth convex function and a convex block-separable regularizer and propose a new randomized coordinate descent method, which we call ALPHA. Our method at every iteration updates a random subset of coordinates, following an arbitrary distribution. No coordinate descent methods capable to handle an arbitrary sampling have been studied in the literature before for this problem. ALPHA is a remarkably flexible algorithm: in special cases, it reduces to deterministic and randomized methods such as gradient descent, coordinate descent, parallel coordinate descent and distributed coordinate descent -- both in nonaccelerated and accelerated variants. The variants with arbitrary (or importance) sampling are new. We provide a complexity analysis of ALPHA, from which we deduce as a direct corollary complexity bounds for its many variants, all matching or improving best known bounds.},
archivePrefix = {arXiv},
arxivId = {1412.8060},
author = {Qu, Zheng and Richt{\'{a}}rik, Peter},
doi = {10.1080/10556788.2016.1190360},
eprint = {1412.8060},
file = {:home/tom/Documents/Mendeley/Qu, Richt{\'{a}}rik_2016_Coordinate descent with arbitrary sampling I algorithms and complexity†.pdf:pdf},
issn = {10294937},
journal = {Optimization Methods and Software},
keywords = {accelerated coordinate descent,arbitrary sampling,complexity analysis,coordinate descent},
number = {5},
pages = {829--857},
title = {{Coordinate descent with arbitrary sampling I: algorithms and complexity†}},
volume = {31},
year = {2016}
}
@article{Friedlander2016,
archivePrefix = {arXiv},
arxivId = {1603.05719},
author = {Friedlander, Michael P. and Goh, Gabriel},
eprint = {1603.05719},
file = {:home/tom/Documents/Mendeley/Friedlander, Goh_2016_Efficient evaluation of scaled proximal operators.pdf:pdf},
journal = {preprint ArXiv},
keywords = {1,90c15,90c25,ams subject classifications,firmly rooted as,interior-point,introduction,optimization,proximal-gradient,proximal-gradient methods have become,quadratic support,quasi-newton,their applica-,they are prized for,workhorse algorithms for convex},
title = {{Efficient evaluation of scaled proximal operators}},
url = {http://arxiv.org/abs/1603.05719},
volume = {1603.05719},
year = {2016}
}
@article{Tao2015,
archivePrefix = {arXiv},
arxivId = {1501.02888},
author = {Tao, Shaozhe and Boley, Daniel and Zhang, Shuzhong},
eprint = {1501.02888},
file = {:home/tom/Documents/Mendeley//Tao, Boley, Zhang_2015_Local Linear Convergence of ISTA and FISTA on the LASSO Problem.pdf:pdf},
journal = {preprint ArXiv},
title = {{Local Linear Convergence of ISTA and FISTA on the LASSO Problem}},
volume = {1501.02888},
year = {2015}
}
@inproceedings{Wang2012a,
address = {Beijing, China},
author = {Wang, Fei and Lee, Noah and Hu, Jianying and Sun, Jimeng and Ebadollahi, Shahram},
booktitle = {SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/2339530.2339605},
file = {:home/tom/Documents/Mendeley/Wang et al._2012_Towards Heterogeneous Temporal Clinical Event Pattern Discovery A Convolutional Approach.pdf:pdf},
isbn = {9781450314626},
keywords = {convolution,dictionary learning,nmf,pattern discovery},
pages = {453--461},
publisher = {ACM},
title = {{Towards Heterogeneous Temporal Clinical Event Pattern Discovery : A Convolutional Approach}},
year = {2012}
}
@article{Hebiri2013,
archivePrefix = {arXiv},
arxivId = {1204.1605},
author = {Hebiri, Mohamed and Lederer, Johannes},
eprint = {1204.1605},
file = {:home/tom/Documents/Mendeley//Hebiri, Lederer_2013_How Correlations Influence Lasso Prediction.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
keywords = {correlations,lars algorithm,lasso,restricted eigenvalue,tun-},
number = {3},
pages = {1846--1854},
title = {{How Correlations Influence Lasso Prediction}},
volume = {59},
year = {2013}
}
@inproceedings{Moreau2015,
author = {Moreau, Thomas and Oudre, Laurent and Vayatis, Nicolas},
booktitle = {NIPS Workshop on Nonparametric Methods for Large Scale Representation Learning},
file = {:home/tom/Documents/Mendeley/Moreau, Oudre, Vayatis_2015_Distributed Convolutional Sparse Coding via Message Passing Interface ( MPI ).pdf:pdf},
title = {{Distributed Convolutional Sparse Coding via Message Passing Interface ( MPI )}},
url = {http://www.cs.cmu.edu/$\sim$andrewgw/rep/MorOudVay.pdf},
year = {2015}
}
@inproceedings{Barak2013,
address = {Stanford University, USA},
archivePrefix = {arXiv},
arxivId = {1312.6652},
author = {Barak, Boaz and Kelner, Jonathan and Steurer, David},
booktitle = {Annual ACM Symposium on Theory of Computing},
eprint = {1312.6652},
file = {:home/tom/Documents/Mendeley/Barak, Kelner, Steurer_2013_Rounding Sum-of-Squares Relaxations.pdf:pdf},
pages = {1----45},
title = {{Rounding Sum-of-Squares Relaxations}},
url = {http://arxiv.org/abs/1312.6652},
year = {2013}
}
@inproceedings{Yellin2017,
address = {Melbourne, Australia},
author = {Yellin, Florence and Haeffele, Benjamin D. and Vidal, Ren{\'{e}}},
booktitle = {IEEE International Symposium on Biomedical Imaging (ISBI)},
file = {:home/tom/Documents/Mendeley/Yellin, Haeffele, Vidal_2017_Blood cell detection and counting in holographic lens-free imaging by convolutional sparse dictionary learn.pdf:pdf},
title = {{Blood cell detection and counting in holographic lens-free imaging by convolutional sparse dictionary learning and coding}},
year = {2017}
}
@inproceedings{Becker2012,
address = {South Lake Tahoe, United States},
archivePrefix = {arXiv},
arxivId = {1206.1156},
author = {Becker, Stephen and Fadili, Jalal M.},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1206.1156},
file = {:home/tom/Documents/Mendeley//Becker, Fadili_2012_A quasi-Newton proximal splitting method.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
pages = {2618--2626},
title = {{A quasi-Newton proximal splitting method}},
url = {http://arxiv.org/abs/1206.1156},
year = {2012}
}
@inproceedings{ying2007automatic,
address = {Aachen, Germany},
author = {Ying, H and Silex, C and Schnitzer, A and Leonhardt, S and Schiek, M},
booktitle = {International Workshop on Wearable and Implantable Body Sensor Networks (BSN)},
pages = {80--85},
title = {{Automatic step detection in the accelerometer signal}},
year = {2007}
}
@inproceedings{Seibert2014,
author = {Seibert, M and Kleinsteuber, Martin and Gribonval, R and Jenatton, Rodolphe and Bach, F},
booktitle = {IEEE Workshop on Statistical Signal Processing (SSP)},
file = {:home/tom/Documents/Mendeley/Seibert et al._2014_On the Sample Complexity of Sparse Dictionary Learning.pdf:pdf},
isbn = {9781479949755},
number = {5},
pages = {244--247},
title = {{On the Sample Complexity of Sparse Dictionary Learning}},
year = {2014}
}
@article{Selesnick2014,
abstract = {This paper seeks to combine linear time-invariant (LTI) filtering and sparsity-based denoising in a principled way in order to effectively filter (denoise) a wider class of signals. LTI filtering is most suitable for signals restricted to a known frequency band, while sparsity-based denoising is suitable for signals admitting a sparse representation with respect to a known transform. However, some signals cannot be accurately categorized as either band-limited or sparse. This paper addresses the problem of filtering noisy data for the particular case where the underlying signal comprises a low-frequency component and a sparse-derivative component. A convex optimization approach is presented and two algorithms derived, one based on majorization-minimization (MM), the other based on the alternating direction method of multipliers (ADMM). It is shown that a particular choice of discrete-time filter, namely zero-phase non-causal recursive filters for finite-length data formulated in terms of banded matrices, makes the algorithms computationally efficient and effective. The computational efficiency of the algorithm stems from the use of fast algorithms for solving banded systems of linear equations. The method is illustrated using data from a physiological-measurement technique (i.e., near infrared spectroscopic time series imaging) that in many cases yields data that is well-approximated as the sum of low-frequency, sparse-derivative and noise components},
author = {Selesnick, Ivan W. and Graber, Harry L. and Pfeil, Douglas S. and Barbour, Randall L.},
doi = {10.1109/TSP.2014.2298836},
file = {:home/tom/Documents/Mendeley/Selesnick et al._2014_Simultaneous low-pass filtering and total variation denoising.pdf:pdf},
isbn = {1053-587X VO  - 62},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Butterworth filter,Total variation denoising,low-pass filter,sparse signal,sparsity,zero-phase filter},
number = {5},
pages = {1109--1124},
title = {{Simultaneous low-pass filtering and total variation denoising}},
volume = {62},
year = {2014}
}
@inproceedings{Qu2014,
abstract = {We consider the problem of recovering the sparsest vector in a subspace {$}\backslashmathcal{\{}S{\}} \backslashsubseteq \backslashmathbb{\{}R{\}}{^}p{$} with {$}\backslashmathrm{\{}dim{\}}(\backslashmathcal{\{}S{\}}) = n {<} p{$}. This problem can be considered a homogeneous variant of the sparse recovery problem, and finds applications in sparse dictionary learning, sparse PCA, and other problems in signal processing and machine learning. Simple convex heuristics for this problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds {$}1/\backslashsqrt{\{}n{\}}{$}. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is {$}\backslashOmega(1){$}. To our knowledge, this is the first practical algorithm to achieve this linear scaling. This result assumes a planted sparse model, in which the target sparse vector is embedded in an otherwise random subspace. Empirically, our proposed algorithm also succeeds in more challenging data models arising, e.g., from sparse dictionary learning.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1412.4659},
author = {Qu, Qing and Sun, Ju and Wright, John},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1109/TIT.2016.2601599},
eprint = {1412.4659},
file = {:home/tom/Documents/Mendeley/Qu, Sun, Wright_2014_Finding a sparse vector in a subspace Linear sparsity using alternating directions.pdf:pdf},
isbn = {0001413104},
issn = {10495258},
pages = {3401--3409},
title = {{Finding a sparse vector in a subspace: Linear sparsity using alternating directions}},
url = {http://arxiv.org/abs/1412.4659},
year = {2014}
}
@article{Sun2015,
address = {Lille, France},
author = {Sun, Ju and Qu, Qinq and Wright, John},
file = {:home/tom/Documents/Mendeley/Sun, Qu, Wright_2015_Complete Dictionary Recovery Using Nonconvex Optimization.pdf:pdf},
journal = {International Conference on Machine Learning (ICML)},
title = {{Complete Dictionary Recovery Using Nonconvex Optimization}},
volume = {37},
year = {2015}
}
@article{Moreau2016,
archivePrefix = {arXiv},
arxivId = {1611.04499},
author = {Moreau, Thomas and Audiffren, Julien},
eprint = {1611.04499},
file = {:home/tom/Documents/Mendeley/Moreau, Audiffren_2016_Post Training in Deep Learning with Last Kernel.pdf:pdf},
journal = {preprint ArXiv},
title = {{Post Training in Deep Learning with Last Kernel}},
url = {http://arxiv.org/abs/1611.04499},
volume = {1611.04499},
year = {2016}
}
@article{Chen1994,
author = {Chen, Gong and Teboulle, Marc},
doi = {10.1007/BF01582566},
file = {:home/tom/Documents/Mendeley//Chen, Teboulle_1994_A proximal-based decomposition method for convex minimization problems.pdf:pdf},
issn = {00255610},
journal = {Mathematical Programming},
keywords = {AMS Subject Classification: 90C25,Augmented Lagrangian,Convex programming,Decomposition-splitting methods,Proximal methods},
number = {1-3},
pages = {81--101},
title = {{A proximal-based decomposition method for convex minimization problems}},
volume = {64},
year = {1994}
}
@article{Fercoq2013,
archivePrefix = {arXiv},
arxivId = {1312.5799},
author = {Fercoq, Olivier and Richt{\'{a}}rik, Peter},
doi = {10.1137/16M1085905},
eprint = {1312.5799},
file = {:home/tom/Documents/Mendeley/Fercoq, Richt{\'{a}}rik_2015_Accelerated, Parallel and Proximal Coordinate Descent.pdf:pdf},
isbn = {0036-1445},
issn = {0036-1445},
journal = {SIAM Journal on Optimization},
keywords = {10,1137,130949993,49m27,65k05,65y20,68q25,68w10,68w20,90c25,acceleration,ams subject classifications,big data,complexity,convex optimization,doi,parallel methods,partial separability,proximal methods,randomized coordinate descent},
number = {4},
pages = {1997--2023},
title = {{Accelerated, Parallel and Proximal Coordinate Descent}},
url = {http://arxiv.org/abs/1312.5799},
volume = {25},
year = {2015}
}
@inproceedings{mladenov2009step,
address = {Dublin, Ireland},
author = {Mladenov, M and Mock, M},
booktitle = {International Workshop on Context-Aware Middleware and Services (COMSWARE)},
organization = {ACM},
pages = {1--5},
title = {{A step counter service for Java-enabled devices using a built-in accelerometer}},
year = {2009}
}
@inproceedings{Oudre2015,
address = {Lyon, France},
author = {Oudre, Laurent and Moreau, Thomas and Truong, Charles and Barrois-M{\"{u}}ller, R{\'{e}}mi and Dadashi, Robert and Gr{\'{e}}gory, Thomas},
booktitle = {Groupe de Recherche et d'Etudes en Traitement du Signal et des Images (GRETSI)},
title = {{D{\'{e}}tection de pas {\`{a}} partir de donn{\'{e}}es d'acc{\'{e}}l{\'{e}}rom{\'{e}}trie}},
year = {2015}
}
@article{Mahoney2011,
abstract = {Randomized algorithms for very large matrix problems have received a great deal of attention in recent years. Much of this work was motivated by problems in large-scale data analysis. Although this work had its origins within theoretical computer science, where researchers were interested in proving worst-case bounds, i.e., bounds without any assumptions at all on the input data, researchers from numerical linear algebra, statistics, applied mathematics, data analysis, and machine learning, as well as domain scientists have subsequently extended and applied these methods in important ways. Although this has been great for the development of the area and for the technology transfer of theoretical ideas into practical applications, this interdisciplinarity has thus far sometimes obscured the underlying simplicity and generality of the core ideas. This review will provide a detailed overview of recent work on randomized algorithms for matrix problems, with an emphasis on a few simple core ideas that underlie not only recent theoretical advances but also the usefulness of these tools in large-scale data applications. Crucial in this context is the connection with concept of statistical leverage. This concept has long been used in statistical regression diagnostics to identify outliers; and it has recently proved crucial in the development of improved worst-case matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists. This connection arises naturally when one explicitly decouples the effect of randomization in these matrix algorithms from the underlying linear algebraic structure. This decoupling also permits much finer control in the application of randomization, as well as the easier exploitation of domain knowledge.},
archivePrefix = {arXiv},
arxivId = {1104.5557},
author = {Mahoney, Michael W.},
doi = {10.1561/2200000035},
eprint = {1104.5557},
file = {:home/tom/Documents/Mendeley/Mahoney_2011_Randomized algorithms for matrices and data.pdf:pdf},
isbn = {9781601985064},
issn = {1935-8237},
journal = {Foundations and Trends in Machine Learning},
number = {2},
pages = {123----224},
title = {{Randomized algorithms for matrices and data}},
url = {http://arxiv.org/abs/1104.5557},
volume = {3},
year = {2011}
}
@article{Rao2008,
abstract = {We consider settings where the observations are drawn from a zero-mean multivariate (real or complex) normal distribution with the population covariance matrix having eigenvalues of arbitrary multiplicity. We assume that the eigenvectors of the population covariance matrix are unknown and focus on inferential procedures that are based on the sample eigenvalues alone (i.e., "eigen-inference"). Results found in the literature establish the asymptotic normality of the fluctuation in the trace of powers of the sample covariance matrix. We develop concrete algorithms for analytically computing the limiting quantities and the covariance of the fluctuations. We exploit the asymptotic normality of the trace of powers of the sample covariance matrix to develop eigenvalue-based procedures for testing and estimation. Specifically, we formulate a simple test of hypotheses for the population eigenvalues and a technique for estimating the population eigenvalues in settings where the cumulative distribution function of the (nonrandom) population eigenvalues has a staircase structure. Monte Carlo simulations are used to demonstrate the superiority of the proposed methodologies over classical techniques and the robustness of the proposed techniques in high-dimensional, (relatively) small sample size settings. The improved performance results from the fact that the proposed inference procedures are "global" (in a sense that we describe) and exploit "global" information thereby overcoming the inherent biases that cripple classical inference procedures which are "local" and rely on "local" information.},
archivePrefix = {arXiv},
arxivId = {math/0701314},
author = {Rao, N. Raj and Mingo, James A. and Speicher, Roland and Edelman, Alan},
doi = {10.1214/07-AOS583},
eprint = {0701314},
file = {:home/tom/Documents/Mendeley/Rao et al._2008_Statistical Eigen-Inference from large wishart matrices.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Eigen-inference,Free probability,Linear statistics.,Random matrix theory,Sample covariance matrices,Second order freeness,Wishart matrices},
number = {6},
pages = {2850--2885},
primaryClass = {math},
title = {{Statistical Eigen-Inference from large wishart matrices}},
volume = {36},
year = {2008}
}
@inproceedings{Tishby2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.02406v1},
author = {Tishby, Naftali and Zaslavsky, Noga},
booktitle = {IEEE Information Theory Workshop (ITW)},
eprint = {arXiv:1503.02406v1},
file = {:home/tom/Documents/Mendeley/Tishby, Zaslavsky_2015_Deep Learning and the Information Bottleneck Principle.pdf:pdf},
title = {{Deep Learning and the Information Bottleneck Principle}},
year = {2015}
}
@article{Barrois2016,
author = {Barrois, R. and Gregory, Th and Oudre, Laurent and Moreau, Th and Truong, Ch and Pulini, A. Aram and Vienne, A. and Labourdette, Ch and Vayatis, N. and Buffat, S. and Yelnik, A. and {De Waele}, C. and Laporte, S. and Vidal, P. P. and Ricard, D.},
doi = {10.1371/journal.pone.0164975},
file = {:home/tom/Documents/Mendeley/Barrois et al._2016_An automated recording method in clinical consultation to rate the limp in lower limb osteoarthritis.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {10},
pages = {e0164975},
pmid = {27776168},
title = {{An automated recording method in clinical consultation to rate the limp in lower limb osteoarthritis}},
volume = {11},
year = {2016}
}
@inproceedings{Strobl2013,
author = {Strobl, Eric V. and Visweswaran, Shyam},
booktitle = {International Conference on Machine Learning and Applications (ICMLA)},
doi = {10.1109/ICMLA.2013.84},
file = {:home/tom/Documents/Mendeley/Strobl, Visweswaran_2013_Deep Multiple Kernel Learning.pdf:pdf},
isbn = {978-0-7695-5144-9},
keywords = {deep learning,kernels,multiple,support vector machine},
pages = {414--417},
title = {{Deep Multiple Kernel Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6784654},
year = {2013}
}
@article{Gramfort2010,
author = {Gramfort, Alexandre and Keriven, Renaud and Clerc, Maureen},
doi = {10.1109/TBME.2009.2037139},
file = {:home/tom/Documents/Mendeley/Gramfort, Keriven, Clerc_2010_Graph-based variability estimation in single-trial event-related neural responses.pdf:pdf},
isbn = {1558-2531 (Electronic) 0018-9294 (Linking)},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
number = {5},
pages = {1051--1061},
pmid = {20142163},
title = {{Graph-based variability estimation in single-trial event-related neural responses}},
volume = {57},
year = {2010}
}
@article{Fikret2013,
author = {Fikret, Işık Karahanoğlu and Caballero-gaudes, C{\'{e}}sar and Lazeyras, Fran{\c{c}}ois and Dimitri, Van De Ville},
file = {:home/tom/Documents/Mendeley/Fikret et al._2013_Total activation fMRI deconvolution through spatio-temporal regularization.pdf:pdf},
journal = {NeuroImage},
pages = {121--134},
title = {{Total activation: fMRI deconvolution through spatio-temporal regularization}},
url = {https://www.sciencedirect.com/science/article/pii/S1053811913001146},
volume = {73},
year = {2013}
}
@article{Parekh2017,
abstract = {Background Automated single-channel spindle detectors, for human sleep EEG, are blind to the presence of spindles in other recorded channels unlike visual annotation by a human expert. New method We propose a multichannel spindle detection method that aims to detect global and local spindle activity in human sleep EEG. Using a non-linear signal model, which assumes the input EEG to be the sum of a transient and an oscillatory component, we propose a multichannel transient separation algorithm. Consecutive overlapping blocks of the multichannel oscillatory component are assumed to be low-rank whereas the transient component is assumed to be piecewise constant with a zero baseline. The estimated oscillatory component is used in conjunction with a bandpass filter and the Teager operator for detecting sleep spindles. Results and comparison with other methods The proposed method is applied to two publicly available databases and compared with 7 existing single-channel automated detectors. F1scores for the proposed spindle detection method averaged 0.66 (0.02) and 0.62 (0.06) for the two databases, respectively. For an overnight 6 channel EEG signal, the proposed algorithm takes about 4 min to detect sleep spindles simultaneously across all channels with a single setting of corresponding algorithmic parameters. Conclusions The proposed method attempts to mimic and utilize, for better spindle detection, a particular human expert behavior where the decision to mark a spindle event may be subconsciously influenced by the presence of a spindle in EEG channels other than the central channel visible on a digital screen.},
author = {Parekh, Ankit and Selesnick, Ivan W. and Osorio, Ricardo S. and Varga, Andrew W. and Rapoport, David M. and Ayappa, Indu},
doi = {10.1016/j.jneumeth.2017.06.004},
file = {:home/tom/Documents/Mendeley/Parekh et al._2017_Multichannel sleep spindle detection using sparse low-rank optimization.pdf:pdf},
isbn = {1872-678X (Electronic)0165-0270 (Linking)},
issn = {1872678X},
journal = {Journal of Neuroscience Methods},
keywords = {Convex optimization,Multichannel signal processing,Sleep EEG,Sparse signal,Spindle detection},
pages = {1--16},
pmid = {28600157},
publisher = {Elsevier B.V.},
title = {{Multichannel sleep spindle detection using sparse low-rank optimization}},
url = {http://dx.doi.org/10.1016/j.jneumeth.2017.06.004},
volume = {288},
year = {2017}
}
@article{Montavon2011,
abstract = {When training deep networks it is common knowledge that an efficient and well generalizing rep- resentation of the problem is formed. In this paper we aim to elucidate what makes the emerging representation successful. We analyze the layer-wise evolution of the representation in a deep net- work by building a sequence of deeper and deeper kernels that subsume the mapping performed by more and more layers of the deep network and measuring how these increasingly complex kernels fit the learning problem. We observe that deep networks create increasingly better representations of the learning problem and that the structure of the deep network controls how fast the representa- tion of the task is formed layer after layer.},
author = {Montavon, Gr{\'{e}}goire and Braun, Mikio and M{\"{u}}ller, Klaus-Robert},
file = {:home/tom/Documents/Mendeley//Montavon, Braun, M{\"{u}}ller_2011_Kernel Analysis of Deep Networks.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {deep networks,kernel principal component analysis,representations},
pages = {2563--2581},
title = {{Kernel Analysis of Deep Networks}},
volume = {12},
year = {2011}
}
@article{Vidaurre2018a,
abstract = {Brain activity is a dynamic combination of the responses to sensory inputs and its own spontaneous processing. Consequently, such brain activity is continuously changing whether or not one is focusing on an externally imposed task. Previously, we have introduced an analysis method that allows us, using Hidden Markov Models (HMM), to model task or rest brain activity as a dynamic sequence of distinct brain networks, overcoming many of the limitations posed by sliding window approaches. Here, we present an advance that enables the HMM to handle very large amounts of data, making possible the inference of very reproducible and interpretable dynamic brain networks in a range of different datasets, including task, rest, MEG and fMRI, with potentially thousands of subjects. We anticipate that the generation of large and publicly available datasets from initiatives such as the Human Connectome Project and UK Biobank, in combination with computational methods that can work at this scale, will bring a breakthrough in our understanding of brain function in both health and disease.},
author = {Vidaurre, Diego and Abeysuriya, Romesh and Becker, Robert and Quinn, Andrew J. and Alfaro-Almagro, Fidel and Smith, Stephen M. and Woolrich, Mark W.},
doi = {10.1016/j.neuroimage.2017.06.077},
file = {:home/tom/Documents/Mendeley/Vidaurre et al._2018_Discovering dynamic brain networks from big data in rest and task.pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
number = {June},
pages = {646--656},
pmid = {28669905},
publisher = {Elsevier Ltd},
title = {{Discovering dynamic brain networks from big data in rest and task}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2017.06.077},
volume = {180},
year = {2018}
}
@article{Douglas2000,
author = {Douglas, Scott C and Amari, Shun-ichi and Kung, S},
file = {:home/tom/Documents/Mendeley/Douglas, Amari, Kung_2000_On Gradient Adaptation with Unit-Norm Constraints.pdf:pdf},
journal = {IEEE Transactions on Signal Processing},
number = {6},
pages = {640--643},
title = {{On Gradient Adaptation with Unit-Norm Constraints}},
volume = {48},
year = {2000}
}
@article{Karimi2014,
archivePrefix = {arXiv},
arxivId = {1401.4220},
author = {Karimi, Sahar and Vavasis, Stephen},
eprint = {1401.4220},
file = {:home/tom/Documents/Mendeley//Karimi, Vavasis_2014_IMRO a proximal quasi-Newton method for solving $l_1$-regularized least square problem.pdf:pdf},
journal = {preprint ArXiv},
keywords = {basis pursuit de-,convex optimization,l 1 -regularized least,minimization of com-,noising problem,proximal methods,quasi-newton methods,sparse recovery,square problem},
title = {{IMRO: a proximal quasi-Newton method for solving $l_1$-regularized least square problem}},
url = {http://arxiv.org/abs/1401.4220},
volume = {1401.4220},
year = {2014}
}
@article{Dalalyan2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1700v1},
author = {Dalalyan, Arnak S and Hebiri, Mohamed and Lederer, Johannes},
eprint = {arXiv:1402.1700v1},
file = {:home/tom/Documents/Mendeley//Dalalyan, Hebiri, Lederer_2014_On the prediction performance of the Lasso.pdf:pdf},
journal = {preprint ArXiv},
title = {{On the prediction performance of the Lasso}},
volume = {1402.1700},
year = {2014}
}
@inproceedings{collobert2008unified,
address = {Helsinki, Finland},
author = {Collobert, Ronan and Weston, Jason},
booktitle = {International Conference on Machine Learning (ICML)},
organization = {ACM},
pages = {160--167},
title = {{A unified architecture for natural language processing: Deep neural networks with multitask learning}},
year = {2008}
}
@inproceedings{Wang2018,
archivePrefix = {arXiv},
arxivId = {1706.06972},
author = {Wang, Yaqing and Yao, Quanming and Kwok, James T. and Ni, Lionel M.},
booktitle = {International Conference on Machine Learning (ICML)},
doi = {10.1109/TIP.2018.2842152},
eprint = {1706.06972},
file = {:home/tom/Documents/Mendeley/Wang et al._2018_Online Convolutional Sparse Coding with Sample-Dependent Dictionary.pdf:pdf},
issn = {10577149},
keywords = {Convolution,Convolutional codes,Convolutional sparse coding,Dictionaries,Dictionary learning,Frequency-domain analysis,Image coding,Online learning,Optimization,Sparse matrices},
title = {{Online Convolutional Sparse Coding with Sample-Dependent Dictionary}},
year = {2018}
}
@inproceedings{Moreau2015a,
author = {Moreau, Thomas and Oudre, Laurent and Vayatis, Nicolas},
booktitle = {Groupe de Recherche et d'Etudes en Traitement du Signal et des Images (GRETSI)},
file = {:home/tom/Documents/Mendeley/Moreau, Oudre, Vayatis_2015_Groupement automatique pour l ' analyse du spectre singulier.pdf:pdf},
title = {{Groupement automatique pour l ' analyse du spectre singulier}},
year = {2015}
}
@inproceedings{Fyshe2012,
address = {La Palma, Canary Islands},
author = {Fyshe, Alona and Fox, Emily B and Dunson, David B and Mitchell, Tom M},
booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
file = {:home/tom/Documents/Mendeley/Fyshe et al._2012_Hierarchical Latent Dictionaries for Models of Brain Activation.pdf:pdf},
pages = {409--421},
title = {{Hierarchical Latent Dictionaries for Models of Brain Activation}},
url = {http://jmlr.csail.mit.edu/proceedings/papers/v22/fyshe12.html},
year = {2012}
}
@inproceedings{Haeffele2017,
address = {Honolulu, HI, USA},
author = {Haeffele, Benjamin D and Vidal, Ren{\'{e}}},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.467},
file = {:home/tom/Documents/Mendeley/Haeffele, Vidal_2017_Global Optimality in Neural Network Training.pdf:pdf},
pages = {7331--7339},
title = {{Global Optimality in Neural Network Training}},
url = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Haeffele_Global_Optimality_in_CVPR_2017_paper.pdf},
year = {2017}
}
@article{Liu2017a,
archivePrefix = {arXiv},
arxivId = {1706.09563},
author = {Liu, Jialin and Garcia-Cardona, Cristina and Wohlberg, Brendt and Yin, Wotao},
eprint = {1706.09563},
file = {:home/tom/Documents/Mendeley/Liu et al._2017_Online Convolutional Dictionary Learning(2).pdf:pdf},
journal = {preprint ArXiv},
keywords = {convolutional dictionary learning,convolutional sparse coding,descent,recursive least squares,stochastic gradient},
title = {{Online Convolutional Dictionary Learning}},
url = {http://arxiv.org/abs/1706.09563},
volume = {1709.00106},
year = {2017}
}
@article{Schmidt2008,
author = {Schmidt, Mikkel N and Hansen, Lars Kai},
file = {:home/tom/Documents/Mendeley/Schmidt, Hansen_2008_Shift Invariant Sparse Coding of Image and Music Data.pdf:pdf},
journal = {Journal of Machine Learning Research (JMLR)},
pages = {1--14},
title = {{Shift Invariant Sparse Coding of Image and Music Data}},
year = {2008}
}
@article{salarian2004gait,
author = {Salarian, A and Russmann, H and Vingerhoets, F and Dehollain, C and Blanc, Y and Burkhard, P and Aminian, K},
journal = {IEEE Transactions on Biomedical Engineering},
number = {8},
pages = {1434--1443},
publisher = {IEEE},
title = {{Gait assessment in Parkinson's disease: toward an ambulatory system for long-term monitoring}},
volume = {51},
year = {2004}
}
@article{Xiang2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1405.4897v1},
author = {Xiang, Zj and Wang, Yun and Ramadge, Pj},
eprint = {arXiv:1405.4897v1},
file = {:home/tom/Documents/Mendeley//Xiang, Wang, Ramadge_2014_Screening tests for lasso problems.pdf:pdf},
journal = {preprint ArXiv},
title = {{Screening tests for lasso problems}},
volume = {1405.4897},
year = {2014}
}
@book{Hastie2015,
author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin J.},
file = {:home/tom/Documents/Mendeley//Hastie, Tibshirani, Wainwright_2015_Statistical Learning with Sparsity.pdf:pdf},
pages = {1--351},
publisher = {CRC Press},
title = {{Statistical Learning with Sparsity}},
year = {2015}
}
@inproceedings{Zhou2009,
abstract = {Alignment of time series is an important problem to solve in many scientific dis- ciplines. In particular, temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale differ- ence between human actions as well as the inter/intra subject variability. In this paper we present canonical time warping (CTW), an extension of canonical cor- relation analysis (CCA) for spatio-temporal alignment of human motion between two subjects. CTW extends previous work on CCA in two ways: (i) it combines CCA with dynamic time warping (DTW), and (ii) it extends CCA by allowing local spatial deformations. We show CTW's effectiveness in three experiments: alignment of synthetic data, alignment of motion capture data of two subjects per- forming similar actions, and alignment of similar facial expressions made by two people. Our results demonstrate thatCTWprovides both visually and qualitatively better alignment than state-of-the-art techniques based on DTW.},
address = {Vancouver, Canada},
author = {Zhou, Feng and de la Torre, Fernando},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1103/PhysRevB.72.205311},
file = {:home/tom/Documents/Mendeley/Zhou, de la Torre_2009_Canonical time warping for alignment of human behavior.pdf:pdf},
isbn = {9781615679119},
issn = {10980121},
keywords = {Canonical Correlation Analysis,Dynamic Time Warping},
pages = {2286--2294},
title = {{Canonical time warping for alignment of human behavior}},
url = {http://f-zhou.com/pdf/2009_nips_ctw.pdf},
year = {2009}
}
@article{Chen1994,
author = {Chen, Gong and Teboulle, Marc},
file = {:home/tom/Documents/Mendeley//Chen, Teboulle_1994_A proximal-based decomposition method for convex minimization problems.pdf:pdf},
journal = {Mathematical Programming},
number = {1-3},
pages = {81--101},
title = {{A proximal-based decomposition method for convex minimization problems}},
volume = {64},
year = {1994}
}
@article{Gribonval2015a,
archivePrefix = {arXiv},
arxivId = {1407.5155},
author = {Gribonval, Remi and Jenatton, Rodolphe and Bach, Francis},
doi = {10.1109/TIT.2015.2472522},
eprint = {1407.5155},
file = {:home/tom/Documents/Mendeley/Gribonval, Jenatton, Bach_2015_Sparse and spurious dictionary learning with noise and outliers.pdf:pdf},
isbn = {2011277906},
journal = {IEEE Transactions on Information Theory},
number = {11},
pages = {6298--6319},
title = {{Sparse and spurious: dictionary learning with noise and outliers}},
url = {http://arxiv.org/abs/1407.5155},
volume = {61},
year = {2015}
}
@article{Moussallam2014,
abstract = {Denoising methods require some assumptions about the signal of interest and the noise. While most denoising procedures require some knowledge about the noise level, which may be unknown in practice, here we assume that the signal expansion in a given dictionary has a distribution that is more heavy-tailed than the noise. We show how this hypothesis leads to a stopping criterion for greedy pursuit algorithms which is independent from the noise level. Inspired by the success of ensemble methods in machine learning, we propose a strategy to reduce the variance of greedy estimates by averaging pursuits obtained from randomly subsampled dictionaries. We call this denoising procedure Blind Random Pursuit Denoising (BIRD). We offer a generalization to multidimensional signals, with a structured sparse model (S-BIRD). The relevance of this approach is demonstrated on synthetic and experimental MEG signals where, without any parameter tuning, BIRD outperforms state-of-the-art algorithms even when they are informed by the noise level. Code is available to reproduce all experiments.},
archivePrefix = {arXiv},
arxivId = {1312.5444},
author = {Moussallam, Manuel and Gramfort, Alexandre and Daudet, Laurent and Richard, Gael},
doi = {10.1109/LSP.2014.2334231},
eprint = {1312.5444},
file = {:home/tom/Documents/Mendeley/Moussallam et al._2014_Blind Denoising with Random Greedy Pursuits.pdf:pdf},
isbn = {1070-9908 VO - 21},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Please add index terms},
number = {11},
pages = {1341--1345},
title = {{Blind Denoising with Random Greedy Pursuits}},
volume = {21},
year = {2014}
}
@article{Lebrun2012,
author = {Lebrun, Marc and Leclaire, Arthur},
doi = {10.5201/ipol.2012.llm-ksvd},
file = {:home/tom/Documents/Mendeley/Lebrun, Leclaire_2012_An Implementation and Detailed Analysis of the K-SVD Image Denoising Algorithm.pdf:pdf},
issn = {2105-1232},
journal = {Image Processing On Line},
keywords = {denoising,dictionaries,learning,patches,sparse representation},
pages = {96--133},
title = {{An Implementation and Detailed Analysis of the K-SVD Image Denoising Algorithm}},
url = {/Users/kvitayau1/Desktop/Dissertation/literature/to_sort/006 - Denoising the image/An implementation and detailed analysis of the K-SVD image denoising algorithm.pdf%5Cnhttp://dx.doi.org/10.5201/ipol.2012.llm-ksvd},
volume = {2 VN - re},
year = {2012}
}
@article{Hossein2007,
author = {Hossein, Hassani},
file = {:home/tom/Documents/Mendeley/Hossein_2007_Singular Spectrum Analysis Methodology and Comparison Hossein.pdf:pdf},
journal = {Journal of Data Science},
pages = {239--257},
title = {{Singular Spectrum Analysis: Methodology and Comparison Hossein}},
volume = {5},
year = {2007}
}
@book{rose2006human,
author = {Rose, J and Gamble, J G and Adams, J M},
publisher = {Lippincott Williams \& Wilkins Philadelphia},
title = {{Human walking}},
year = {2006}
}
@techreport{Tseng1988,
author = {Tseng, Paul},
booktitle = {Technical Report LIDS-P-1840, Massachusetts Institute of Technology, Laboratory for Information and Decision Systems},
file = {:home/tom/Documents/Mendeley/Tseng_1988_Coordinate Ascent for Maximizing Nondifferentiable Concave Functions.pdf:pdf;:home/tom/Documents/Mendeley/Tseng_1988_Coordinate Ascent for Maximizing Nondifferentiable Concave Functions(2).pdf:pdf},
institution = {Laboratory for Information and Decision Systems (LIDS), MIT},
keywords = {coordinate ascent,method of multipliers,separable convex programming},
title = {{Coordinate Ascent for Maximizing Nondifferentiable Concave Functions}},
volume = {0171},
year = {1988}
}
@article{Oculo3,
author = {Brodsky, MC and Keating, GF},
journal = {Neuro-Opthalmology},
number = {34},
pages = {274--275},
title = {{Chiasmal glioma in spasmus nutans: a cautionary note}},
volume = {Sep},
year = {2014}
}
@article{larochelle2009exploring,
author = {Larochelle, Hugo and Bengio, Yoshua and Louradour, J{\'{e}}r{\^{o}}me and Lamblin, Pascal},
journal = {Journal of Machine Learning Research (JMLR)},
number = {Jan},
pages = {1--40},
title = {{Exploring strategies for training deep neural networks}},
volume = {10},
year = {2009}
}
@article{VanHorn2008,
author = {{Van Horn}, Marion R. and Cullen, Kathleen E.},
journal = {Journal of Neurophysiology},
keywords = {Oculo},
mendeley-tags = {Oculo},
number = {4},
pages = {1967----1982},
title = {{Dynamic Coding of Vertical Facilitated Vergence by Premotor Saccadic Brust Neurons}},
volume = {100},
year = {2008}
}
@book{Brockwell2009,
author = {Brockwell, Peter J. and Davis, Richard A.},
doi = {10.1007/978-0-387-98135-2},
file = {:home/tom/Documents/Mendeley/Brockwell, Davis_2009_Time Series Theory and Methods.pdf:pdf},
isbn = {1441903208},
issn = {01727397},
pages = {579},
pmid = {15772297},
title = {{Time Series: Theory and Methods}},
url = {https://books.google.com/books?id=TVIpBgAAQBAJ&pgis=1},
year = {2009}
}
@inproceedings{Kalchbrenner2014,
abstract = {The ability to accurately represent sen- tences is central to language understand- ing. We describe a convolutional architec- ture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pool- ing, a global pooling operation over lin- ear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily ap- plicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment predic- tion, six-way question classification and Twitter sentiment prediction by distant su- pervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.},
address = {Baltimore, USA},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.2188v1},
author = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
booktitle = {Annual meeting of the Association of Computational Linguistics (ACL)},
eprint = {arXiv:1404.2188v1},
file = {:home/tom/Documents/Mendeley/Kalchbrenner, Grefenstette, Blunsom_2014_A Convolutional Neural Network for Modelling Sentences.pdf:pdf},
keywords = {Deconvolution,Neural net,convolution,transport},
mendeley-tags = {Deconvolution,transport},
pages = {212----217},
publisher = {Association of Computational Linguistics (ACL)},
title = {{A Convolutional Neural Network for Modelling Sentences}},
year = {2014}
}
@article{Karimireddy2018,
archivePrefix = {arXiv},
arxivId = {1810.06999},
author = {Karimireddy, Sai Praneeth and Koloskova, Anastasia and Stich, Sebastian U. and Jaggi, Martin},
eprint = {1810.06999},
file = {:home/tom/Documents/Mendeley/Karimireddy et al._2018_Efficient Greedy Coordinate Descent for Composite Problems.pdf:pdf},
journal = {preprint ArXiv},
title = {{Efficient Greedy Coordinate Descent for Composite Problems}},
url = {http://arxiv.org/abs/1810.06999},
volume = {1810.06999},
year = {2018}
}
@article{Zok2004,
abstract = {A double integration technique is presented that estimates whole body centre of mass (CoM) displacement from signals of a single force platform, compensating for the drift and low frequency noise inherent in the signals. The technique is composed of two different integration techniques, which may also be used separately, and is applied to transitory motor tasks with known initial and final conditions such as step ascent and descent, single step, etc. First, the lowest frequencies within the force platform signals and considered not to be associated with actual movement are filtered out. Second, a regular and a time-reversed double integration are performed and weighted against each other. The technique's accuracy was assessed using computer generated force platform signals that were artificially perturbed. Experimental data were used to compare the estimated CoM displacement to that obtained from a regular double integration and from segmental analysis performed on stereophotogrammetric data. It was shown that the proposed technique's CoM displacement estimates were more repeatable and up to 50% more accurate than those of a regular double integration. Moreover, the CoM displacement estimated using a single force platform and the proposed technique was found to be not statistically different from that obtained with more demanding measurement and processing techniques such as stereophotogrammetry and segmental analysis. ?? 2004 IPEM. Published by Elsevier Ltd. All rights reserved.},
author = {Zok, Mounir and Mazz{\`{a}}, Claudia and {Della Croce}, Ugo},
doi = {10.1016/j.medengphy.2004.07.005},
file = {:home/tom/Documents/Mendeley/Zok, Mazz{\`{a}}, Della Croce_2004_Total body centre of mass displacement estimated using ground reactions during transitory motor tasks Appl.pdf:pdf},
isbn = {1350-4533 (Print)\r1350-4533 (Linking)},
issn = {13504533},
journal = {Medical Engineering and Physics},
keywords = {Centre of mass,Force platform,Integration,Movement analysis},
number = {9 SPEC.ISS.},
pages = {791--798},
pmid = {15564116},
title = {{Total body centre of mass displacement estimated using ground reactions during transitory motor tasks: Application to step ascent}},
volume = {26},
year = {2004}
}
@article{cadieu2012learning,
author = {Cadieu, Charles F. and Olshausen, Bruno A.},
file = {:home/tom/Documents/Mendeley//Cadieu, Olshausen_2012_Learning intermediate-level representations of form and motion from natural movies.pdf:pdf},
journal = {Neural computation},
number = {4},
pages = {827--866},
publisher = {MIT Press},
title = {{Learning intermediate-level representations of form and motion from natural movies}},
volume = {24},
year = {2012}
}
@inproceedings{Bradley2009,
author = {Bradley, David M and Bagnell, J Andrew},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Documents/Mendeley/Bradley, Bagnell_2009_Differentiable Sparse Coding.pdf:pdf},
isbn = {9781605609492},
pages = {113--120},
title = {{Differentiable Sparse Coding}},
year = {2009}
}
@techreport{Camero1966,
author = {Camero, Scott H.},
file = {:home/tom/Documents/Mendeley/Camero_1966_Piece-wise Linear Approximations.pdf:pdf},
institution = {Computer Science Division, IIT Research Institute, Chicago, IL, USA},
title = {{Piece-wise Linear Approximations}},
year = {1966}
}
@article{Bay2008,
author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1016/j.cviu.2007.09.014},
file = {:home/tom/Documents/Mendeley/Bay et al._2008_Speeded-Up Robust Features.pdf:pdf},
isbn = {9783540338321},
issn = {10773142},
journal = {Computer Vision and Image Understanding (CVIU)},
keywords = {camera calibration,feature description,interest points,local features,object recognition},
number = {September},
pages = {346--359},
pmid = {16081019},
title = {{Speeded-Up Robust Features}},
url = {papers2://publication/uuid/54DC4FA5-76E1-4FF2-A32D-C2900405827E},
volume = {110},
year = {2008}
}
@article{Barber2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1404.5609v1},
author = {Barber, Rina F. and Cand{\`{e}}s, Emmanuel J.},
eprint = {arXiv:1404.5609v1},
file = {:home/tom/Documents/Mendeley/Barber, Cand{\`{e}}s_2014_Controlling the false discovery rate via knockoffs.pdf:pdf},
journal = {preprint ArXiv},
keywords = {false discovery rate,fdr,lasso,martingale theory,mutation methods,sequential hypothesis testing,variable selection},
title = {{Controlling the false discovery rate via knockoffs}},
url = {http://arxiv.org/abs/1404.5609},
volume = {1404.5609},
year = {2014}
}
@article{Hesterberg2008,
archivePrefix = {arXiv},
arxivId = {0802.0964},
author = {Hesterberg, Tim and Choi, Nam Hee and Meier, Lukas and Fraley, Chris},
doi = {10.1214/08-SS035},
eprint = {0802.0964},
file = {:home/tom/Documents/Mendeley//Hesterberg et al._2008_Least angle and $ell_1$ penalized regression A review.pdf:pdf},
isbn = {1935-7516},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {Received,lasso,regression,regularization,vari- able selection.,ℓ1 penalty},
pages = {61--93},
title = {{Least angle and $\ell_1$ penalized regression: A review}},
volume = {2},
year = {2008}
}
@article{Jafari2011,
author = {Jafari, Maria G. and Plumbley, Mark D.},
doi = {10.1109/JSTSP.2011.2157892},
file = {:home/tom/Documents/Mendeley/Jafari, Plumbley_2011_Fast Dictionary Learning for Sparse Representations of Speech Signals.pdf:pdf},
isbn = {1932-4553},
issn = {1932-4553},
journal = {IEEE Journal of Selected Topics in Signal Processing},
keywords = {Adaptive dictionary,dictionary learning,sparse decomposition,sparse dictionary,speech analysis,speech denoising},
number = {5},
pages = {1025--1031},
title = {{Fast Dictionary Learning for Sparse Representations of Speech Signals}},
volume = {5},
year = {2011}
}
@inproceedings{Chainais2013,
address = {St. Martin, France},
annote = {Diffusion based algorithm to estimate a dictionary over a network of workers.
There is no proof and little experiements.},
archivePrefix = {arXiv},
arxivId = {arXiv:1304.3568v1},
author = {Chainais, Pierre and Richard, Cedric},
booktitle = {IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)},
doi = {10.1109/CAMSAP.2013.6714025},
eprint = {arXiv:1304.3568v1},
file = {:home/tom/Documents/Mendeley/Chainais, Richard_2013_Learning a common dictionary over a sensor network.pdf:pdf},
isbn = {9781467331463},
keywords = {adaptive networks,block coordinate descent,dictionary learning,diffusion,dis-,matrix factorization,sparse coding,tributed estimation},
pages = {133--136},
title = {{Learning a common dictionary over a sensor network}},
year = {2013}
}
@inproceedings{Chen2016,
address = {Phoenix, United States},
author = {Chen, Boheng and Li, Jie and Ma, Biyun and Gang, Wei},
booktitle = {IEEE International Conference on Image Processing (ICIP)},
file = {:home/tom/Documents/Mendeley/Chen et al._2016_Convolutional Sparse Coding Classification Model for Image Classification.pdf:pdf},
pages = {1918----1922},
title = {{Convolutional Sparse Coding Classification Model for Image Classification}},
year = {2016}
}
@article{DeCock2002,
author = {{De Cock}, Katrien and {De Moor}, Bart},
file = {:home/tom/Documents/Mendeley/De Cock, De Moor_2002_Subspace angles and distances between ARMA models.pdf:pdf},
journal = {System and Control Letter},
keywords = {arma models,distance measure,linear sys-,principal angles,stochastic realization,tems},
number = {4},
pages = {265--270},
title = {{Subspace angles and distances between ARMA models}},
volume = {46},
year = {2002}
}
@inproceedings{krizhevsky2012imagenet,
address = {South Lake Tahoe, United States},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in neural information processing systems (NIPS)},
pages = {1097--1105},
title = {{Imagenet classification with deep convolutional neural networks}},
year = {2012}
}
@article{karantonis2006implementation,
author = {Karantonis, D M and Narayanan, M R and Mathie, M and Lovell, N H and Celler, B G},
journal = {IEEE Transactions on Information Technology in Biomedicine},
number = {1},
pages = {156--167},
publisher = {IEEE},
title = {{Implementation of a real-time human movement classifier using a triaxial accelerometer for ambulatory monitoring}},
volume = {10},
year = {2006}
}
@inproceedings{Johnson2015,
abstract = {By reducing optimization to a sequence of small subproblems, working set methods achieve fast convergence times for many challenging problems. Despite excellent performance, theoretical understanding of working sets is limited, and implementations often resort to heuristics to determine subproblem size, makeup, and stopping criteria. We propose Blitz, a fast working set algorithm accompanied by useful guarantees. Making no assumptions on data, our theory relates subproblem size to progress toward convergence. This result motivates methods for optimizing algorithmic parameters and discarding irrelevant variables as iterations progress. Applied to L1-regularized learning, Blitz convincingly outperforms existing solvers in sequential, limited-memory, and distributed settings. Blitz is not specific to L1-regularized learning, making the algorithm relevant to many applications involving sparsity or constraints.},
address = {Lille, France},
author = {Johnson, Tyler and Guestrin, Carlos},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:home/tom/Documents/Mendeley/Johnson, Guestrin_2015_Blitz A Principled Meta-Algorithm for Scaling Sparse Optimization.pdf:pdf},
pages = {1171--1179},
title = {{Blitz: A Principled Meta-Algorithm for Scaling Sparse Optimization}},
year = {2015}
}
@inproceedings{Neyshabur2015,
abstract = {We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1506.02617},
author = {Neyshabur, Behnam and Salakhutdinov, Ruslan and Srebro, Nathan},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1506.02617},
file = {:home/tom/Documents/Mendeley/Neyshabur, Salakhutdinov, Srebro_2015_Path-SGD Path-Normalized Optimization in Deep Neural Networks.pdf:pdf},
issn = {10495258},
pages = {2422--2430},
title = {{Path-SGD: Path-Normalized Optimization in Deep Neural Networks}},
url = {http://arxiv.org/abs/1506.02617},
year = {2015}
}
@article{Jones2009,
abstract = {Variations in cortical oscillations in the alpha (7-14 Hz) and beta (15-29 Hz) range have been correlated with attention, working memory, and stimulus detection. The mu rhythm recorded with magnetoencephalography (MEG) is a prominent oscillation generated by Rolandic cortex containing alpha and beta bands. Despite its prominence, the neural mechanisms regulating mu are unknown. We characterized the ongoing MEG mu rhythm from a localized source in the finger representation of primary somatosensory (SI) cortex. Subjects showed variation in the relative expression of mu-alpha or mu-beta, which were nonoverlapping for roughly 50% of their respective durations on single trials. To delineate the origins of this rhythm, a biophysically principled computational neural model of SI was developed, with distinct laminae, inhibitory and excitatory neurons, and feedforward (FF, representative of lemniscal thalamic drive) and feedback (FB, representative of higher-order cortical drive or input from nonlemniscal thalamic nuclei) inputs defined by the laminar location of their postsynaptic effects. The mu-alpha component was accurately modeled by rhythmic FF input at approximately 10-Hz. The mu-beta component was accurately modeled by the addition of approximately 10-Hz FB input that was nearly synchronous with the FF input. The relative dominance of these two frequencies depended on the delay between FF and FB drives, their relative input strengths, and stochastic changes in these variables. The model also reproduced key features of the impact of high prestimulus mu power on peaks in SI-evoked activity. For stimuli presented during high mu power, the model predicted enhancement in an initial evoked peak and decreased subsequent deflections. In agreement, the MEG-evoked responses showed an enhanced initial peak and a trend to smaller subsequent peaks. These data provide new information on the dynamics of the mu rhythm in humans and the model provides a novel mechanistic interpretation of this rhythm and its functional significance.},
author = {Jones, Stephanie R. and Pritchett, D. L. and Sikora, M. A. and Stufflebeam, S. M. and Hamalainen, M. and Moore, C. I.},
doi = {10.1152/jn.00535.2009},
file = {:home/tom/Documents/Mendeley/Jones et al._2009_Quantitative Analysis and Biophysically Realistic Neural Modeling of the MEG Mu Rhythm Rhythmogenesis and Modulation o.pdf:pdf},
isbn = {1522-1598 (Electronic)\r0022-3077 (Linking)},
issn = {0022-3077},
journal = {Journal of Neurophysiology},
number = {6},
pages = {3554--3572},
pmid = {19812290},
title = {{Quantitative Analysis and Biophysically Realistic Neural Modeling of the MEG Mu Rhythm: Rhythmogenesis and Modulation of Sensory-Evoked Responses}},
url = {http://jn.physiology.org/cgi/doi/10.1152/jn.00535.2009},
volume = {102},
year = {2009}
}
@inproceedings{JiaDeng2009,
address = {Miami Beach, FL, USA},
author = {{Jia Deng} and {Wei Dong} and Socher, R. and {Li-Jia Li} and {Kai Li} and {Li Fei-Fei}},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPRW.2009.5206848},
file = {:home/tom/Documents/Mendeley/Jia Deng et al._2009_ImageNet A large-scale hierarchical image database.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
pages = {248--255},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206848},
year = {2009}
}
@misc{Python36,
author = {{Python Software Fundation}},
pages = {https://python.org},
title = {{Python Language Reference, version 3.6}},
year = {2017}
}
@phdthesis{mariani,
author = {Mariani, B},
school = {EPFL},
title = {{Assessment of Foot Signature Using Wearable Sensors for Clinical Gait Analysis and Real-Time Activity Recognition}},
year = {2012}
}
@inproceedings{brajdic2013walk,
address = {Zurich, Switzerland},
author = {Brajdic, A and Harle, R},
booktitle = {ACM international joint conference on Pervasive and ubiquitous computing},
organization = {ACM},
pages = {225--234},
title = {{Walk detection and step counting on unconstrained smartphones}},
year = {2013}
}
@article{Andoni2014,
author = {Andoni, Alexandr and Panigrahy, Rina and Valiant, Gregory and Zhang, Li},
file = {:home/tom/Documents/Mendeley/Andoni et al._2014_Learning Polynomials with Neural Networks.pdf:pdf},
journal = {Journal of Machine Learning Research (JMLR)},
number = {2},
pages = {1908--1916},
title = {{Learning Polynomials with Neural Networks}},
volume = {32},
year = {2014}
}
@inproceedings{Sakoe1971,
address = {Budapest, Hungary},
author = {Sakoe, Hiroaki and Chiba, Seibi},
booktitle = {International Congress on Acoustics},
keywords = {2000 book nlp},
pages = {65--69},
publisher = {{Akad{\'{e}}miai} {Kiad{\'{o}}}},
title = {{A Dynamic Programming Approach to Continuous Speech Recognition}},
volume = {3},
year = {1971}
}
@inproceedings{hinton2008using,
address = {Vancouver, Canada},
author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1249--1256},
title = {{Using deep belief nets to learn covariance kernels for Gaussian processes}},
year = {2008}
}
@article{Zou2006a,
abstract = {penalty; the lasso penalty (via the elastic net) can then be directly integrated into the regression criterion, leading to a modified with loadings.},
annote = {* True sparse PCA - with orthogonality constraints},
archivePrefix = {arXiv},
arxivId = {1205.0121v2},
author = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1198/106186006X113430},
eprint = {1205.0121v2},
file = {:home/tom/Documents/Mendeley/Zou, Hastie, Tibshirani_2006_Sparse principal component analysis.pdf:pdf},
isbn = {106186006X},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
number = {2},
pages = {262--286},
pmid = {21811560},
title = {{Sparse principal component analysis}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed&cmd=Retrieve&dopt=AbstractPlus&list_uids=10700132108553319406related:7nMSBHaBfpQJ},
volume = {15},
year = {2006}
}
@inproceedings{veltkamp2001shape,
address = {Genova, Italy},
author = {Veltkamp, R C},
booktitle = {International Conference on Shape Modeling and Applications (SMI)},
pages = {188--197},
title = {{Shape matching: Similarity measures and algorithms}},
year = {2001}
}
@techreport{krizhevsky2009learning,
author = {Krizhevsky, Alex},
institution = {University of Toronto},
title = {{Learning multiple layers of features from tiny images}},
type = {Master's thesis},
year = {2009}
}
@article{Luo1993,
author = {Luo, Zhi Quan and Tseng, Paul},
doi = {10.1007/BF02096261},
file = {:home/tom/Documents/Mendeley/Luo, Tseng_1993_Error bounds and convergence analysis of feasible descent methods a general approach.pdf:pdf},
issn = {02545330},
journal = {Annals of Operations Research},
keywords = {Error bound,feasible descent methods,linear convergence},
number = {1},
pages = {157--178},
title = {{Error bounds and convergence analysis of feasible descent methods: a general approach}},
volume = {46-47},
year = {1993}
}
@article{Grosse2007,
author = {Grosse, Roger and Raina, Rajat and Kwong, Helen and Ng, Andrew Y},
file = {:home/tom/Documents/Mendeley/Grosse et al._2007_Shift-Invariant Sparse Coding for Audio Classification.pdf:pdf},
journal = {Cortex},
keywords = {Convolution,dictionary learning,sparse coding},
pages = {9},
title = {{Shift-Invariant Sparse Coding for Audio Classification}},
volume = {8},
year = {2007}
}
@article{Scheinberg2013,
archivePrefix = {arXiv},
arxivId = {1311.6547},
author = {Scheinberg, Katya and Tang, Xiaocheng},
eprint = {1311.6547},
file = {:home/tom/Documents/Mendeley//Scheinberg, Tang_2013_Practical Inexact Proximal Quasi-Newton Method with Global Complexity Analysis.pdf:pdf},
journal = {preprint ArXiv},
title = {{Practical Inexact Proximal Quasi-Newton Method with Global Complexity Analysis}},
url = {http://arxiv.org/abs/1311.6547},
volume = {1311.6547},
year = {2013}
}
@article{kim2004step,
author = {Kim, J and Jang, H and Hwang, D.-H. and Park, C},
journal = {Journal of Global Positioning Systems},
number = {1-2},
pages = {273--289},
publisher = {Scientific Research Publishing},
title = {{A step, stride and heading determination for the pedestrian navigation system}},
volume = {3},
year = {2004}
}
@inproceedings{Keogh2001,
abstract = {In recent years, there has been an explosion of interest in mining time series databases. As with most computer science problems, representation of the data is the key to efficient and effective solutions. One of the most commonly used representations is piecewise linear approximation. This representation has been used by various researchers to support clustering, classification, indexing and association rule mining of time series data. A variety of algorithms have been proposed to obtain this representation, with several algorithms having been independently rediscovered several times. In this paper, we undertake the first extensive review and empirical comparison of all proposed iechniques. We show that all these algorithms have fatal flaws from a data mining perspective. We introduce a novel algorithm that we empirically show to be superior to all others in the literature.},
address = {San Jose, United States},
author = {Keogh, Eamonn and Chu, S. and Hart, D. and Pazzani, M.},
booktitle = {IEEE International Conference on Data Mining (ICDM)},
doi = {10.1109/ICDM.2001.989531},
file = {:home/tom/Documents/Mendeley/Keogh et al._2001_An online algorithm for segmenting time series.pdf:pdf},
isbn = {0-7695-1119-8},
issn = {15504786},
pages = {289--296},
publisher = {IEEE},
title = {{An online algorithm for segmenting time series}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=989531},
year = {2001}
}
@inproceedings{jimenez2009comparison,
author = {Jimenez, A R and Seco, F and Prieto, C and Guevara, J},
booktitle = {International Symposium on Intelligent Signal Processing (WISP)},
organization = {IEEE},
pages = {37--42},
title = {{A comparison of pedestrian dead-reckoning algorithms using a low-cost MEMS IMU}},
year = {2009}
}
@article{Agarwal2014,
archivePrefix = {arXiv},
arxivId = {1310.7991},
author = {Agarwal, Alekh and Anandkumar, Animashree and Jain, Prateek},
eprint = {1310.7991},
file = {:home/tom/Documents/Mendeley/Agarwal, Anandkumar, Jain_2014_Learning sparsely used overcomplete dictionaries via alternating minimization.pdf:pdf},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {alternating minimization,dictionary learning,incoherence,lasso,sparse coding},
pages = {1--15},
title = {{Learning sparsely used overcomplete dictionaries via alternating minimization}},
url = {http://arxiv.org/abs/1310.7991},
volume = {35},
year = {2014}
}
@article{Bioucas-Dias2007,
author = {Bioucas-Dias, Jos?? M. and Figueiredo, M??rio A T},
doi = {10.1109/TIP.2007.909319},
file = {:home/tom/Documents/Mendeley/Bioucas-Dias, Figueiredo_2007_A new TwIST Two-step iterative shrinkagethresholding algorithms for image restoration.pdf:pdf},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Algorithm design and analysis,Convex analysis,Deconvolution,Image deconvolution,Image restoration,Non-smooth optimization,Optimization,Regularization,Total variation,Wavelets},
number = {12},
pages = {2992--3004},
pmid = {18092598},
title = {{A new TwIST: Two-step iterative shrinkage/thresholding algorithms for image restoration}},
volume = {16},
year = {2007}
}
@article{Zanella2009a,
abstract = {Random matrices play a crucial role in the design and analysis of multiple-input multiple-output (MIMO) systems. In particular, performance of MIMO systems depends on the statistical properties of a subclass of random matrices known as Wishart when the propagation environment is characterized by Rayleigh or Rician fading. This paper focuses on the stochastic analysis of this class of matrices and proposes a general methodology to evaluate some multiple nested integrals of interest. With this methodology we obtain a closed-form expression for the joint probability density function of k consecutive ordered eigenvalues and, as a special case, the PDF of the lscrth ordered eigenvalue of Wishart matrices. The distribution of the largest eigenvalue can be used to analyze the performance of MIMO maximal ratio combining systems. The PDF of the smallest eigenvalue can be used for MIMO antenna selection techniques. Finally, the PDF the kth largest eigenvalue finds applications in the performance analysis of MIMO singular value decomposition systems.},
author = {Zanella, Alberto and Chiani, Marco and Win, Moe Z.},
doi = {10.1109/TCOMM.2009.04.070143},
file = {:home/tom/Documents/Mendeley/Zanella, Chiani, Win_2009_On the marginal distribution of the eigenvalues of wishart matrices.pdf:pdf},
isbn = {9781424420742},
issn = {00906778},
journal = {IEEE Transactions on Communications},
keywords = {Eeigenvalue distribution,Marginal distribution,Multiple-input multiple-output (MIMO),Wishart matrices},
number = {4},
pages = {1050--1060},
title = {{On the marginal distribution of the eigenvalues of wishart matrices}},
volume = {57},
year = {2009}
}
@article{Song1997,
author = {Song, D and Gupta, Arjun K.},
file = {:home/tom/Documents/Mendeley//Song, Gupta_1997_$L_p$-norm Uniform Distribution.pdf:pdf},
journal = {The American Mathematical Society},
number = {2},
pages = {595--601},
title = {{$L_p$-norm Uniform Distribution}},
volume = {125},
year = {1997}
}
@article{Oculo17,
author = {Kelly, TW},
journal = {Pediatrics},
number = {45},
pages = {295--296},
title = {{Optic glioma presenting as spasmus nutans}},
volume = {Feb},
year = {1970}
}
@article{Golyandina2015,
archivePrefix = {arXiv},
arxivId = {1309.5050},
author = {Golyandina, Nina and Korobeynikov, Anton and Shlemov, Alex and Usevich, Konstantin},
doi = {10.18637/jss.v067.i02},
eprint = {1309.5050},
file = {:home/tom/Documents/Mendeley/Golyandina et al._2015_Multivariate and 2D Extensions of Singular Spectrum Analysis with the Rssa Package.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {analysis,decomposition,forecasting,image processing,r package,singular spectrum analysis,time series},
number = {1},
pages = {1--78},
title = {{Multivariate and 2D Extensions of Singular Spectrum Analysis with the Rssa Package}},
url = {http://arxiv.org/abs/1309.5050%0Ahttp://dx.doi.org/10.18637/jss.v067.i02},
volume = {67},
year = {2015}
}
@misc{Olshausen1997,
author = {Olshausen, Bruno A. and Field, David J},
booktitle = {Vision Research},
doi = {http://dx.doi.org/10.1016/S0042-6989(97)00169-7},
file = {:home/tom/Documents/Mendeley/Olshausen, Field_1997_Sparse coding with an incomplete basis set a strategy employed by protect{V1}.pdf:pdf},
isbn = {0042-6989},
issn = {00426989},
keywords = {Coding,Gabor-wavelet,Natural images,V1},
number = {23},
pages = {3311--3325},
pmid = {9425546},
title = {{Sparse coding with an incomplete basis set: a strategy employed by \protect{V1}}},
url = {http://www.sciencedirect.com/science/article/pii/S0042698997001697},
volume = {37},
year = {1997}
}
@inproceedings{Berthet2013,
address = {Princeton, United States},
archivePrefix = {arXiv},
arxivId = {arXiv:1304.0828v2},
author = {Berthet, Quentin and Rigollet, Philippe},
booktitle = {Conference on Learning Theory (COLT)},
eprint = {arXiv:1304.0828v2},
file = {:home/tom/Documents/Mendeley/Berthet, Rigollet_2013_Computational Lower Bounds for Sparse PCA.pdf:pdf},
keywords = {and phrases,planted,polynomial-time reduction,sparse pca},
pages = {1046--1066},
title = {{Computational Lower Bounds for Sparse PCA}},
url = {http://arxiv.org/abs/1304.0828},
year = {2013}
}
@inproceedings{Pascanu2014,
abstract = {We study the complexity of functions computable by deep feedforward neural networks with piece-wise linear activations in terms of the number of regions of linearity that they have. Deep networks are able to sequentially map portions of each layer's input space to the same output. In this way, deep models compute functions with a compositional structure that is able to re-use pieces of computation exponentially often in terms of their depth. This note investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piece-wise linear activation functions.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1869v1},
author = {Mont\'ufar, Guido F. and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1007/978-1-4471-5779-3_4},
eprint = {arXiv:1402.1869v1},
file = {:home/tom/Documents/Mendeley//Mont'ufar et al._2014_On the Number of Linear Regions of Deep Neural Networks.pdf:pdf},
isbn = {978-1-4471-5779-3; 978-1-4471-5778-6},
issn = {10495258},
keywords = {deep learning,input space partition,maxout,neural network,rectifier},
pages = {2924--2932},
title = {{On the Number of Linear Regions of Deep Neural Networks}},
year = {2014}
}
@article{Oculo30,
author = {Smith, JL and Flynn, JT and Spiro, HJ},
journal = {Journal of Clinical Neuro-Ophthalmology},
number = {2},
pages = {85--91},
title = {{Monocular vertical oscillations of amblyopia: The Heimann- Bielschowsky phenomenon}},
volume = {Jun},
year = {1982}
}
@inproceedings{Barchiesi2011,
address = {Prague, Czech Republic},
author = {Barchiesi, Daniele and Plumbley, Mark D.},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2011.5947682},
file = {:home/tom/Documents/Mendeley/Barchiesi, Plumbley_2011_Dictionary Learning of Convolved Signals.pdf:pdf},
keywords = {Convolution,dictionary learning},
pages = {5812--5815},
title = {{Dictionary Learning of Convolved Signals}},
year = {2011}
}
@inproceedings{Dalal2005,
address = {San Diego, CA, USA},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Dalal, Navneet and Triggs, Bill},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2005.177},
eprint = {9411012},
file = {:home/tom/Documents/Mendeley/Dalal, Triggs_2005_Histograms of Oriented Gradients for Human Detection.pdf:pdf},
isbn = {0-7695-2372-2},
issn = {1063-6919},
pages = {886--893},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{Histograms of Oriented Gradients for Human Detection}},
year = {2005}
}
@article{Becker2009,
author = {Becker, Stephen and Bobin, J{\'{e}}r{\^{o}}me and Cand{\`{e}}s, Emmanuel J.},
file = {:home/tom/Documents/Mendeley/Becker, Bobin, Cand{\`{e}}s_2009_Nesta a fast and accurate first-order method for sparse recovery ´.pdf:pdf},
issn = {19364954},
journal = {SIAM Journal on Imaging Sciences},
keywords = {1 mini-,compressed sensing,continuation methods,duality in convex optimization,functions,mization,nesterov,s method,smooth approximations of nonsmooth,total-variation},
number = {1},
pages = {1--37},
title = {{Nesta: a fast and accurate first-order method for sparse recovery ´}},
volume = {91125},
year = {2009}
}
@article{Chen1994,
author = {Chen, Gong and Teboulle, Marc},
journal = {Mathematical Programming},
number = {1-3},
pages = {81--101},
title = {{A proximal-based decomposition method for convex minimization problems}},
volume = {64},
year = {1994}
}
@inproceedings{Sermanet2014,
abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
booktitle = {International Conference on Learning Representation (ICLR)},
doi = {10.1109/CVPR.2015.7299176},
eprint = {1312.6229},
file = {:home/tom/Documents/Mendeley/Sermanet et al._2014_OverFeat Integrated Recognition, Localization and Detection using Convolutional Networks.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
pmid = {1000200972},
title = {{OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}},
url = {http://arxiv.org/abs/1312.6229},
year = {2014}
}
@misc{Nemirovski2012,
author = {Nemirovski, Arkadi},
file = {:home/tom/Documents/Mendeley/Nemirovski_2012_Tutorial Mirror Descent Algorithms for Large-Scale Deterministic and stochastic convex Optimization.pdf:pdf},
title = {{Tutorial: Mirror Descent Algorithms for Large-Scale Deterministic and stochastic convex Optimization}},
year = {2012}
}
@misc{Moreau2017b,
address = {Paris, France},
author = {Moreau, Thomas and Grisel, Olivier},
booktitle = {PyParis},
howpublished = {Oral Presentation, PyParis},
title = {{Robustifying {\tt concurrent.futures}}},
year = {2017}
}
@article{Oculo7,
author = {Farmer, J and Hoyt, CS},
journal = {American Journal of Ophthalmology},
number = {98},
pages = {504--509},
title = {{Monocular nystagmus in infancy and early childhood}},
volume = {Oct},
year = {1984}
}
@inproceedings{DAspremont2007a,
address = {Corvallis, United States},
archivePrefix = {arXiv},
arxivId = {arXiv:0707.0705v3},
author = {D'Aspremont, Alexandre and Bach, Francis R and Ghaoui, Laurent El},
booktitle = {International Conference on Machine Learning (ICML)},
doi = {10.1145/1273496.1273519},
eprint = {arXiv:0707.0705v3},
file = {:home/tom/Documents/Mendeley/d'Aspremont, Bach, Ghaoui_2007_Full regularization path for sparse principal component analysis.pdf:pdf},
isbn = {9781595937933},
number = {1},
pages = {177--184},
title = {{Full regularization path for sparse principal component analysis}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273519},
year = {2007}
}
@inproceedings{Lee2009,
abstract = {There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3605v3},
author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y.},
booktitle = {International Conference on Machine Learning (ICML)},
doi = {10.1145/1553374.1553453},
eprint = {arXiv:1301.3605v3},
file = {:home/tom/Documents/Mendeley/Lee et al._2009_Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations.pdf:pdf},
isbn = {9781605585161},
issn = {02643294},
pages = {1--8},
pmid = {20957573},
title = {{Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553453},
year = {2009}
}
@inproceedings{Pati1993,
abstract = {We describe a recursive algorithm to compute representations of functions with respect to nonorthogonal and possibly overcomplete dictionaries of elementary building blocks e.g. affine (wavelet) frames. We propose a modification to the matching pursuit algorithm of Mallat and Zhang (1992) that maintains full backward orthogonality of the residual (error) at every step and thereby leads to improved convergence. We refer to this modified algorithm as orthogonal matching pursuit (OMP). It is shown that all additional computation required for the OMP algorithm may be performed recursively},
address = {Pacific Grove, CA, USA},
author = {Pati, Y.C. and Rezaiifar, R. and Krishnaprasad, P.S.},
booktitle = {Asilomar Conference on Signals, Systems and Computers},
doi = {10.1109/ACSSC.1993.342465},
file = {:home/tom/Documents/Mendeley/Pati, Rezaiifar, Krishnaprasad_1993_Orthogonal matching pursuit recursive function approximation with applications to wavelet decomposit.pdf:pdf},
isbn = {0-8186-4120-7},
issn = {1058-6393},
pages = {40--44},
title = {{Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition}},
url = {http://ieeexplore.ieee.org/document/342465/},
year = {1993}
}
@article{Necoara2013,
abstract = {In this paper we propose a parallel coordinate descent algorithm for solving smooth convex optimization problems with separable constraints that may arise, e.g. in distributed model predictive control (MPC) for linear network systems. Our algorithm is based on block coordinate descent updates in parallel and has a very simple iteration. We prove (sub)linear rate of convergence for the new algorithm under standard assumptions for smooth convex optimization. Further, our algorithm uses local information and thus is suitable for distributed implementations. Moreover, it has low iteration complexity, which makes it appropriate for embedded control. An MPC scheme based on this new parallel algorithm is derived, for which every subsystem in the network can compute feasible and stabilizing control inputs using distributed and cheap computations. For ensuring stability of the MPC scheme, we use a terminal cost formulation derived from a distributed synthesis. Preliminary numerical tests show better performance for our optimization algorithm than other existing methods. ?? 2013 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1302.3092v1},
author = {Necoara, Ion and Clipici, Dragos},
doi = {10.1016/j.jprocont.2012.12.012},
eprint = {arXiv:1302.3092v1},
file = {:home/tom/Documents/Mendeley/Necoara, Clipici_2013_Efficient parallel coordinate descent algorithm for convex optimization problems with separable constraints Applic.pdf:pdf},
issn = {09591524},
journal = {Journal of Process Control},
keywords = {(Sub)linear convergence rate,Coordinate descent optimization,Distributed model predictive control,Embedded control,Parallel algorithm},
number = {3},
pages = {243--253},
publisher = {Elsevier Ltd},
title = {{Efficient parallel coordinate descent algorithm for convex optimization problems with separable constraints: Application to distributed MPC}},
url = {http://dx.doi.org/10.1016/j.jprocont.2012.12.012},
volume = {23},
year = {2013}
}
@book{Bertsekas1997,
author = {Bertsekas, Dimitri P. and Tsitsiklis, John},
file = {:home/tom/Documents/Mendeley/Bertsekas, Tsitsiklis_1997_Parallel and Distributed Computation Numerical Methods (Optimization and Neural Computation).pdf:pdf},
pages = {1----738},
title = {{Parallel and Distributed Computation: Numerical Methods (Optimization and Neural Computation)}},
year = {1997}
}
@article{Mora2013,
author = {Mora, Reynoso and Mora, Pedro Reynoso},
file = {:home/tom/Documents/Mendeley/Mora, Mora_2013_UC Berkeley UC Berkeley Electronic Theses and Dissertations.pdf:pdf},
title = {{UC Berkeley UC Berkeley Electronic Theses and Dissertations}},
year = {2013}
}
@article{Barrois2015,
author = {Barrois, Remi and Oudre, Laurent and Moreau, Thomas and Truong, Charles and Vayatis, Nicolas and Buffat, St\'ephane and Yelnik, Alain and de Waele, Catherine and Gregory, Thomas and Laporte, Serge},
journal = {Computer methods in biomechanics and biomedical engineering},
number = {Sup1},
pages = {1880--1881},
publisher = {Taylor \& Francis},
title = {{Quantify osteoarthritis gait at the doctor's office: a simple pelvis accelerometer based method independent from footwear and aging}},
volume = {18},
year = {2015}
}
@article{Harris1954,
author = {Harris, Zellig S.},
doi = {10.1080/00437956.1954.11659520},
file = {:home/tom/Documents/Mendeley/Harris_1954_Distributional Structure.pdf:pdf},
isbn = {978-90-277-1267-7},
issn = {0043-7956},
journal = {WORD},
number = {2-3},
pages = {146--162},
title = {{Distributional Structure}},
url = {http://www.tandfonline.com/doi/full/10.1080/00437956.1954.11659520},
volume = {10},
year = {1954}
}
@inproceedings{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1411.1792},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1411.1792},
file = {:home/tom/Documents/Mendeley//Yosinski et al._2014_How transferable are features in deep neural networks.pdf:pdf},
pages = {3320--3328},
title = {{How transferable are features in deep neural networks?}},
url = {http://arxiv.org/abs/1411.1792},
year = {2014}
}
@article{Karimi2014,
archivePrefix = {arXiv},
arxivId = {1401.4220},
author = {Karimi, Sahar and Vavasis, Stephen},
eprint = {1401.4220},
file = {:home/tom/Documents/Mendeley//Karimi, Vavasis_2014_IMRO a proximal quasi-Newton method for solving $l_1$-regularized least square problem.pdf:pdf},
journal = {preprint ArXiv},
title = {{IMRO: a proximal quasi-Newton method for solving {\$}l{\_}1{\$}-regularized least square problem}},
volume = {1401.4220},
year = {2014}
}
@article{Condat2013,
author = {Condat, Laurent},
journal = {IEEE Signal Processing Letters},
number = {11},
pages = {1054--1057},
title = {{A Direct Algorithm for 1D Total Variation Denoising To cite this version : HAL Id : hal-00675043 A Direct Algorithm for 1D Total Variation Denoising}},
volume = {20},
year = {2013}
}
@inproceedings{Krishnan2011,
address = {Colorado Spring, CO, USA},
author = {Krishnan, Dilip and Tay, Terence and Fergus, Rob},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Documents/Mendeley/Krishnan, Tay, Fergus_2011_Blind deconvolution using a normalized sparsity measure.pdf:pdf},
isbn = {1457703947},
keywords = {Deconvolution,transport},
mendeley-tags = {Deconvolution,transport},
pages = {233--240},
title = {{Blind deconvolution using a normalized sparsity measure}},
year = {2011}
}
@inproceedings{Alvarez-Meza2013,
address = {Bruges, Belgium},
author = {{\'{A}}lvarez-Meza, A. M. and Acosta-Medina, C. D. and Castellanos-Dom{\'{i}}nguez, G.},
booktitle = {European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)},
file = {:home/tom/Documents/Mendeley/{\'{A}}lvarez-Meza, Acosta-Medina, Castellanos-Dom{\'{i}}nguez_2013_Automatic Singular Spectrum Analysis for Time-Series Decomposition.pdf:pdf},
isbn = {9782874190810},
pages = {131--136},
title = {{Automatic Singular Spectrum Analysis for Time-Series Decomposition}},
year = {2013}
}
@inproceedings{poultney2006efficient,
address = {Vancouver, Canada},
author = {Poultney, Christopher and Chopra, Sumit and Cun, Yann L and Others},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1137--1144},
title = {{Efficient learning of sparse representations with an energy-based model}},
year = {2006}
}
@inproceedings{Liu2010,
abstract = {Recently the low-rank representation (LRR) has been successfully used in exploring the multiple subspace structures of data. It assumes that the observed data is drawn from several low-rank subspaces and sometimes contaminated by outliers and occlusions. However, the noise (low-rank representation residual) is assumed to be sparse, which is generally characterized by minimizing the l1 -norm of the residual. This actually assumes that the residual follows the Laplacian distribution. The Laplacian assumption, however, may not be accurate enough to describe various noises in real scenarios. In this paper, we propose a new framework, termed robust low-rank representation, by considering the low-rank representation as a low-rank constrained estimation for the errors in the observed data. This framework aims to find the maximum likelihood estimation solution of the low-rank representation residuals. We present an efficient iteratively reweighted inexact augmented Lagrange multiplier algorithm to solve the new problem. Extensive experimental results show that our framework is more robust to various noises (illumination, occlusion, etc) than LRR, and also outperforms other state-of-the-art methods.},
address = {Haifa, Israel},
archivePrefix = {arXiv},
arxivId = {0810.3286v1},
author = {Liu, Guangcan and Lin, Zhouchen and Yu, Yong},
booktitle = {International Conference on Machine Learning (ICML)},
doi = {10.1109/TCYB.2013.2286106},
eprint = {0810.3286v1},
file = {:home/tom/Documents/Mendeley/Liu, Lin, Yu_2010_Robust subspace segmentation via low-rank representation.pdf:pdf},
isbn = {9781605589077},
issn = {21682267},
keywords = {Low-rank representation,matrix recovery,robust regression,subspace segmentation},
pages = {663--670},
pmid = {24196982},
title = {{Robust subspace segmentation via low-rank representation}},
year = {2010}
}
@inproceedings{yu2009deep,
address = {Vancouver, Canada},
author = {Yu, Kai and Xu, Wei and Gong, Yihong},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1889--1896},
title = {{Deep learning with kernel regularization for visual recognition}},
year = {2009}
}
@inproceedings{Peyre2011,
abstract = {This paper introduces a novel approach to learn a dictionary in a sparsity-promoting analysis-type prior. The dictionary is opti- mized in order to optimally restore a set of exemplars from their degraded noisy versions. Towards this goal, we cast our prob- lem as a bilevel programming problem for which we propose a gradient descent algorithm to reach a stationary point that might be a local minimizer. When the dictionary analysis operator specializes to a convolution, our method turns out to be a way of learning generalized total variation-type prior. Applications to 1-D signal denoising are reported and potential applicability and extensions are discusses.},
annote = {Give an algorithm to solve the DL problem with a prior analysis approach.
Use a sparsity regularisation based on the Hubert norm (see note)},
author = {Peyr{\'{e}}, Gabriel and Fadili, Jalal M.},
booktitle = {International Conference on Sampling Theory and Applications (SampTA)},
file = {:home/tom/Documents/Mendeley/Peyr{\'{e}}, Fadili_2011_Learning analysis sparsity priors.pdf:pdf},
keywords = {analysis prior,dictionary learning,total varia-},
number = {1},
pages = {2--5},
title = {{Learning analysis sparsity priors}},
url = {http://hal.archives-ouvertes.fr/hal-00542016/},
volume = {2},
year = {2011}
}
@article{Xiang2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1405.4897v1},
author = {Xiang, Zj and Wang, Yun and Ramadge, Pj},
eprint = {arXiv:1405.4897v1},
file = {:home/tom/Documents/Mendeley//Xiang, Wang, Ramadge_2014_Screening tests for lasso problems.pdf:pdf},
journal = {preprint ArXiv},
keywords = {dictionary screening,dual lasso,feature selection,lasso,sparse representation},
title = {{Screening tests for lasso problems}},
url = {http://arxiv.org/abs/1405.4897},
volume = {1405.4897},
year = {2014}
}
@inproceedings{saxe2011random,
address = {Bellevue, United States},
author = {Saxe, Andrew and Koh, Pang W and Chen, Zhenghao and Bhand, Maneesh and Suresh, Bipin and Ng, Andrew Y},
booktitle = {International Conference on Machine Learning (ICML)},
pages = {1089--1096},
title = {{On random weights and unsupervised feature learning}},
year = {2011}
}
@misc{Mairal2010a,
author = {Mairal, Julien},
file = {:home/tom/Documents/Mendeley/Mairal_2010_Sparse Coding and Dictionary Learning for Image Analysis.pdf:pdf},
keywords = {()},
number = {September},
title = {{Sparse Coding and Dictionary Learning for Image Analysis}},
year = {2010}
}
@article{Oculo2,
author = {Averbuch-Heller, L and Zivotofsky, AZ and Remler, BF and Das, VE and Dell'Osso, Louis F and Leigh, RJ},
journal = {Neurology},
number = {45},
pages = {509--515},
title = {{Convergent- divergent pendular nystagmus: possible role of the vergence system}},
volume = {Mar},
year = {1995}
}
@inproceedings{Moreau2017,
abstract = {Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, that are optimal in the class of first-order methods for non-smooth, convex functions, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). However, these methods don't exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks was proposed in \cite{Gregor10}, coined LISTA, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately. In this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the $\ell_1$ ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.},
archivePrefix = {arXiv},
arxivId = {1609.00285},
author = {Moreau, Thomas and Bruna, Joan},
booktitle = {International Conference on Learning Representation (ICLR)},
eprint = {1609.00285},
file = {:home/tom/Documents/Mendeley/Moreau, Bruna_2017_Understanding Neural Sparse Coding with Matrix Factorization.pdf:pdf},
title = {{Understanding Neural Sparse Coding with Matrix Factorization}},
year = {2017}
}
@inproceedings{Elhamifar2012,
author = {Elhamifar, Ehsan and Sapiro, Guillermo and Vidal, Ren{\'{e}}},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2012.6247852},
file = {:home/tom/Documents/Mendeley/Elhamifar, Sapiro, Vidal_2012_See all by looking at a few Sparse modeling for finding representative objects.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
pages = {1600--1607},
pmid = {1006110},
title = {{See all by looking at a few: Sparse modeling for finding representative objects}},
year = {2012}
}
@article{Chandrasekaran2012,
archivePrefix = {arXiv},
arxivId = {1012.0621},
author = {Chandrasekaran, Venkat and Recht, Benjamin and Parrilo, Pablo A. and Willsky, Alan S.},
doi = {10.1007/s10208-012-9135-7},
eprint = {1012.0621},
file = {:home/tom/Documents/Mendeley/Chandrasekaran et al._2012_The Convex Geometry of Linear Inverse Problems.pdf:pdf},
isbn = {978-1-4244-8215-3},
issn = {16153375},
journal = {Foundations of Computational Mathematics},
keywords = {Atomic norms,Convex optimization,Gaussian width,Real algebraic geometry,Semidefinite programming,Symmetry},
number = {6},
pages = {805--849},
title = {{The Convex Geometry of Linear Inverse Problems}},
volume = {12},
year = {2012}
}
@inproceedings{Knopp2010,
abstract = {Most methods for the recognition of shape classes from 3D datasets focus on classifying clean, often manually generated models. However, 3D shapes obtained through acquisition techniques such as Structure-from-Motion or LIDAR scanning are noisy, clutter and holes. In that case global shape features-still dominating the 3D shape class recognition literature-are less appropriate. Inspired by 2D methods, recently researchers have started to work with local features. In keeping with this strand, we propose a new robust 3D shape classification method. It contains two main contributions. First, we extend a robust 2D feature descriptor, SURF, to be used in the context of 3D shapes. Second, we show how 3D shape class recognition can be improved by probabilistic Hough transform based methods, already popular in 2D. Through our experiments on partial shape retrieval, we show the power of the proposed 3D features. Their combination with the Hough transform yields superior results for class recognition on standard datasets. The potential for the applicability of such a method in classifying 3D obtained from Structure-from-Motion methods is promising, as we show in some initial experiments.},
address = {Heraklion, Crete, Greece},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Knopp, Jan and Prasad, Mukta and Willems, Geert and Timofte, Radu and {Van Gool}, Luc},
booktitle = {European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-642-15567-3_43},
eprint = {9780201398298},
file = {:home/tom/Documents/Mendeley/Knopp et al._2010_Hough transform and 3D SURF for robust three dimensional classification.pdf:pdf},
isbn = {3642155669},
issn = {03029743},
pages = {589--602},
pmid = {4520227},
title = {{Hough transform and 3D SURF for robust three dimensional classification}},
year = {2010}
}
@inproceedings{yosinski2014transferable,
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
booktitle = {Advances in neural information processing systems (NIPS)},
file = {:home/tom/Documents/Mendeley//Yosinski et al._2014_How transferable are features in deep neural networks.pdf:pdf},
pages = {3320--3328},
title = {{How transferable are features in deep neural networks?}},
year = {2014}
}
@article{Berthet2013a,
archivePrefix = {arXiv},
arxivId = {arXiv:1202.5070v3},
author = {Berthet, Quentin and Rigollet, Philippe},
doi = {10.1214/13-AOS1127},
eprint = {arXiv:1202.5070v3},
file = {:home/tom/Documents/Mendeley/Berthet, Rigollet_2013_Optimal detection of sparse principal components in high dimension.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {High-dimensional detection,Minimax lower bounds,Planted clique,Semidefinite relaxation,Sparse principal component analysis,Spiked covariance model},
number = {4},
pages = {1780--1815},
title = {{Optimal detection of sparse principal components in high dimension}},
volume = {41},
year = {2013}
}
@article{He2011,
author = {He, Bingsheng and Tao, Min and Xu, Minghua and Yuan, Xiaoming},
doi = {10.1080/02331934.2011.611885},
file = {:home/tom/Documents/Mendeley/He et al._2011_An alternating direction-based contraction method for linearly constrained separable convex programming problems.pdf:pdf},
issn = {0233-1934},
journal = {Optimization},
keywords = {90c06,90c22,90c25,alternating direction method,ams subject classifications,constraint,contraction method,convex programming,linear,separable structure},
number = {January},
pages = {1--24},
title = {{An alternating direction-based contraction method for linearly constrained separable convex programming problems}},
year = {2011}
}
@misc{Hinton2012,
author = {Hinton, Geoffrey E and Srivastava, Nitish and Swersky, Kevin},
booktitle = {COURSERA: Neural Networks for Machine Learning},
file = {:home/tom/Documents/Mendeley/Hinton, Srivastava, Swersky_2012_Lecture 6a- overview of mini-batch gradient descent.pdf:pdf},
howpublished = {Slide for online class COURSERA: Neural Networks for Machine Learning},
pages = {31},
title = {{Lecture 6a- overview of mini-batch gradient descent}},
url = {http://www.cs.toronto.edu/$\sim$tijmen/csc321/slides/lecture_slides_lec6.pdf},
year = {2012}
}
@article{Papyan2017a,
archivePrefix = {arXiv},
arxivId = {1607.02009},
author = {Papyan, Vardan and Sulam, Jeremias and Elad, Michael},
doi = {10.1109/TSP.2017.2733447},
eprint = {1607.02009},
file = {:home/tom/Documents/Mendeley/Papyan, Sulam, Elad_2017_Working Locally Thinking Globally Theoretical Guarantees for Convolutional Sparse Coding.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Basis Pursuit,Computational modeling,Convolution,Convolutional Sparse Coding,Convolutional codes,Dictionaries,Global modeling,Local Processing,Matching pursuit algorithms,Mathematical model,Orthogonal Matching Pursuit,Sparse Representations,Sparse matrices,Stability Guarantees,Uniqueness Guarantees},
number = {21},
pages = {5687--5701},
title = {{Working Locally Thinking Globally: Theoretical Guarantees for Convolutional Sparse Coding}},
volume = {65},
year = {2017}
}
@article{Schaworonkow2018,
author = {Schaworonkow, Natalie and Nikulin, Vadim V},
doi = {10.1101/401091},
file = {:home/tom/Documents/Mendeley/Schaworonkow, Nikulin_2018_Spatial neuronal synchronization and the waveform of oscillations implications for EEG and MEG.pdf:pdf},
journal = {bioRxiv},
title = {{Spatial neuronal synchronization and the waveform of oscillations : implications for EEG and MEG}},
year = {2018}
}
@article{Mallat2012,
abstract = {Pattern classification often requires using translation invariant representations, which are stable and hence Lipschitz continuous to deformations. A Fourier transform does not provide such Lipschitz stability. Scattering operators are obtained by iterating on wavelet transforms and modulus operators. The resulting representation is proved to be translation invariant and Lipschitz continuous to deformations, up to a log term. It is computed with a non-linear convolution network, which scatters functions along an infinite set of paths. Invariance to the action of any compact Lie subgroup of the general linear group is obtained with a combined scattering, which iterates over wavelet transforms defined on this group. Scattering representations yield new metrics on stationary processes, which are stable to random deformations.},
archivePrefix = {arXiv},
arxivId = {1101.2286},
author = {Mallat, St\'ephane},
doi = {10.1002/cpa.21413},
eprint = {1101.2286},
file = {:home/tom/Documents/Mendeley/Mallat_2012_Group Invariant Scattering.pdf:pdf},
issn = {00103640},
journal = {Communications on Pure and Applied Mathematics},
number = {10},
pages = {1331--1398},
title = {{Group Invariant Scattering}},
volume = {65},
year = {2012}
}
@article{tudor2004many,
author = {Tudor-Locke, Catrine and {Bassett Jr}, David R},
journal = {Sports Medicine},
number = {1},
pages = {1--8},
publisher = {Springer},
title = {{How many steps/day are enough?}},
volume = {34},
year = {2004}
}
@article{Brady1989,
author = {Brady, Martin L. and Raghavan, Raghu and Slawny, Joseph},
doi = {10.1109/31.31314},
file = {:home/tom/Documents/Mendeley/Brady, Raghavan, Slawny_1989_Back Propagation Fails to Separate Where Perceptrons Succeed.pdf:pdf},
issn = {00984094},
journal = {IEEE Transactions on Circuits and Systems},
number = {5},
pages = {665--674},
title = {{Back Propagation Fails to Separate Where Perceptrons Succeed}},
volume = {36},
year = {1989}
}
@inproceedings{Keskar2017,
abstract = {This book is aimed to provide an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria: 1) expertise or knowledge of the authors; 2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and 3) the application areas that have the potential to be impacted significantly by deep learning and that have gained concentrated research efforts, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning. In Chapter 1, we provide the background of deep learning, as intrinsically connected to the use of multiple layers of nonlinear transformations to derive features from the sensory signals such as speech and visual images. In the most recent literature, deep learning is embodied also as representation learning, which involves a hierarchy of features or concepts where higher-level representations of them are defined from lower-level ones and where the same lower-level representations help to define higher-level ones. In Chapter 2, a brief historical account of deep learning is presented. In particular, selected chronological development of speech recognition is used to illustrate the recent impact of deep learning that has become a dominant technology in speech recognition industry within only a few years since the start of a collaboration between academic and industrial researchers in applying deep learning to speech recognition. In Chapter 3, a three-way classification scheme for a large body of work in deep learning is developed. We classify a growing number of deep learning techniques into unsupervised, supervised, and hybrid categories, and present qualitative descriptions and a literature survey for each category. From Chapter 4 to Chapter 6, we discuss in detail three popular deep networks and related learning methods, one in each category. Chapter 4 is devoted to deep autoencoders as a prominent example of the unsupervised deep learning techniques. Chapter 5 gives a major example in the hybrid deep network category, which is the discriminative feed-forward neural network for supervised learning with many layers initialized using layer-by-layer generative, unsupervised pre-training. In Chapter 6, deep stacking networks and several of the variants are discussed in detail, which exemplify the discriminative or supervised deep learning techniques in the three-way categorization scheme. In Chapters 7-11, we select a set of typical and successful applications of deep learning in diverse areas of signal and information processing and of applied artificial intelligence. In Chapter 7, we review the applications of deep learning to speech and audio processing, with emphasis on speech recognition organized according to several prominent themes. In Chapters 8, we present recent results of applying deep learning to language modeling and natural language processing. Chapter 9 is devoted to selected applications of deep learning to information retrieval including Web search. In Chapter 10, we cover selected applications of deep learning to image object recognition in computer vision. Selected applications of deep learning to multi-modal processing and multi-task learning are reviewed in Chapter 11. Finally, an epilogue is given in Chapter 12 to summarize what we presented in earlier chapters and to discuss future challenges and directions.},
address = {Toulon, France},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.7522v4},
author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
booktitle = {International Conference on Learning Representation (ICLR)},
doi = {10.1227/01.NEU.0000255452.20602.C9},
eprint = {arXiv:1412.7522v4},
file = {:home/tom/Documents/Mendeley/Keskar et al._2017_On Large-Batch Training for Deep Learning Generalization Gap and Sharp Minima.pdf:pdf},
isbn = {9781405161251},
issn = {1607-551X},
pmid = {17460516},
title = {{On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}},
year = {2017}
}
@inproceedings{Lowe1999,
abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
address = {Corfu, Greece},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, D.G.},
booktitle = {International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.1999.790410},
eprint = {0112017},
file = {:home/tom/Documents/Mendeley/Lowe_1999_Object recognition from local scale-invariant features.pdf:pdf},
isbn = {0-7695-0164-8},
issn = {0-7695-0164-8},
pages = {1150--1157},
pmid = {15806121},
primaryClass = {cs},
publisher = {IEEE},
title = {{Object recognition from local scale-invariant features}},
url = {http://ieeexplore.ieee.org/document/790410/},
year = {1999}
}
@inproceedings{Megalooikonomou2005,
abstract = {Efficiently and accurately searching for similarities among time series and discovering interesting patterns is an important and non-trivial problem. In this paper, we introduce a new representation of time series, the Multiresolution Vector Quantized (MVQ) approximation, along with a new distance function. The novelty of MVQ is that it keeps both local and global information about the original time series in a hierarchical mechanism, processing the original time series at multiple resolutions. Moreover, the proposed representation is symbolic employing key subsequences and potentially allows the application of text-based retrieval techniques into the similarity analysis of time series. The proposed method is fast and scales linearly with the size of database and the dimensionality. Contrary to the vast majority in the literature that uses the Euclidean distance, MVQ uses a multi-resolution/hierarchical distance function. We performed experiments with real and synthetic data. The proposed distance function consistently outperforms all the major competitors (Euclidean, Dynamic Time Warping, Piecewise Aggregate Approximation) achieving up to 20% better precision/recall and clustering accuracy on the tested datasets.},
author = {Megalooikonomou, V. and Faloutsos, C.},
booktitle = {International Conference on Data Engineering (ICDE)},
doi = {10.1109/ICDE.2005.10},
file = {:home/tom/Documents/Mendeley/Megalooikonomou, Faloutsos_2005_A Multiresolution Symbolic Representation of Time Series.pdf:pdf},
isbn = {0-7695-2285-8},
issn = {10844627},
pages = {668--679},
title = {{A Multiresolution Symbolic Representation of Time Series}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1410183},
year = {2005}
}
@inproceedings{Oudre2011,
address = {Barcelona, Spain},
author = {Oudre, Laurent and Lung-Yut-Fong, Alexandre and Bianchi, Pascal},
booktitle = {European Signal Processing Conference},
file = {:home/tom/Documents/Mendeley/Oudre, Lung-Yut-Fong, Bianchi_2011_Segmentation of accelerometer signals recorded during continuous treadmill walking.pdf:pdf},
isbn = {2076-1465 VO -},
issn = {22195491},
pages = {1564--1568},
title = {{Segmentation of accelerometer signals recorded during continuous treadmill walking}},
year = {2011}
}
@article{McCulloch1943,
abstract = {Because of the 'all-or-none' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behaviour of every net can be described in these terms, with the addition of more complicated logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which nehaves under the other and gives the same results, algthough perhaps not in the same time. Various applications of the calculus are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {McCulloch, W S and Pitts, W},
doi = {10.1007/BF02478259},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/Mendeley/McCulloch, Pitts_1943_A Logical Calculus of the Idea Immanent in Nervous Activity.pdf:pdf},
isbn = {0007-4985},
issn = {0007-4985},
journal = {The Bulletin of Mathematical Biophysics},
keywords = {McCulloch and Pitts,neuron},
number = {4},
pages = {115--133},
pmid = {2185863},
title = {{A Logical Calculus of the Idea Immanent in Nervous Activity}},
url = {http://www.cse.chalmers.se/$\sim$coquand/AUTOMATA/mcp.pdf},
volume = {5},
year = {1943}
}
@article{Wohlberg2016,
author = {Wohlberg, Brendt},
file = {:home/tom/Documents/Mendeley/Wohlberg_2016_Efficient Algorithms for Convolutional Sparse Representations.pdf:pdf},
journal = {IEEE Transactions on Image Processing},
keywords = {ADMM,Index Terms— Sparse representation,convolutional sparse representation,dictionary learning,sparse coding},
number = {1},
title = {{Efficient Algorithms for Convolutional Sparse Representations}},
volume = {25},
year = {2016}
}
@article{Liu2017,
archivePrefix = {arXiv},
arxivId = {1706.09563},
author = {Liu, Jialin and Garcia-Cardona, Cristina and Wohlberg, Brendt and Yin, Wotao},
eprint = {1706.09563},
file = {:home/tom/Documents/Mendeley/Liu et al._2017_Online Convolutional Dictionary Learning.pdf:pdf},
journal = {preprint ArXiv},
title = {{Online Convolutional Dictionary Learning}},
url = {http://arxiv.org/abs/1706.09563},
volume = {1706.09563},
year = {2017}
}
@article{Bristow2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2407v1},
author = {Bristow, Hilton and Lucey, Simon},
eprint = {arXiv:1406.2407v1},
file = {:home/tom/Documents/Mendeley/Bristow, Lucey_2014_Optimization Methods for Convolutional Sparse Coding.pdf:pdf},
journal = {preprint ArXiv},
keywords = {admm,convolution,fista,fourier,l 1,learning,machine,sisc,sparse coding},
title = {{Optimization Methods for Convolutional Sparse Coding}},
volume = {1406.2407},
year = {2014}
}
@article{Hornik1989,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/Mendeley/Hornik, Stinchcombe, White_1989_Multilayer feedforward networks are universal approximators.pdf:pdf},
isbn = {08936080 (ISSN)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
number = {5},
pages = {359--366},
pmid = {74},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}
@article{Balakrishnan2014,
archivePrefix = {arXiv},
arxivId = {1408.2156},
author = {Balakrishnan, Sivaraman and Wainwright, Martin J. and Yu, Bin},
eprint = {1408.2156},
file = {:home/tom/Documents/Mendeley/Balakrishnan, Wainwright, Yu_2014_Statistical guarantees for the EM algorithm From population to sample-based analysis.pdf:pdf},
journal = {preprint ArXiv},
title = {{Statistical guarantees for the EM algorithm: From population to sample-based analysis}},
url = {http://arxiv.org/abs/1408.2156},
volume = {1408.2156},
year = {2014}
}
@article{Friedman2007,
archivePrefix = {arXiv},
arxivId = {0708.1485},
author = {Friedman, Jerome and Hastie, Trevor and H{\"{o}}fling, Holger and Tibshirani, Robert},
doi = {10.1214/07-AOAS131},
eprint = {0708.1485},
file = {:home/tom/Documents/Mendeley//Friedman et al._2007_Pathwise coordinate optimization.pdf:pdf},
journal = {The Annals of Applied Statistics},
number = {2},
pages = {302--332},
title = {{Pathwise coordinate optimization}},
volume = {1},
year = {2007}
}
@article{Zhang2012,
abstract = {Given a sample covariance matrix, we examine the problem of maxi- mizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse principal component analysis and has a wide array of applications in ma- chine learning and engineering. Unfortunately, this problem is also combinatorially hard and we discuss convex relaxation techniques that efficiently produce good ap- proximate solutions. We then describe several algorithms solving these relaxations as well as greedy algorithms that iteratively improve the solution quality. Finally, we illustrate sparse PCA in several applications, ranging from senate voting and finance to news data.},
archivePrefix = {arXiv},
arxivId = {1011.3781},
author = {Zhang, Youwei and D'Aspremont, Alexandre and {El Ghaoui}, Laurent},
doi = {10.1007/978-1-4614-0769-0_31},
eprint = {1011.3781},
isbn = {978-1-4614-0769-0},
issn = {08848289},
journal = {International Series in Operations Research and Management Science},
pages = {915--940},
title = {{Sparse PCA: Convex relaxations, algorithms and applications}},
volume = {166},
year = {2012}
}
@inproceedings{shin2007adaptive,
author = {Shin, S H and Park, C G and Kim, J W and Hong, H S and Lee, J M},
booktitle = {Sensors Applications Symposium, 2007. SAS'07. IEEE},
organization = {IEEE},
pages = {1--5},
title = {{Adaptive step length estimation algorithm using low-cost MEMS inertial sensors}},
year = {2007}
}
@article{hinton2012deep,
author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and Others},
journal = {IEEE Signal Processing Magazine},
number = {6},
pages = {82--97},
publisher = {IEEE},
title = {{Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups}},
volume = {29},
year = {2012}
}
@inproceedings{Sokolic2017,
abstract = {This paper studies the generalization error of invariant classifiers. In particular, we consider the common scenario where the classification task is invariant to certain transformations of the input, and that the classifier is constructed (or learned) to be invariant to these transformations. Our approach relies on factoring the input space into a product of a base space and a set of transformations. We show that whereas the generalization error of a non-invariant classifier is proportional to the complexity of the input space, the generalization error of an invariant classifier is proportional to the complexity of the base space. We also derive a set of sufficient conditions on the geometry of the base space and the set of transformations that ensure that the complexity of the base space is much smaller than the complexity of the input space. Our analysis applies to general classifiers such as convolutional neural networks. We demonstrate the implications of the developed theory for such classifiers with experiments on the MNIST and CIFAR-10 datasets.},
archivePrefix = {arXiv},
arxivId = {1610.04574},
author = {Sokolic, Jure and Giryes, Raja and Sapiro, Guillermo and Rodrigues, Miguel R. D.},
booktitle = {Artificial Intelligence and Statistics (AISTAT)},
eprint = {1610.04574},
file = {:home/tom/Documents/Mendeley/Sokolic et al._2017_Generalization Error of Invariant Classifiers.pdf:pdf},
pages = {1094--1103},
title = {{Generalization Error of Invariant Classifiers}},
url = {http://arxiv.org/abs/1610.04574},
year = {2017}
}
@book{Oculo21,
author = {Leigh, R. John and Zee, David S.},
publisher = {Oxford University Press, USA},
title = {{The neurology of eye movements}},
year = {2015}
}
@article{brunelli1993face,
author = {Brunelli, R and Poggio, T},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {10},
pages = {1042--1052},
publisher = {IEEE Computer Society},
title = {{Face recognition: Features versus templates}},
volume = {15},
year = {1993}
}
@article{Yuan2006,
author = {Yuan, Ming and Lin, Yi},
file = {:home/tom/Documents/Mendeley/Yuan, Lin_2006_Model selection and estimation in regression with.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {analysis of variance,lasso,least angle regression,non-negative garrotte,piecewise linear solution path},
number = {1},
pages = {49--67},
title = {{Model selection and estimation in regression with}},
volume = {68},
year = {2006}
}
@article{bubeck2014theory,
author = {Bubeck, S{\'{e}}bastien},
journal = {preprint ArXiv},
title = {{Theory of convex optimization for machine learning}},
volume = {1405.4980},
year = {2014}
}
@inproceedings{Gori1991,
address = {Seattle, WA, USA},
author = {Gori, M and Tesi, A},
booktitle = {International Joint Conference on Neural Networks (IJCNN)},
organization = {IEEE},
pages = {896},
title = {{Backpropagation converges for multi-layered networks and linearly-separable patterns}},
volume = {2},
year = {1991}
}
@inproceedings{scholkopf2001generalized,
author = {Sch{\"{o}}lkopf, Bernhard and Herbrich, Ralf and Smola, Alex J},
booktitle = {International Conference on Computational Learning Theory (COLT)},
organization = {Springer},
pages = {416--426},
title = {{A generalized representer theorem}},
year = {2001}
}
@article{DAspremont2014,
archivePrefix = {arXiv},
arxivId = {1205.0121},
author = {D'Aspremont, Alexandre and Bach, Francis and Ghaoui, Laurent El},
doi = {10.1007/s10107-014-0751-7},
eprint = {1205.0121},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {62H25,90C22,90C27},
number = {1-2},
pages = {89--110},
title = {{Approximation bounds for sparse principal component analysis}},
volume = {148},
year = {2014}
}
@article{Tibshirani1996,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tibshirani, Robert},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/Mendeley//Tibshirani_1996_Regression Shrinkage and Selection via the Lasso.pdf:pdf},
journal = {Journal of the royal statistical society. Series B (methodological)},
number = {1},
pages = {267--288},
pmid = {25246403},
title = {{Regression Shrinkage and Selection via the Lasso}},
volume = {58},
year = {1996}
}
@article{Nesterov1983,
author = {Nesterov, Yuri},
journal = {Soviet Mathematics Doklady},
number = {2},
pages = {372--376},
title = {{A method of solving a convex programming problem with convergence rate O (1/k2)}},
volume = {27},
year = {1983}
}
@article{Buchstaber1994,
author = {Buchstaber, V M},
journal = {Translations of the American Mathematical Society-Series 2},
pages = {1--18},
publisher = {Providence [etc.] American Mathematical Society, 1949-},
title = {{Time series analysis and grassmannians}},
volume = {162},
year = {1994}
}
@article{williamson2000gait,
author = {Williamson, R and Andrews, B},
journal = {IEEE Transactions on Rehabilitation Engineering},
number = {3},
pages = {312--319},
publisher = {IEEE},
title = {{Gait event detection for FES using accelerometers and supervised machine learning}},
volume = {8},
year = {2000}
}
@article{marcus1993building,
author = {Marcus, Mitchell P and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
journal = {Computational linguistics},
number = {2},
pages = {313--330},
publisher = {MIT Press},
title = {{Building a large annotated corpus of English: The Penn Treebank}},
volume = {19},
year = {1993}
}
@inproceedings{Lee2001,
author = {Lee, Dd and Seung, Hs},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Documents/Mendeley//Lee, Seung_2001_Algorithms for non-negative matrix factorization.pdf:pdf},
number = {1},
pages = {556--562},
title = {{Algorithms for non-negative matrix factorization}},
year = {2001}
}
@article{Arora2013,
archivePrefix = {arXiv},
arxivId = {1308.6273},
author = {Arora, Sanjeev and Ge, R and Moitra, A},
eprint = {1308.6273},
file = {:home/tom/Documents/Mendeley/Arora, Ge, Moitra_2013_New Algorithms for Learning Incoherent and Overcomplete Dictionaries.pdf:pdf},
journal = {Journal of Machine Learning Research (JMLR)},
pages = {1--20},
title = {{New Algorithms for Learning Incoherent and Overcomplete Dictionaries}},
url = {http://arxiv.org/abs/1308.6273},
volume = {35},
year = {2013}
}
@article{He2000,
author = {He, B. S. and Yang, Hai and Wang, S. L.},
doi = {10.1023/A:1004603514434},
file = {:home/tom/Documents/Mendeley/He, Yang, Wang_2000_Alternating Direction Method with Self-Adaptive Penalty Parameters for Monotone Variational Inequalities.pdf:pdf},
issn = {0022-3239},
journal = {Journal of Optimization Theory and Applications},
keywords = {alternating direction,method,monotone variational inequalities,variable penalty parameters},
number = {2},
pages = {337--356},
title = {{Alternating Direction Method with Self-Adaptive Penalty Parameters for Monotone Variational Inequalities}},
url = {http://link.springer.com/10.1023/A:1004603514434},
volume = {106},
year = {2000}
}
@article{renaudin2012step,
author = {Renaudin, V and Susi, M and Lachapelle, G},
journal = {Sensors},
number = {7},
pages = {8507--8525},
title = {{Step length estimation using handheld inertial sensors}},
volume = {12},
year = {2012}
}
@inproceedings{Abalov2014,
address = {Cox's Bazar, Bangladesh},
author = {Abalov, N V and Gubarev, V V},
booktitle = {International Forum on Strategic Technology (IFOST)},
file = {:home/tom/Documents/Mendeley/Abalov, Gubarev_2014_Automated grouping of decomposition components of time series for singular spectrum analysis.pdf:pdf},
keywords = {considerable amount of manual,dialog-based input from,grouping,identification,singular spectrum analysis,ssa,time seires},
title = {{Automated grouping of decomposition components of time series for singular spectrum analysis}},
year = {2014}
}
@inproceedings{Chalasani2013,
address = {Dallas, TX, USA},
annote = {Proximal method with line search and momentum acceleration (FISTA)
They add a predictive sparse decomposition paprt that learn jointly the sparrse code and a fast encoder.

The algorithm is the same as the one in beck_09_Fista.
The results with a variational rate are not so good n the implementation. might have some errors.},
author = {Chalasani, Rakesh and Principe, Jose C. and Ramakrishnan, Naveen},
booktitle = {International Joint Conference on Neural Networks (IJCNN)},
file = {:home/tom/Documents/Mendeley/Chalasani, Principe, Ramakrishnan_2013_A fast proximal method for convolutional sparse coding.pdf:pdf},
issn = {2161-4393},
keywords = {Convolution,Feature Extraction,Sparse Coding,Unsupervised Learning},
pages = {1--5},
title = {{A fast proximal method for convolutional sparse coding}},
year = {2013}
}
@article{Oculo20,
author = {Lee, A.},
journal = {Journal of Pediatric Ophthalmology and Strabismus},
number = {33},
pages = {68--69},
title = {{Neuroimaging in all cases of spasmus nutans}},
volume = {Jan-Feb},
year = {1996}
}
@article{Chan2010,
author = {Chan, W. W P and Galiana, Henrietta L.},
doi = {10.1109/TBME.2009.2016112},
file = {:home/tom/Documents/Mendeley/Chan, Galiana_2010_A nonlinear model of the neural integrator improves detection of deficits in the human VOR.pdf:pdf},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Lesion detection,Modeling,Neural integrator (NI),Vestibulo-ocular reflex (VOR)},
number = {5},
pages = {1012--1023},
pmid = {19272974},
title = {{A nonlinear model of the neural integrator improves detection of deficits in the human VOR}},
volume = {57},
year = {2010}
}
@article{Osher2009,
author = {Osher, Stanley and Li, Yingying},
file = {:home/tom/Documents/Mendeley//Osher, Li_2009_Coordinate descent optimization for $ell_1$ minimization with application to compressed sensing a greedy algorithm.pdf:pdf},
journal = {Inverse Problems and Imaging},
keywords = {and phrases,basis pursuit,bregman iteration,constrained,greedy sweep,shrinkage},
number = {3},
pages = {487--503},
title = {{Coordinate descent optimization for $\ell_1$ minimization with application to compressed sensing; a greedy algorithm}},
volume = {3},
year = {2009}
}
@article{oymak2015sharp,
author = {Oymak, Samet and Recht, Benjamin and Soltanolkotabi, Mahdi},
file = {:home/tom/Documents/Mendeley//Oymak, Recht, Soltanolkotabi_2015_Sharp Time-Data Tradeoffs for Linear Inverse Problems.pdf:pdf},
journal = {preprint ArXiv},
title = {{Sharp Time--Data Tradeoffs for Linear Inverse Problems}},
volume = {1507.04793},
year = {2015}
}
@misc{Grisel2016,
author = {Moreau, Thomas and Grisel, Olivier},
howpublished = {https://github.com/tomMoral/loky},
title = {{Loky}},
url = {https://github.com/tomMoral/loky}
}
@inproceedings{Yu2015,
address = {Lille, France},
author = {Yu, Rose and Cheng, Dehua and Liu, Yan},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:home/tom/Documents/Mendeley/Yu, Cheng, Liu_2015_Accelerated Online Low-Rank Tensor Learning for Multivariate spatio-temporal Streams.pdf:pdf},
pages = {1--10},
title = {{Accelerated Online Low-Rank Tensor Learning for Multivariate spatio-temporal Streams}},
volume = {37},
year = {2015}
}
@article{Dalalyan2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1700v1},
author = {Dalalyan, Arnak S and Hebiri, Mohamed and Lederer, Johannes},
eprint = {arXiv:1402.1700v1},
file = {:home/tom/Documents/Mendeley//Dalalyan, Hebiri, Lederer_2014_On the prediction performance of the Lasso.pdf:pdf},
journal = {preprint ArXiv},
title = {{On the prediction performance of the Lasso}},
volume = {1402.1700},
year = {2014}
}
@inproceedings{le1990handwritten,
address = {Denver, United States},
author = {{Le Cun}, B Boser and Denker, J S and Henderson, D and Howard, Richard E and Hubbard, W and Jackel, Lawrence D},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
organization = {Citeseer},
pages = {396--404},
title = {{Handwritten digit recognition with a back-propagation network}},
year = {1990}
}
@techreport{Kong2013,
author = {Kong, Bailey and Fowlkes, Charless C},
doi = {10.1109/CVPR.2013.57},
file = {:home/tom/Documents/Mendeley/Kong, Fowlkes_2013_Fast Convolutional Sparse Coding.pdf:pdf},
institution = {Department of Computer Science, University of California, Irvine},
isbn = {1063-6919 VO -},
keywords = {convolution,dictionary learning,sparse coding},
pages = {1--7},
title = {{Fast Convolutional Sparse Coding}},
url = {http://hiltonbristow.com/static/papers/2013_CVPR_Bristow.pdf},
year = {2013}
}
@book{Goodfellow2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {MIT Press},
doi = {10.1016/B978-0-12-391420-0.09987-X},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/Mendeley/Goodfellow, Bengio, Courville_2016_Deep Learning.pdf:pdf},
isbn = {3540620583, 9783540620587},
issn = {1432122X},
keywords = {machine learning},
pmid = {21728107},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B978012391420009987X},
year = {2016}
}
@inproceedings{Gramfort2017,
address = {Long Beach, CA, USA},
archivePrefix = {arXiv},
arxivId = {1705.08006},
author = {Jas, Mainak and {Dupr{\'{e}} la Tour}, Tom and Şimşekli, Umut and Gramfort, Alexandre},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1705.08006},
file = {:home/tom/Documents/Mendeley/Jas et al._2017_Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding.pdf:pdf},
pages = {1--15},
title = {{Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding}},
url = {http://arxiv.org/abs/1705.08006},
year = {2017}
}
@article{Rakotomamonjy2015,
author = {Rakotomamonjy, Alain and Gasso, Gilles},
journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)},
number = {1},
pages = {142--153},
publisher = {IEEE Press},
title = {{Histogram of gradients of time-frequency representations for audio scene classification}},
volume = {23},
year = {2015}
}
@article{Fuchs2004,
author = {Fuchs, Jean Jacques},
doi = {10.1109/TIT.2004.828141},
file = {:home/tom/Documents/Mendeley/Fuchs_2004_On sparse representations in arbitrary redundant bases.pdf:pdf},
isbn = {0018-9448},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Basis pursuit,Global matched filter,Linear program,Quadratic program,Redundant dictionaries,Sparse representations},
number = {6},
pages = {1341--1344},
title = {{On sparse representations in arbitrary redundant bases}},
volume = {50},
year = {2004}
}
@book{Bandeira2015,
author = {Bandeira, Afonso S},
file = {:home/tom/Documents/Mendeley/Bandeira_2015_Ten Lectures and Forty-Two Open Problems in the Mathematics of Data Science.pdf:pdf},
pages = {1--151},
title = {{Ten Lectures and Forty-Two Open Problems in the Mathematics of Data Science}},
year = {2015}
}
@inproceedings{Shalev2009,
abstract = {We describe and analyze two stochastic methods for ℓ1 regularized loss minimization problems, such as the Lasso. The ﬁrst method updates the weight of a single feature at each iteration while the second method updates the entire weight vector but only uses a single training example at each iteration. In both methods, the choice of feature or example is uniformly at random. Our theoretical runtime analysis suggests that the stochastic methods should outperform state-of-the-art deterministic approaches, including their deterministic counterparts, when the size of the problem is large. We demonstrate the advantage of stochastic methods by experimenting with synthetic and natural data sets.},
address = {Montreal, Canada},
author = {Shalev-Shwartz, Shai and Tewari, A},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:home/tom/Documents/Mendeley/Shalev-Shwartz, Tewari_2009_Stochastic Methods for $ell_1$-regularized Loss Minimization.pdf:pdf},
keywords = {coordinate descent,l1 regularization,mirror descent,optimization,sparsity},
pages = {929--936},
title = {{Stochastic Methods for $\ell_1$-regularized Loss Minimization}},
year = {2009}
}
@article{Scheinberg2013,
archivePrefix = {arXiv},
arxivId = {1311.6547},
author = {Scheinberg, Katya and Tang, Xiaocheng},
eprint = {1311.6547},
journal = {preprint ArXiv},
title = {{Practical Inexact Proximal Quasi-Newton Method with Global Complexity Analysis}},
volume = {1311.6547},
year = {2013}
}
@article{Oculo29,
author = {Sharpe, JA and Hoyt, WF and Rosenberg, MA},
journal = {Archive of Neurology},
number = {32},
pages = {191--4},
title = {{Convergence-evoked nystagmus. Congenital and acquired forms}},
volume = {Mar},
year = {1975}
}
@article{Oculo10,
author = {Good, Wv and Jan, JE and Hoyt, CS and Billson, FA and Schoettker, PJ and Klaeger, K},
journal = {Developmental Medicine and Child Neurology},
number = {39},
pages = {421--424},
title = {{Monocular vision loss can cause bilateral nystagmus in young children}},
volume = {Jun},
year = {1997}
}
@inproceedings{Heide2015,
address = {Boston, MA, USA},
author = {Heide, Felix and Heidrich, Wolfgang and Wetzstein, Gordon},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7299149},
file = {:home/tom/Documents/Mendeley/Heide, Heidrich, Wetzstein_2015_Fast and flexible convolutional sparse coding.pdf:pdf},
isbn = {978-1-4673-6964-0},
pages = {5135--5143},
title = {{Fast and flexible convolutional sparse coding}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7299149},
year = {2015}
}
@article{Vapnik1971,
author = {Vapnik, V. N. and Chervonenkis, A. Ya},
file = {:home/tom/Documents/Mendeley/Vapnik, Chervonenkis_1971_On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities.pdf:pdf},
journal = {Theory of Probability and its Applications},
number = {2},
pages = {264--280},
title = {{On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities}},
volume = {XVI},
year = {1971}
}
@inproceedings{fortunestep,
address = {Gainesville, FL, USA},
author = {Fortune, E and Lugade, V and Morrow, M and Kaufman, K},
booktitle = {American Society of Biomechanics Annual Meeting (ASB)},
title = {{Step counts using a tri-axial accelerometer during activity}},
year = {2012}
}
@article{Saigo2004,
abstract = {MOTIVATION: Remote homology detection between protein sequences is a central problem in computational biology. Discriminative methods involving support vector machines (SVMs) are currently the most effective methods for the problem of superfamily recognition in the Structural Classification Of Proteins (SCOP) database. The performance of SVMs depends critically on the kernel function used to quantify the similarity between sequences. RESULTS: We propose new kernels for strings adapted to biological sequences, which we call local alignment kernels. These kernels measure the similarity between two sequences by summing up scores obtained from local alignments with gaps of the sequences. When tested in combination with SVM on their ability to recognize SCOP superfamilies on a benchmark dataset, the new kernels outperform state-of-the-art methods for remote homology detection. AVAILABILITY: Software and data available upon request.},
author = {Saigo, Hiroto and Vert, Jean Philippe and Ueda, Nobuhisa and Akutsu, Tatsuya},
doi = {10.1093/bioinformatics/bth141},
file = {:home/tom/Documents/Mendeley/Saigo et al._2004_Protein homology detection using string alignment kernels.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
number = {11},
pages = {1682--1689},
pmid = {14988126},
title = {{Protein homology detection using string alignment kernels}},
volume = {20},
year = {2004}
}
@article{Luo2016,
archivePrefix = {arXiv},
arxivId = {1602.00223},
author = {Luo, Luo and Chen, Zihao and Zhang, Zhihua and Li, Wu-Jun},
eprint = {1602.00223},
file = {:home/tom/Documents/Mendeley//Luo et al._2016_Variance-Reduced Second-Order Methods.pdf:pdf},
journal = {preprint ArXiv},
keywords = {ICML,boring formatting information,machine learning},
title = {{Variance-Reduced Second-Order Methods}},
url = {http://arxiv.org/abs/1602.00223},
volume = {1602.00223},
year = {2016}
}
@article{Ning2014,
abstract = {This paper jointly addresses the problems of chromatogram baseline correction and noise reduction. The proposed approach is based on modeling the series of chromatogram peaks as sparse with sparse derivatives, and on modeling the baseline as a low-pass signal. A convex optimization problem is formulated so as to encapsulate these non-parametric models. To account for the positivity of chromatogram peaks, an asymmetric penalty function is utilized. A robust, computationally efficient, iterative algorithm is developed that is guaranteed to converge to the unique optimal solution. The approach, termed Baseline Estimation and Denoising With Sparsity (BEADS), is evaluated and compared with two state-of-the-art methods using both simulated and real chromatogram data.},
author = {Ning, Xiaoran and Selesnick, Ivan W. and Duval, Laurent},
doi = {10.1016/j.chemolab.2014.09.014},
file = {:home/tom/Documents/Mendeley/Ning, Selesnick, Duval_2014_Chromatogram baseline estimation and denoising using sparsity (BEADS).pdf:pdf},
issn = {18733239},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {Asymmetric penalty,Baseline correction,Baseline drift,Convex optimization,Low-pass filtering,Sparse derivative},
pages = {156--167},
publisher = {Elsevier B.V.},
title = {{Chromatogram baseline estimation and denoising using sparsity (BEADS)}},
url = {http://dx.doi.org/10.1016/j.chemolab.2014.09.014},
volume = {139},
year = {2014}
}
@phdthesis{Gillis2011,
address = {Louvain-La-Neuve, Belgium},
author = {Gillis, Nicolas},
file = {:home/tom/Documents/Mendeley/Gillis_2011_Nonnegative matrix factorization Complexity, algorithms and applications.pdf:pdf},
school = {Universit\'e catholique de Louvain},
title = {{Nonnegative matrix factorization: Complexity, algorithms and applications}},
year = {2011}
}
@inproceedings{Vu2013,
address = {South Lake Tahoe, United States},
author = {Vu, Vincent Q and Lei, Jing and Cho, Juhee and Rohe, Karl},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Documents/Mendeley/Vu et al._2013_Fantope Projection and Selection A near-optimal convex relaxation of sparse PCA.pdf:pdf},
pages = {2670--2678},
title = {{Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA}},
year = {2013}
}
@article{Oculo12,
author = {Gottlob, I. and Wizov, S. and Reinecke, R.},
journal = {Investigative Ophthalmology and Visual Science},
number = {36},
pages = {2768--2771},
title = {{Spasmus nutans. A long-term follow-up}},
volume = {Dec},
year = {1995}
}
@article{Oculo18,
author = {Kushnner, BJ},
journal = {Archive Ophthalmology},
number = {113},
pages = {1298--1300},
title = {{Infantile uniocular blindness with bilateral nystagmus. A syndrome}},
volume = {Oct},
year = {1995}
}
@article{Bubeck2015,
archivePrefix = {arXiv},
arxivId = {1405.4980},
author = {Bubeck, S{\'{e}}bastien},
doi = {10.1561/2200000050},
eprint = {1405.4980},
file = {:home/tom/Documents/Mendeley//Bubeck_2015_Convex Optimization Algorithms and Complexity.pdf:pdf},
isbn = {2200000049},
issn = {1935-8237},
journal = {Foundations and Trends in Machine Learning},
number = {3-4},
pages = {231--357},
title = {{Convex Optimization: Algorithms and Complexity}},
url = {http://www.nowpublishers.com/article/Details/MAL-050},
volume = {8},
year = {2015}
}
@article{Oculo22,
author = {McIlwaine, GG and Carrim, ZI and Lueck, CJ and Chrisp, TM},
journal = {Journal of Neuro-Ophthalmology},
number = {25},
pages = {40--43},
title = {{A mechanical theory to account for bitemporal hemianopia from chiasmal compression}},
volume = {Mar},
year = {2005}
}
@article{Bubeck2015,
archivePrefix = {arXiv},
arxivId = {1405.4980},
author = {Bubeck, S{\'{e}}bastien},
eprint = {1405.4980},
file = {:home/tom/Documents/Mendeley//Bubeck_2015_Convex Optimization Algorithms and Complexity.pdf:pdf},
journal = {Foundations and Trends{{\textregistered}} in Machine Learning},
number = {3-4},
pages = {231--357},
title = {{Convex Optimization: Algorithms and Complexity}},
volume = {8},
year = {2015}
}
@book{buzsaki2006rhythms,
author = {Buzsaki, G},
publisher = {Oxford University Press},
title = {{Rhythms of the Brain}},
year = {2006}
}
@article{Hong2012,
archivePrefix = {arXiv},
arxivId = {1208.3922},
author = {Hong, Mingyi and Luo, Zhi-Quan},
eprint = {1208.3922},
file = {:home/tom/Documents/Mendeley//Hong, Luo_2012_On the Linear Convergence of the Alternating Direction Method of Multipliers.pdf:pdf},
journal = {preprint ArXiv},
keywords = {49,90,alternating directions of multipliers,ams,by the national science,computer engineering,department of electrical and,dual ascent,error bound,foundation,grant number dms-1015346,linear convergence,minneapolis,mn 55455,mos,subject classifications,the research is supported,university of minnesota,usa},
title = {{On the Linear Convergence of the Alternating Direction Method of Multipliers}},
url = {http://arxiv.org/abs/1208.3922},
volume = {1208.3922},
year = {2012}
}
@article{Hesterberg2008,
archivePrefix = {arXiv},
arxivId = {0802.0964},
author = {Hesterberg, Tim and Choi, Nam Hee and Meier, Lukas and Fraley, Chris},
eprint = {0802.0964},
file = {:home/tom/Documents/Mendeley//Hesterberg et al._2008_Least angle and $ell_1$ penalized regression A review.pdf:pdf},
journal = {Statistics Surveys},
pages = {61--93},
title = {{Least angle and ℓ1 penalized regression: A review}},
volume = {2},
year = {2008}
}
@article{Ferradans2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1307.5551v1},
author = {Ferradans, Sira and Papadakis, Nicolas and Rabin, Julien and Peyr{\'{e}}, Gabriel and Aujol, Jean Fran{\c{c}}ois},
doi = {10.1007/978-3-642-38267-3_36},
eprint = {arXiv:1307.5551v1},
file = {:home/tom/Documents/Mendeley/Ferradans et al._2011_Regularized discrete optimal transport.pdf:pdf},
isbn = {9783642382666},
issn = {03029743},
journal = {Informatica},
keywords = {Deconvolution,Optimal Transport,color transfer,convex optimization,manifold learning,proximal splitting,transport,variational regularization},
mendeley-tags = {Deconvolution,transport},
number = {1},
pages = {428--439},
title = {{Regularized discrete optimal transport}},
volume = {35},
year = {2011}
}
@article{Wohlberg2017,
archivePrefix = {arXiv},
arxivId = {1705.04407},
author = {Wohlberg, Brendt},
eprint = {1705.04407},
file = {:home/tom/Documents/Mendeley/Wohlberg_2017_Convolutional Sparse Representations with Gradient Penalties.pdf:pdf},
journal = {preprint ArXiv},
title = {{Convolutional Sparse Representations with Gradient Penalties}},
url = {http://arxiv.org/abs/1705.04407},
volume = {1705.04407},
year = {2017}
}
@article{Liang2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1711.01530v1},
author = {Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
eprint = {arXiv:1711.01530v1},
file = {:home/tom/Documents/Mendeley/Liang et al._2017_Fisher-Rao Metric, Geometry, and Complexity of Neural Networks.pdf:pdf},
journal = {preprint ArXiv},
keywords = {and phrases,capacity control,deep learning,fisher-rao metric,generalization error,gradient,infor-,invariance,mation geometry,natural,relu activation,statistical learning theory},
title = {{Fisher-Rao Metric, Geometry, and Complexity of Neural Networks}},
volume = {1711.01530},
year = {2017}
}
@article{Edelman1998,
archivePrefix = {arXiv},
arxivId = {physics/9806030},
author = {Edelman, Alan and Arias, Tom{\'{a}}s A. and Smith, Steven T.},
doi = {10.1137/S0895479895290954},
eprint = {9806030},
file = {:home/tom/Documents/Mendeley//Edelman, Arias, Smith_1998_The Geometry of Algorithms with Orthogonality Constraints.pdf:pdf},
isbn = {08954798},
issn = {0895-4798},
journal = {SIAM Journal on Matrix Analysis and Applications},
keywords = {conjugate gradient,eigenvalue optimization,eigenvalues and eigenvectors,electronic,grassmann man-,ifold,invariant subspace,newton,orthogonality constraints,rayleigh quotient iteration,reduced gradient method,s method,sequential quadratic programming,stiefel manifold,structures computation,subspace tracking},
number = {2},
pages = {303--353},
primaryClass = {physics},
title = {{The Geometry of Algorithms with Orthogonality Constraints}},
url = {http://epubs.siam.org/doi/abs/10.1137/S0895479895290954},
volume = {20},
year = {1998}
}
@inproceedings{He2016,
address = {Las Vegas, NV, USA},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.03385v1},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
eprint = {arXiv:1512.03385v1},
file = {:home/tom/Documents/Mendeley/He et al._2016_Deep Residual Learning for Image Recognition.pdf:pdf},
pages = {770--778},
title = {{Deep Residual Learning for Image Recognition}},
year = {2016}
}
@article{Gramfort2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.08596v1},
author = {Gramfort, Alexandre and Cuturi, Marco},
eprint = {arXiv:1503.08596v1},
file = {:home/tom/Documents/Mendeley/Gramfort, Cuturi_2015_Fast Optimal Transport Averaging of Neuroimaging Data.pdf:pdf},
journal = {preprint ArXiv},
keywords = {Deconvolution,transport},
mendeley-tags = {Deconvolution,transport},
title = {{Fast Optimal Transport Averaging of Neuroimaging Data}},
volume = {1503.08596},
year = {2015}
}
@article{Koenig2001,
abstract = {Frequency-transformed EEG resting data has been widely used to describe normal and abnormal brain functional states as function of the spectral power in different frequency bands. This has yielded a series of clinically relevant findings. However, by transforming the EEG into the frequency domain, the initially excellent time resolution of time-domain EEG is lost. The topographic time-frequency decomposition is a novel computerized EEG analysis method that combines previously available techniques from time-domain spatial EEG analysis and time-frequency decomposition of single-channel time series. It yields a new, physiologically and statistically plausible topographic time-frequency representation of human multichannel EEG. The original EEG is accounted by the coefficients of a large set of user defined EEG like time-series, which are optimized for maximal spatial smoothness and minimal norm. These coefficients are then reduced to a small number of model scalp field configurations, which vary in intensity as a function of time and frequency. The result is thus a small number of EEG field configurations, each with a corresponding time-frequency (Wigner) plot. The method has several advantages: It does not assume that the data is composed of orthogonal elements, it does not assume stationarity, it produces topographical maps and it allows to include user-defined, specific EEG elements, such as spike and wave patterns. After a formal introduction of the method, several examples are given, which include artificial data and multichannel EEG during different physiological and pathological conditions. {\textcopyright} 2001 Academic Press.},
author = {Koenig, T. and Marti-Lopez, F. and Valdes-Sosa, P.},
doi = {10.1006/nimg.2001.0825},
file = {:home/tom/Documents/Mendeley/Koenig, Marti-Lopez, Valdes-Sosa_2001_Topographic time-frequency decomposition of the EEG.pdf:pdf},
isbn = {1053-8119},
issn = {10538119},
journal = {NeuroImage},
number = {2},
pages = {383--390},
pmid = {11467912},
title = {{Topographic time-frequency decomposition of the EEG}},
volume = {14},
year = {2001}
}
@article{Qiu2002,
abstract = {In this paper, we present a method to represent achromatic and chromatic image signals independently for content-based image indexing and retrieval for image database applications. Starting from an opponent colour representation, human colour vision theories and modern digital signal processing technologies are applied to develop a compact and computationally efficient visual appearance model for coloured image patterns. We use the model to compute the statistics of achromatic and chromatic spatial patterns of colour images for indexing and content-based retrieval. Two types of colour images databases, one colour texture database and another photography colour image database are used to evaluate the performance of the developed method in content-based image indexing and retrieval. Experimental results are presented to show that the new method is superior or competitive to state-of-the-art content-based image indexing and retrieval techniques. ?? 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
author = {Qiu, G.},
doi = {10.1016/S0031-3203(01)00162-5},
file = {:home/tom/Documents/Mendeley/Qiu_2002_Indexing chromatic and achromatic patterns for content-based colour image retrieval.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Achromatic patterns,Chromatic patterns,Colour imaging,Colour vision,Content-based image indexing and retrieval,Image database,Vector quantization},
number = {8},
pages = {1675--1686},
title = {{Indexing chromatic and achromatic patterns for content-based colour image retrieval}},
volume = {35},
year = {2002}
}
@article{Byrd1995,
author = {Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
file = {:home/tom/Documents/Mendeley/Byrd et al._1995_A Limited Memory Algorithm for Bound Constrained Optimization.pdf:pdf},
journal = {SIAM Journal on Scientific Computing},
keywords = {1,49,65,ams subject classifications,bound constrained optimization,describe a limited memory,in this paper we,introduction,large-scale optimization,limited memory method,method,nonlinear optimization,quasi-newton,quasi-newton algorithm},
number = {5},
pages = {1190--1208},
title = {{A Limited Memory Algorithm for Bound Constrained Optimization}},
volume = {16},
year = {1995}
}
@article{Wang2014,
archivePrefix = {arXiv},
arxivId = {1407.0107},
author = {Wang, Huahua and Banerjee, Arindam},
eprint = {1407.0107},
file = {:home/tom/Documents/Mendeley/Wang, Banerjee_2014_Randomized Block Coordinate Descent for Online and Stochastic Optimization.pdf:pdf},
isbn = {1407.0107},
journal = {preprint ArXiv},
title = {{Randomized Block Coordinate Descent for Online and Stochastic Optimization}},
url = {http://arxiv.org/abs/1407.0107},
volume = {1407.0107},
year = {2014}
}
@article{Papyan2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1607.08194v1},
author = {Papyan, Vardan and Romano, Yaniv and Elad, Michael},
eprint = {arXiv:1607.08194v1},
file = {:home/tom/Documents/Mendeley/Papyan, Romano, Elad_2017_Convolutional Neural Networks Analyzed via Convolutional Sparse Coding.pdf:pdf},
journal = {Journal of Machine Learning Research (JMLR)},
number = {1},
pages = {2887--2938},
title = {{Convolutional Neural Networks Analyzed via Convolutional Sparse Coding}},
volume = {18},
year = {2017}
}
@article{rabiner1989tutorial,
author = {Rabiner, Lawrence},
journal = {Proceedings of the IEEE},
number = {2},
pages = {257--286},
publisher = {IEEE},
title = {{A tutorial on hidden Markov models and selected applications in speech recognition}},
volume = {77},
year = {1989}
}
@article{Bobin2008,
author = {Bobin, J{\'{e}}r{\^{o}}me and Moudden, Y and Fadili, Jalal M. and Starck, Jean-Luc},
doi = {10.1007/s10851-008-0065-6},
file = {:home/tom/Documents/Mendeley/Bobin et al._2008_Morphological Diversity and Sparsity for Multichannel Data Restoration.pdf:pdf},
issn = {0924-9907},
journal = {Journal of Mathematical Imaging and Vision},
keywords = {multichannel data,overcomplete representations,restoration,sparsity},
number = {2},
pages = {149--168},
title = {{Morphological Diversity and Sparsity for Multichannel Data Restoration}},
url = {http://www.springerlink.com/index/10.1007/s10851-008-0065-6},
volume = {33},
year = {2008}
}
@article{Nesterov2005,
author = {Nesterov, Yuri},
doi = {10.1007/s10107-004-0552-5},
file = {:home/tom/Documents/Mendeley//Nesterov_2005_Smooth minimization of non-smooth functions.pdf:pdf},
isbn = {1010700405},
issn = {00255610},
journal = {Mathematical Programming},
keywords = {Complexity theory,Convex optimization,Non-smooth optimization,Optimal methods,Structural optimization},
number = {1},
pages = {127--152},
title = {{Smooth minimization of non-smooth functions}},
volume = {103},
year = {2005}
}
@article{Gray2006,
author = {Gray, Robert M},
file = {:home/tom/Documents/Mendeley/Gray_2006_Toeplitz and Circulant Matrices A Review.pdf:pdf},
journal = {Foundations and Trends in Communications and Information Theory},
number = {3},
pages = {155--239},
title = {{Toeplitz and Circulant Matrices: A Review}},
volume = {2},
year = {2006}
}
@inproceedings{Bruna2015,
address = {South Brisbane, Australia},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bruna, Joan and Sprechmann, Pablo and LeCun, Yan},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/Mendeley//Bruna, Sprechmann, LeCun_2015_Source separation with scattering non-negative matrix factorization.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pages = {1876----1880},
pmid = {25246403},
title = {{Source separation with scattering non-negative matrix factorization}},
year = {2015}
}
@incollection{sohn2006mobility,
author = {Sohn, T and Varshavsky, A and LaMarca, A and Chen, M and Choudhury, T and Smith, I and Consolvo, S and Hightower, J and Griswold, W and {De Lara}, E},
booktitle = {UbiComp 2006: Ubiquitous Computing},
pages = {212--224},
publisher = {Springer},
title = {{Mobility detection using everyday gsm traces}},
year = {2006}
}
@inproceedings{Lowe1999,
abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
address = {Corfu, Greece},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, D G},
booktitle = {International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.1999.790410},
eprint = {0112017},
isbn = {0-7695-0164-8},
issn = {0-7695-0164-8},
pages = {1150--1157},
pmid = {15806121},
primaryClass = {cs},
publisher = {IEEE},
title = {{Object recognition from local scale-invariant features}},
url = {http://ieeexplore.ieee.org/document/790410/},
year = {1999}
}
@inproceedings{Daneshmand2016,
archivePrefix = {arXiv},
arxivId = {1612.07335},
author = {Daneshmand, Amir and Scutari, Gesualdo and Facchinei, Francisco},
booktitle = {IEEE Conference on Signals, Systems and Computers},
eprint = {1612.07335},
file = {:home/tom/Documents/Mendeley/Daneshmand, Scutari, Facchinei_2016_Distributed Dictionary Learning.pdf:pdf},
pages = {1001--1005},
title = {{Distributed Dictionary Learning}},
year = {2016}
}
@article{Oculo11,
author = {Good, WV and Koch, TS and Jan, JE},
journal = {Developmental Medicine and Child Neurology},
number = {35},
pages = {1106--1110},
title = {{Monocular nystagmus caused by unilateral anterior visual- pathway disease}},
volume = {Dec},
year = {1993}
}
@article{Frasconi1997,
archivePrefix = {arXiv},
arxivId = {arXiv:hep-lat/0007035},
author = {Frasconi, Paolo and Gori, Marco and Tesi, Alberto},
doi = {10.1002/jssc.200600256},
eprint = {0007035},
file = {:home/tom/Documents/Mendeley/Frasconi, Gori, Tesi_1997_Successes and failures of backpropagation A theoretical investigation.pdf:pdf},
isbn = {9780203209493},
issn = {0065-230X},
journal = {Progress in Neural Networks: Architecture},
keywords = {1-butyl-3-methylimidazolium hexafluorophosphate,1002,2006,200600256,accepted,august 23,august 25,doi 10,headspace liquid-phase microex-,high-performance liquid chromatography,jssc,july 4,phenols,received,revised,traction},
number = {265},
pages = {42 -- 47},
pmid = {19878769},
primaryClass = {arXiv:hep-lat},
title = {{Successes and failures of backpropagation: A theoretical investigation}},
volume = {5},
year = {1997}
}
@article{Bahl1975,
author = {Bahl, L and Jelinek, Frederick},
journal = {IEEE Transactions on Information Theory},
number = {4},
pages = {404--411},
publisher = {IEEE},
title = {{Decoding for channels with insertions, deletions, and substitutions with applications to speech recognition}},
volume = {21},
year = {1975}
}
@article{Achille2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.01353v3},
author = {Achille, Alessandro and Soatto, Stefano},
eprint = {arXiv:1611.01353v3},
file = {:home/tom/Documents/Mendeley/Achille, Soatto_2016_Information Dropout Learning Optimal Representations Through Noisy Computation.pdf:pdf},
journal = {preprint ArXiv},
title = {{Information Dropout: Learning Optimal Representations Through Noisy Computation}},
volume = {1611.01353},
year = {2016}
}
@article{Papadakis2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1304.5784v2},
author = {Papadakis, Nicolas and Peyr{\'{e}}, Gabriel and Oudet, Edouard},
doi = {10.1137/130920058},
eprint = {arXiv:1304.5784v2},
file = {:home/tom/Documents/Mendeley/Papadakis, Peyr{\'{e}}, Oudet_2014_Optimal transport with proximal splitting.pdf:pdf},
issn = {19364954},
journal = {SIAM Journal on Imaging Sciences},
keywords = {Deconvolution,transport},
mendeley-tags = {Deconvolution,transport},
pages = {1--22},
title = {{Optimal transport with proximal splitting}},
url = {http://epubs.siam.org/doi/abs/10.1137/130920058},
year = {2014}
}
@inproceedings{Smaragdis2007,
author = {Smaragdis, Paris},
booktitle = {IEEE Transaction on Audio, Speech and Languages Processing},
file = {:home/tom/Documents/Mendeley/Smaragdis_2007_Convolutive Speech Bases and Their Application to Supervised Speech Separation.pdf:pdf},
number = {1},
pages = {1--12},
title = {{Convolutive Speech Bases and Their Application to Supervised Speech Separation}},
volume = {15},
year = {2007}
}
@inproceedings{Le2013,
address = {Vancouver, Canada},
archivePrefix = {arXiv},
arxivId = {1112.6209},
author = {Le, Quoc V and Ranzato, Marc'Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S and Dean, Jeff and Ng, Andrew Y},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/MSP.2011.940881},
eprint = {1112.6209},
file = {:home/tom/Documents/Mendeley/Le et al._2013_Building high-level features using large scale unsupervised learning.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {10535888},
keywords = {deep learning,unsupervised learning},
pages = {8595--8598},
pmid = {20957573},
title = {{Building high-level features using large scale unsupervised learning}},
url = {http://arxiv.org/abs/1112.6209},
year = {2013}
}
@inproceedings{Mairal2014,
abstract = {An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data. Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, e.g., digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.3332v1},
author = {Mairal, Julien and Koniusz, Piotr and Harchaoui, Zaid and Schmid, Cordelia},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {arXiv:1406.3332v1},
file = {:home/tom/Documents/Mendeley/Mairal et al._2014_Convolutional Kernel Networks.pdf:pdf},
pages = {2627--2635},
title = {{Convolutional Kernel Networks}},
url = {http://arxiv.org/abs/1406.3332},
year = {2014}
}
@inproceedings{Skau2018,
author = {Skau, Erik and Wohlberg, Brendt},
booktitle = {IEEE Image, Video, and Multidimensional Signal Processing Workshop (IVMSP)},
doi = {10.1109/IVMSPW.2018.8448536},
file = {:home/tom/Documents/Mendeley/Skau, Wohlberg_2018_A Fast Parallel Algorithm for Convolutional Sparse Coding.pdf:pdf},
isbn = {9781538609514},
title = {{A Fast Parallel Algorithm for Convolutional Sparse Coding}},
year = {2018}
}
@article{Oudre2018,
author = {Oudre, Laurent and Barrois-M{\"{u}}ller, R{\'{e}}mi and Moreau, Thomas and Truong, Charles and Vienne-Jumeau, Ali{\'{e}}nor and Ricard, Damien and Vayatis, Nicolas and Vidal, Pierre-Paul},
doi = {10.1016/B978-0-7020-4226-3.00024-X},
file = {:home/tom/Documents/Mendeley/Oudre et al._2018_Template-Based Step Detection with Inertial Measurement Units.pdf:pdf},
journal = {Sensors},
keywords = {biomedical signal processing,gait analysis,inertial measurement units,pattern recognition,physiological signals,step detection},
number = {11},
pages = {4033},
title = {{Template-Based Step Detection with Inertial Measurement Units}},
url = {http://dx.doi.org/10.1016/B978-0-7020-4226-3.00024-X},
volume = {18},
year = {2018}
}
@article{Oculo4,
author = {Cullen, Kathleen E. and {Van Horn}, Marion R.},
journal = {European Journal of Neuroscience},
number = {33},
pages = {2147--2154},
title = {{The neural control of fast vs. slow vergence eye movements}},
volume = {Jun},
year = {2011}
}
@inproceedings{Yen2014,
author = {Yen, En-Hsu and Hsieh, Cho-Jui and Ravikumar, Pradeep K. and Dhillon, Inderjit S.},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Documents/Mendeley/Yen et al._2014_Constant Nullspace Strong Convexity and Fast Convergence of Proximal Methods under High-Dimensional Settings.pdf:pdf;:home/tom/Documents/Mendeley/Yen et al._2014_Constant Nullspace Strong Convexity and Fast Convergence of Proximal Methods under High-Dimensional Settings(2).pdf:pdf},
issn = {10495258},
pages = {1008--1016},
title = {{Constant Nullspace Strong Convexity and Fast Convergence of Proximal Methods under High-Dimensional Settings}},
url = {http://papers.nips.cc/paper/5257-constant-nullspace-strong-convexity-and-fast-convergence-of-proximal-methods-under-high-dimensional-settings},
year = {2014}
}
@inproceedings{Jung2014,
archivePrefix = {arXiv},
arxivId = {1402.4078},
author = {Jung, Alexander and Eldar, Yonina C. and G{\"{o}}rtz, Norbert},
booktitle = {European Signal Processing Conference (EUSIPCO)},
eprint = {1402.4078},
file = {:home/tom/Documents/Mendeley/Jung, Eldar, G{\"{o}}rtz_2014_Performance Limits of Dictionary Learning for Sparse Coding.pdf:pdf},
title = {{Performance Limits of Dictionary Learning for Sparse Coding}},
url = {http://arxiv.org/abs/1402.4078},
year = {2014}
}
@inproceedings{Spielman2013,
abstract = {We consider the problem of learning sparsely used dictionaries with an arbitrary square dictionary and a random, sparse coefficient matrix. We prove that $O (n \log n)$ samples are sufficient to uniquely determine the coefficient matrix. Based on this proof, we design a polynomial-time algorithm, called Exact Recovery of Sparsely-Used Dictionaries (ER-SpUD), and prove that it probably recovers the dictionary and coefficient matrix when the coefficient matrix is sufficiently sparse. Simulation results show that ER-SpUD reveals the true dictionary as well as the coefficients with probability higher than many state-of-the-art algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.5882v1},
author = {Spielman, Daniel A. and Wang, Huan and Wright, John},
booktitle = {Conference on Learning Theroy (COLT)},
eprint = {arXiv:1206.5882v1},
file = {:home/tom/Documents/Mendeley/Spielman, Wang, Wright_2013_Exact recovery of sparsely-used dictionaries.pdf:pdf},
isbn = {9781577356332},
issn = {10450823},
keywords = {dictionary learning,matrix decomposition,matrix sparsification},
pages = {3087--3090},
title = {{Exact recovery of sparsely-used dictionaries}},
volume = {23},
year = {2013}
}
@techreport{Scherrer2012a,
abstract = {We present a generic framework for par- allel coordinate descent (CD) algorithms that includes, as special cases, the orig- inal sequential algorithms Cyclic CD and Stochastic CD, as well as the recent paral- lel Shotgun algorithm. We introduce two novel parallel algorithms that are also spe- cial cases—Thread-Greedy CD and Coloring- Based CD—and give performance measure- ments for an OpenMP implementation of these. 1.},
author = {Scherrer, Chad and Halappanavar, Mahantesh and Tewari, Ambuj and Haglin, David},
booktitle = {Pacific Northwest National Laboratory (PNNL)},
file = {:home/tom/Documents/Mendeley/Scherrer et al._2012_Scaling Up Coordinate Descent Algorithms for Large $ell_1$ Regularization Problems.pdf:pdf},
institution = {Pacific Northwest National Laboratory (PNNL)},
isbn = {978-1-4503-1285-1},
title = {{Scaling Up Coordinate Descent Algorithms for Large $\ell_1$ Regularization Problems}},
volume = {2},
year = {2012}
}
@article{Giryes2016,
annote = {* Use projected gradient descent as a base for there work.
* Use an approximation of the solution set to do a 2 stage, innacurate projection on the solution set and give theoretical analysis

* Small link to LISTA, not so clear.},
archivePrefix = {arXiv},
arxivId = {1605.09232},
author = {Giryes, Raja and Eldar, Yonina C. and Bronstein, Alex M. and Sapiro, Guillermo},
eprint = {1605.09232},
file = {:home/tom/Documents/Mendeley//Giryes et al._2018_Tradeoffs between Convergence Speed and Reconstruction Accuracy in Inverse Problems.pdf:pdf},
journal = {IEEE Transaction on Signal Processing},
number = {7},
title = {{Tradeoffs between Convergence Speed and Reconstruction Accuracy in Inverse Problems}},
url = {http://arxiv.org/abs/1605.09232},
volume = {66},
year = {2018}
}
@article{Jones2016,
abstract = {Rhythms are a prominent signature of brain activity. Their expression is correlated with numerous examples of healthy information processing and their fluctuations are a marker of disease states. Yet, their causal or epiphenomenal role in brain function is still highly debated. We review recent studies showing brain rhythms are not always ‘rhythmic', by which we mean representative of repeated cycles of activity. Rather, high power and continuous rhythms in averaged signals can represent brief transient events on single trials whose density accumulates in the average. We also review evidence showing time-domain signals with vastly different waveforms can exhibit identical spectral-domain frequency and power. Further, non-oscillatory waveform feature can create spurious high spectral power. Knowledge of these possibilities is essential when interpreting rhythms and is easily missed without considering pre-processed data. Lastly, we discuss how these findings suggest new directions to pursue in our quest to discover the mechanism and meaning of brain rhythms.},
author = {Jones, Stephanie R.},
doi = {10.1016/j.conb.2016.06.010},
file = {:home/tom/Documents/Mendeley/Jones_2016_When brain rhythms aren't ‘rhythmic' implication for their mechanisms and meaning.pdf:pdf},
isbn = {0959-4388},
issn = {18736882},
journal = {Current Opinion in Neurobiology},
pages = {72--80},
pmid = {27400290},
publisher = {Elsevier Ltd},
title = {{When brain rhythms aren't ‘rhythmic': implication for their mechanisms and meaning}},
url = {http://dx.doi.org/10.1016/j.conb.2016.06.010},
volume = {40},
year = {2016}
}
@inproceedings{alaoui2015fast,
author = {Alaoui, Ahmed and Mahoney, Michael W},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {775--783},
title = {{Fast randomized kernel ridge regression with statistical guarantees}},
year = {2015}
}
@article{Vautard1989,
author = {Vautard, Robert and Ghil, Michael},
file = {:home/tom/Documents/Mendeley/Vautard, Ghil_1989_Deterministic chaos, stochastic processes, and dimension.pdf:pdf},
journal = {Physica D: Nonlinear Phenomena},
number = {3},
pages = {395--424},
title = {{Deterministic chaos, stochastic processes, and dimension}},
volume = {35},
year = {1989}
}
@article{DAspremont2014,
archivePrefix = {arXiv},
arxivId = {1205.0121},
author = {D'Aspremont, Alexandre and Bach, Francis and Ghaoui, Laurent El},
doi = {10.1007/s10107-014-0751-7},
eprint = {1205.0121},
file = {:home/tom/Documents/Mendeley/d'Aspremont, Bach, Ghaoui_2014_Approximation bounds for sparse principal component analysis.pdf:pdf},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {62H25,90C22,90C27},
number = {1-2},
pages = {89--110},
title = {{Approximation bounds for sparse principal component analysis}},
volume = {148},
year = {2014}
}
@article{salakhutdinov2009semantic,
author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
journal = {International Journal of Approximate Reasoning},
number = {7},
pages = {969--978},
publisher = {Elsevier},
title = {{Semantic hashing}},
volume = {50},
year = {2009}
}
@article{Yue2016,
archivePrefix = {arXiv},
arxivId = {1605.07522},
author = {Yue, Man-Chung and Zhou, Zirui and So, Anthony Man-Cho},
eprint = {1605.07522},
file = {:home/tom/Documents/Mendeley/Yue, Zhou, So_2016_Inexact Regularized Proximal Newton Method Provable Convergence Guarantees for Non-Smooth Convex Minimization without.pdf:pdf},
journal = {preprint ArXiv},
title = {{Inexact Regularized Proximal Newton Method: Provable Convergence Guarantees for Non-Smooth Convex Minimization without Strong Convexity}},
url = {http://arxiv.org/abs/1605.07522},
volume = {1605.07522},
year = {2016}
}
@inproceedings{Wang2015,
abstract = {Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized computation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic modeling and obtain competitive results.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1506.04448},
author = {Wang, Yining and Tung, Hsiao-Yu and Smola, Alexander and Anandkumar, Animashree},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1506.04448},
keywords = {count sketch,randomized methods,spectral methods,tensor cp decomposition,topic},
pages = {991--999},
title = {{Fast and Guaranteed Tensor Decomposition via Sketching}},
url = {http://arxiv.org/abs/1506.04448},
year = {2015}
}
@article{Loh2014,
archivePrefix = {arXiv},
arxivId = {1412.5632},
author = {Loh, Po-Ling and Wainwright, Martin J.},
eprint = {1412.5632},
file = {:home/tom/Documents/Mendeley/Loh, Wainwright_2014_Support recovery without incoherence A case for nonconvex regularization.pdf:pdf},
journal = {preprint ArXiv},
title = {{Support recovery without incoherence: A case for nonconvex regularization}},
url = {http://arxiv.org/abs/1412.5632},
volume = {1412.5632},
year = {2014}
}
@article{Wang2007,
abstract = {A nonparametric multiple regression method, based on the extended Gini that depends on one parameter, v, is investigated. The parameter v enables production of infinite alternative linear approximations to the regression curve which differ in the weighting schemes applied to the slopes of the curve. The method allows the investigator to stress different sections of one independent variable while keeping the treatment of the other independent variables intact. As an application we investigate nonresponse patterns in a survey of household expenditures to learn about the relationship between nonresponse and income. The empirical results show that the higher the income, the higher the response rate, and the larger the household, the higher the response rate. The Arab population tends to respond more than the Jewish one, whereas the ultrareligious group tends to respond less than the rest of the population. The implications on the bias in the estimates are discussed.},
author = {Wang, Hansheng and Guodong, Li and Guohua, Jiang},
doi = {10.1198/00},
file = {:home/tom/Documents/Mendeley/Wang, Guodong, Guohua_2007_Robust Regression Shrinkage and Consistent Variable Selection Through the LAD-Lasso.pdf:pdf},
isbn = {0040170040000},
issn = {0735-0015},
journal = {Journal of Business \& Economic Statistics},
keywords = {lad,lad-lasso,lasso,oracle property},
number = {3},
pages = {347--355},
pmid = {18392118},
title = {{Robust Regression Shrinkage and Consistent Variable Selection Through the LAD-Lasso}},
url = {http://www.tandfonline.com/doi/abs/10.1198/00},
volume = {25},
year = {2007}
}
@article{Neyshabur2017,
archivePrefix = {arXiv},
arxivId = {1706.08947},
author = {Kawaguchi, Kenji and {Pack Kaelbling}, Leslie and Bengio, Yoshua},
eprint = {1706.08947},
file = {:home/tom/Documents/Mendeley/Kawaguchi, Pack Kaelbling, Bengio_2017_Generalization in Deep Learning.pdf:pdf},
journal = {preprint ArXiv},
title = {{Generalization in Deep Learning}},
url = {http://arxiv.org/abs/1706.08947},
volume = {1710.05468},
year = {2017}
}
@article{LoPresti2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1212.0402},
author = {{Lo Presti}, Liliana and {La Cascia}, Marco},
doi = {10.1016/j.patcog.2015.11.019},
eprint = {arXiv:1212.0402},
file = {:home/tom/Documents/Mendeley/Lo Presti, La Cascia_2016_3D skeleton-based human action classification A survey.pdf:pdf},
isbn = {9783319106052},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Action classification,Action recognition,Body joint,Body pose representation,Skeleton},
pages = {130--147},
publisher = {Elsevier},
title = {{3D skeleton-based human action classification: A survey}},
url = {http://dx.doi.org/10.1016/j.patcog.2015.11.019},
volume = {53},
year = {2016}
}
@phdthesis{Moreau2017c,
author = {Moreau, Thomas},
file = {:home/tom/Documents/Mendeley/Moreau_2017_Convolutional Sparse Representations – application to physiological signals and interpretab- ility for Deep Learning.pdf:pdf},
school = {CMLA, ENS Paris-Saclay, Universit{\'{e}} Paris-Saclay},
title = {{Convolutional Sparse Representations – application to physiological signals and interpretab- ility for Deep Learning}},
year = {2017}
}
@article{Oculo21b,
author = {May, EF and Truxal, AR},
journal = {Journal of Neuro-Ophthalmology},
number = {17},
pages = {84--85},
title = {{Loss of vision alone may result in seesaw nystagmus.}},
volume = {Jun},
year = {1997}
}
@article{Ahmed1974,
author = {Ahmed, N. and Natarajan, T. and Rao, K.R.},
doi = {10.1109/T-C.1974.223784},
file = {:home/tom/Documents/Mendeley/Ahmed, Natarajan, Rao_1974_Discrete Cosine Transform.pdf:pdf},
isbn = {0018-9340},
issn = {0018-9340},
journal = {IEEE Transactions on Computers},
number = {1},
pages = {90--93},
title = {{Discrete Cosine Transform}},
url = {http://dasan.sejong.ac.kr/$\sim$dihan/dip/p5_DCT.pdf},
volume = {C-23},
year = {1974}
}
@article{Lee2014,
archivePrefix = {arXiv},
arxivId = {1206.1623v1},
author = {Lee, Jason D and Sun, Yeukai and Saunders, Michael A},
eprint = {1206.1623v1},
file = {:home/tom/Documents/Mendeley/Lee, Sun, Saunders_2014_Proximal Newton-type Methods for Minimizing Convex Objective Functions in Composite Form(2).pdf:pdf},
journal = {SIAM Journal on Optimization},
number = {3},
pages = {1420--1443},
title = {{Proximal Newton-type Methods for Minimizing Convex Objective Functions in Composite Form}},
volume = {24},
year = {2014}
}
@article{Bruna2012,
archivePrefix = {arXiv},
arxivId = {1203.1513},
author = {Bruna, Joan and Mallat, St\'ephane},
doi = {E618F3E2-0F49-4C93-89B8-CDCE0EADE5D9},
eprint = {1203.1513},
file = {:home/tom/Documents/Mendeley/Bruna, Mallat_2013_Invariant Scattering Networks.pdf:pdf},
isbn = {0162-8828},
issn = {1939-3539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {8},
pages = {1872--1886},
pmid = {23070037},
title = {{Invariant Scattering Networks}},
url = {http://people.ee.duke.edu/$\sim$lcarin/Bo8.6.2012.pdf},
volume = {35},
year = {2013}
}
@article{JMLR:v16:liu15a,
author = {Liu, Ji and Wright, Stephen J and R{\'{e}}, Christopher and Bittorf, Victor and Sridhar, Srikrishna},
file = {:home/tom/Documents/Mendeley//Liu et al._2015_An Asynchronous Parallel Stochastic Coordinate Descent Algorithm.pdf:pdf},
journal = {Journal of Machine Learning Research (JMLR)},
pages = {285--322},
title = {{An Asynchronous Parallel Stochastic Coordinate Descent Algorithm}},
url = {http://jmlr.org/papers/v16/liu15a.html},
volume = {16},
year = {2015}
}
@article{Zanella2009,
abstract = {Random matrices play a crucial role in the design and analysis of multiple-input multiple-output (MIMO) systems. In particular, performance of MIMO systems depends on the statistical properties of a subclass of random matrices known as Wishart when the propagation environment is characterized by Rayleigh or Rician fading. This paper focuses on the stochastic analysis of this class of matrices and proposes a general methodology to evaluate some multiple nested integrals of interest. With this methodology we obtain a closed-form expression for the joint probability density function of k consecutive ordered eigenvalues and, as a special case, the PDF of the lscrth ordered eigenvalue of Wishart matrices. The distribution of the largest eigenvalue can be used to analyze the performance of MIMO maximal ratio combining systems. The PDF of the smallest eigenvalue can be used for MIMO antenna selection techniques. Finally, the PDF the kth largest eigenvalue finds applications in the performance analysis of MIMO singular value decomposition systems.},
author = {Zanella, Alberto and Chiani, Marco and Win, Moe Z.},
doi = {10.1109/TCOMM.2009.04.070143},
file = {:home/tom/Documents/Mendeley/Zanella, Chiani, Win_2009_On the marginal distribution of the eigenvalues of wishart matrices(2).pdf:pdf},
isbn = {9781424420742},
issn = {00906778},
journal = {IEEE Transactions on Communications},
keywords = {Eeigenvalue distribution,Marginal distribution,Multiple-input multiple-output (MIMO),Wishart matrices},
number = {4},
pages = {1050--1060},
title = {{On the marginal distribution of the eigenvalues of wishart matrices}},
volume = {57},
year = {2009}
}
@article{Lin2015a,
abstract = {We consider the problem of minimizing the sum of two convex functions: one is smooth and given by a gradient oracle, and the other is separable over blocks of coordinates and has a simple known structure over each block. We develop an accelerated randomized proximal coordinate gradient (APCG) method for minimizing such convex composite functions. For strongly convex functions, our method achieves faster linear convergence rates than existing randomized proximal coordinate gradient methods. Without strong convexity, our method enjoys accelerated sublinear convergence rates. We show how to apply the APCG method to solve the regularized empirical risk minimization (ERM) problem and devise efficient implementations that avoid full-dimensional vector operations. For ill-conditioned ERM problems, our method obtains improved convergence rates than the state-of-the-art stochastic dual coordinate ascent method.},
archivePrefix = {arXiv},
arxivId = {1407.1296},
author = {Lin, Qihang and Lu, Zhaosong and Xiao, Lin},
doi = {10.1137/141000270},
eprint = {1407.1296},
file = {:home/tom/Documents/Mendeley/Lin, Lu, Xiao_2015_An Accelerated Randomized Proximal Coordinate Gradient Method and its Application to Regularized Empirical Risk Minim.pdf:pdf},
issn = {1052-6234},
journal = {SIAM Journal on Optimization},
keywords = {65Y20,68W20,90C06,90C25,acceler-ated proximal gradient method,convex optimization,coordinate descent method,randomized algorithm},
number = {4},
pages = {2244--2273},
title = {{An Accelerated Randomized Proximal Coordinate Gradient Method and its Application to Regularized Empirical Risk Minimization}},
url = {http://epubs.siam.org/doi/10.1137/141000270},
volume = {25},
year = {2015}
}
@inproceedings{Schmidt2011,
abstract = {We consider the problem of optimizing the sum of a smooth convex function and a non-smooth convex function using proximal-gradient methods, where an error is present in the calculation of the gradient of the smooth term or in the proximity operator with respect to the non-smooth term. We show that both the basic proximal-gradient method and the accelerated proximal-gradient method achieve the same convergence rate as in the error-free case, provided that the errors decrease at appropriate rates.Using these rates, we perform as well as or better than a carefully chosen fixed error level on a set of structured sparsity problems.},
address = {Grenada, Spain},
archivePrefix = {arXiv},
arxivId = {1109.2415},
author = {Schmidt, Mark and Roux, Nicolas Le and Bach, Francis},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1109.2415},
file = {:home/tom/Documents/Mendeley/Schmidt, Roux, Bach_2011_Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization.pdf:pdf},
isbn = {9781618395993},
pages = {1458--1466},
title = {{Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization}},
year = {2011}
}
@article{Ayachi2016,
author = {Ayachi, F and Nguyen, H and Goubault, E and Boissy, P and Duval, C},
file = {:home/tom/Documents/Mendeley/Ayachi et al._2016_The Use of Empirical Mode Decomposition-Based Algorithm and Inertial Measurement Units to Auto-Detect Daily Living Ac.pdf:pdf},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
number = {10},
pages = {1060--1070},
title = {{The Use of Empirical Mode Decomposition-Based Algorithm and Inertial Measurement Units to Auto-Detect Daily Living Activities of Healthy Adults}},
volume = {24},
year = {2016}
}
@article{Shin2017,
abstract = {<p>Beta oscillations (15-29Hz) are among the most prominent signatures of brain activity. Beta power is predictive of healthy and abnormal behaviors, including perception, attention and motor action. In non-averaged signals, beta can emerge as transient high-power 'events'. As such, functionally relevant differences in averaged power across time and trials can reflect changes in event number, power, duration, and/or frequency span. We show that functionally relevant differences in averaged beta power in primary somatosensory neocortex reflect a difference in the number of high-power beta events per trial, i.e. event rate. Further, beta events occurring close to the stimulus were more likely to impair perception. These results are consistent across detection and attention tasks in human magnetoencephalography, and in local field potentials from mice performing a detection task. These results imply that an increased propensity of beta events predicts the failure to effectively transmit information through specific neocortical representations.</p>},
author = {Shin, Hyeyoung and Law, Robert and Tsutsui, Shawn and Moore, Christopher I. and Jones, Stephanie R.},
doi = {10.7554/eLife.29086},
file = {:home/tom/Documents/Mendeley/Shin et al._2017_The rate of transient beta frequency events predicts behavior across tasks and species.pdf:pdf},
isbn = {9781479999880},
issn = {2050084X},
journal = {eLife},
pages = {1--31},
title = {{The rate of transient beta frequency events predicts behavior across tasks and species}},
volume = {6},
year = {2017}
}
@inproceedings{Giselsson2014,
address = {Los Angeles, CA},
author = {Giselsson, Pontus and Boyd, Stephen},
booktitle = {IEEE Conference on Decision and Control (CDC)},
file = {:home/tom/Documents/Mendeley/Giselsson, Boyd_2014_Monotonicity and Restart in Fast Gradient Methods.pdf:pdf},
keywords = {optimization,proximal method,sparse coding},
pages = {5058----5063},
publisher = {IEEE},
title = {{Monotonicity and Restart in Fast Gradient Methods}},
year = {2014}
}
@article{Rubinstein2008,
author = {Rubinstein, Ron and Zibulevsky, Michael and Elad, Michael},
doi = {10.1.1.182.9978},
file = {:home/tom/Documents/Mendeley/Rubinstein, Zibulevsky, Elad_2008_Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit.pdf:pdf},
journal = {CS Technion},
number = {8},
pages = {1--15},
title = {{Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit}},
url = {http://cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/2008/CS/CS-2008-08.revised.pdf},
volume = {40},
year = {2008}
}
@article{Hinton2006,
author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
file = {:home/tom/Documents/Mendeley/Hinton, Osindero, Teh_2006_A fast learning algorithm for deep belief nets.pdf:pdf},
journal = {Neural computation},
number = {7},
pages = {1527--1554},
publisher = {MIT Press},
title = {{A fast learning algorithm for deep belief nets}},
volume = {18},
year = {2006}
}
@book{Combettes2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Combettes, Patrick L and Bauschke, Heinz H.},
booktitle = {Springer Science & Business Media},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/Mendeley//Combettes, Bauschke_2011_Convex Analysis and Monotone Operator Theory in Hilbert Spaces.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pages = {1--468},
pmid = {25246403},
publisher = {Springer},
title = {{Convex Analysis and Monotone Operator Theory in Hilbert Spaces}},
year = {2011}
}
@inproceedings{Sprechmann2013,
address = {Vancouver, Canada},
author = {Sprechmann, Pablo and Bronstein, Alex M. and Bronstein, Michael and Sapiro, Guillermo},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:home/tom/Documents/Mendeley/Sprechmann et al._2013_Learnable Low Rank Sparse Models for Speech Denoising.pdf:pdf},
pages = {136--140},
title = {{Learnable Low Rank Sparse Models for Speech Denoising}},
year = {2013}
}
@article{Liu2015,
abstract = {We describe an asynchronous parallel stochastic coordinate descent algorithm for minimizing smooth unconstrained or separably constrained functions. The method achieves a linear convergence rate on functions that satisfy an essential strong convexity property and a sublinear rate ($1/K$) on general convex functions. Near-linear speedup on a multicore system can be expected if the number of processors is $O(n^{1/2})$ in unconstrained optimization and $O(n^{1/4})$ in the separable-constrained case, where $n$ is the number of variables. We describe results from implementation on 40-core processors.},
archivePrefix = {arXiv},
arxivId = {1311.1873},
author = {Liu, Ji and Wright, Stephen J and R{\'{e}}, Christopher and Bittorf, Victor and Sridhar, Srikrishna},
eprint = {1311.1873},
file = {:home/tom/Documents/Mendeley//Liu et al._2015_An Asynchronous Parallel Stochastic Coordinate Descent Algorithm.pdf:pdf},
isbn = {9781634393973},
issn = {15337928},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {asynchronous parallel optimization,stochastic coordinate descent},
pages = {285--322},
title = {{An asynchronous parallel stochastic coordinate descent algorithm}},
url = {http://arxiv.org/abs/1311.1873},
volume = {16},
year = {2015}
}
@article{Woody1967,
author = {Woody, Charles D},
file = {:home/tom/Documents/Mendeley/Woody_1967_Characterisation of an adaptive Filter for the Analysis of Variable Latency in Electric Signals.pdf:pdf},
journal = {Medical and Biological engineering},
number = {6},
pages = {539--553},
title = {{Characterisation of an adaptive Filter for the Analysis of Variable Latency in Electric Signals}},
volume = {5},
year = {1967}
}
@article{Vidaurre2018,
abstract = {Frequency-specific oscillations and phase-coupling of neuronal populations have been proposed as an essential mechanism for the coordination of activity between brain areas during cognitive tasks. To provide an effective substrate for cognitive function, we reasoned that ongoing functional brain networks should also be able to reorganize and coordinate in a similar manner. To test this hypothesis, we here use a novel method for identifying repeating patterns of network dynamics, and show that resting networks in magnetoencephalography are well characterised by visits to short-lived transient brain states ($\sim$50-100ms), with spatially distinct power and phase-coupling in specific frequency bands. Brain states were identified for sensory, motor networks and higher-order cognitive networks; these include a posterior cognitive network in the alpha range (8-12Hz) and an anterior cognitive network in the delta/theta range (1-7Hz). Both cognitive networks exhibit particularly high power and coherence, and contain brain areas corresponding to posterior and anterior subdivisions of the default mode network. Our results show that large-scale cortical phase-coupling networks operate in very specific frequency bands, possibly reflecting functional specialisation at different intrinsic timescales.},
author = {Vidaurre, Diego and Hunt, Laurence T. and Quinn, Andrew J. and Hunt, Benjamin A.E. and Brookes, Matthew J. and Nobre, Anna C. and Woolrich, Mark W.},
doi = {10.1038/s41467-018-05316-z},
file = {:home/tom/Documents/Mendeley/Vidaurre et al._2018_Spontaneous cortical activity transiently organises into frequency specific phase-coupling networks.pdf:pdf},
isbn = {4146701805},
issn = {20411723},
journal = {Nature Communications},
number = {1},
pmid = {30061566},
publisher = {Springer US},
title = {{Spontaneous cortical activity transiently organises into frequency specific phase-coupling networks}},
url = {http://dx.doi.org/10.1038/s41467-018-05316-z},
volume = {9},
year = {2018}
}
@article{dijkstra2008detection,
author = {Dijkstra, B and Zijlstra, W and Scherder, E and Kamsma, Y},
journal = {Age and ageing},
number = {4},
pages = {436--441},
publisher = {Br Geriatrics Soc},
title = {{Detection of walking periods and number of steps in older adults and patients with Parkinson's disease: accuracy of a pedometer and an accelerometry-based method}},
volume = {37},
year = {2008}
}
@article{Oculo25,
author = {Odom, JV and Bach, M and Brigell, M and Holder, GE and McCulloch, DL and Tormene, AP and Et, Al.},
journal = {Documenta Ophthalmologica},
number = {120},
pages = {111--9},
title = {{ISCEV standard for clinical visual evoked potentials}},
volume = {Feb},
year = {2010}
}
@inproceedings{tran2012high,
address = {Ho Chi Minh City, Vietnam},
author = {Tran, K and Le, T and Dinh, T},
booktitle = {International Symposium on Signal Processing and Information Technology (ISSPIT)},
organization = {IEEE},
pages = {213--217},
title = {{A high-accuracy step counting algorithm for iPhones using accelerometer}},
year = {2012}
}
@article{DAspremont2008,
annote = {* True sparse PCA - with orthogonality constraints},
author = {D'Aspremont, Alexandre and Bach, Francis and Ghaoui, L.E.},
file = {:home/tom/Documents/Mendeley/d'Aspremont, Bach, Ghaoui_2008_Optimal solutions for sparse principal component analysis.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {lasso,pca,sparse eigenvalues,sparse recovery,subset selection},
pages = {1269--1294},
title = {{Optimal solutions for sparse principal component analysis}},
url = {http://dl.acm.org/citation.cfm?id=1442775},
volume = {9},
year = {2008}
}
@phdthesis{Goroshin2015,
author = {Goroshin, Rostislav},
file = {:home/tom/Documents/Mendeley/Goroshin_2015_Unsupervised Feature Learning in Computer Vision.pdf:pdf},
number = {September},
pages = {1----100},
school = {New-York University},
title = {{Unsupervised Feature Learning in Computer Vision}},
year = {2015}
}
@article{Oudre2012,
abstract = {In this paper, we introduce a novel nonparametric classification technique based on the use of the Wasserstein distance. The proposed scheme is applied in a biomedical context for the analysis of recorded accelerometer data: the aim is to retrieve three types of periodic activities (walking, biking, and running) from a time-frequency representation of the data. The main interest of the use of the Wasserstein distance lies in the fact that it is less sensitive to the location of the frequency peaks than to the global structure of the frequency pattern, allowing us to detect activities almost independently of their speed or incline. Our system is tested on a 24-subject corpus: results show that the use of Wasserstein distance combined with some supervised learning techniques allows us to compare with some more complex classification systems.},
author = {Oudre, Laurent and Jakubowicz, J{\'{e}}r{\'{e}}mie and Bianchi, Pascal and Simon, Chantal},
doi = {10.1109/TBME.2012.2190930},
file = {:home/tom/Documents/Mendeley/Oudre et al._2012_Classification of periodic activities using the Wasserstein distance.pdf:pdf},
isbn = {0018-9294},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Accelerometer signals,Wasserstein distance,biomedical signal processing,classification},
number = {6},
pages = {1610--1619},
pmid = {22434794},
title = {{Classification of periodic activities using the Wasserstein distance}},
volume = {59},
year = {2012}
}
@article{nemirovski2009robust,
author = {Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
journal = {SIAM Journal on optimization},
number = {4},
pages = {1574--1609},
publisher = {SIAM},
title = {{Robust stochastic approximation approach to stochastic programming}},
volume = {19},
year = {2009}
}
@article{willemsen1990automatic,
author = {Willemsen, A and Bloemhof, F and Boom, H},
journal = {IEEE Transactions on Biomedical Engineering},
number = {12},
pages = {1201--1208},
publisher = {IEEE},
title = {{Automatic stance-swing phase detection from accelerometer data for peroneal nerve stimulation}},
volume = {37},
year = {1990}
}
@article{Brockwell1988,
author = {Brockwell, Peter J. and Davis, Richard A.},
doi = {10.1016/0304-4149(88)90063-4},
file = {:home/tom/Documents/Mendeley/Brockwell, Davis_1988_Simple consistent estimation of the coefficients of a linear filter.pdf:pdf},
issn = {03044149},
journal = {Stochastic Processes and their Applications},
keywords = {ARMA process,asymptotic distribution,identification,innovations,linear filter,linear process,preliminary estimation},
number = {1},
pages = {47--59},
title = {{Simple consistent estimation of the coefficients of a linear filter}},
volume = {28},
year = {1988}
}
@article{Johnson1984,
author = {Johnson, William B. and Lindenstrauss, Joram},
file = {:home/tom/Documents/Mendeley/Johnson, Lindenstrauss_1984_Extensions of Lipschitz Mappings into a Hilbert Space.pdf:pdf},
journal = {Contemporary Mathematics},
number = {1},
pages = {189--206},
title = {{Extensions of Lipschitz Mappings into a Hilbert Space}},
volume = {26},
year = {1984}
}
@inproceedings{Jiang2011,
address = {Colorado Spring, CO, USA},
author = {Jiang, Zhuolin and Lin, Zhe and Davis, Larry S and Incorporated, Adobe Systems and Jose, San},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Documents/Mendeley/Jiang et al._2011_Learning A Discriminative Dictionary for Sparse Coding via Label Consistent K-SVD.pdf:pdf},
pages = {1697--1704},
title = {{Learning A Discriminative Dictionary for Sparse Coding via Label Consistent K-SVD}},
year = {2011}
}
@misc{libby2012simple,
author = {Libby, R},
howpublished = {Research report},
title = {{A Simple Method for Reliable Footstep Detection in Embedded Sensor Platforms}},
year = {2012}
}
@article{Cohen2017,
archivePrefix = {arXiv},
arxivId = {1705.02302},
author = {Cohen, Nadav and Sharir, Or and Levine, Yoav and Tamari, Ronen and Yakira, David and Shashua, Amnon},
eprint = {1705.02302},
file = {:home/tom/Documents/Mendeley/Cohen et al._2017_Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions.pdf:pdf},
journal = {preprint ArXiv},
keywords = {convolutional networks,expressiveness,hierarchical tensor decompositions},
title = {{Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions}},
url = {http://arxiv.org/abs/1705.02302},
volume = {1705.02302},
year = {2017}
}
@article{Olah2017,
annote = {https://distill.pub/2017/feature-visualization},
author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
doi = {10.23915/distill.00007},
journal = {Distill},
title = {{Feature Visualization}},
year = {2017}
}
@book{VanHandel2014,
abstract = {These lecture notes were written for the course ORF 570: Probability in High Dimension that I taught at Princeton in the Spring 2014 semester. The aim was to introduce in as cohesive a manner as I could manage a set of methods, many of which have their origin in probability in Banach spaces, that arise across a broad range of contemporary problems in di↵erent areas. The notes are necessarily incomplete. The ambitious syllabus for the course was laughably beyond the scope of Princeton's 12-week semester. As a result, there are regrettable omissions, as well as many fascinating topics that I would have liked to but could not cover in the context of this course. These include: a. Bernstein's inequality does not appear anywhere in these notes (disgrace-ful!), nor do any Bernstein-type concentration inequalities (such as con-centration of the exponential distribution and Talagrand's concentration inequality for empirical processes) and the notion of modified log-Sobolev inequalities. These should be included at the end of Part I. b. Chaining with adaptive truncation and entropy with brackets. Beyond be-ing a classical topic in empirical process theory, the power of the idea of adaptive truncation has again proven its value in the recent solution of the long-standing Bernoulli problem due to Bednorz and Lata la.},
author = {{Van Handel}, Ramon},
booktitle = {Princeton University},
file = {:home/tom/Documents/Mendeley/Van Handel_2014_Probability in High Dimension.pdf:pdf},
title = {{Probability in High Dimension}},
url = {http://www.princeton.edu/$\sim$rvan/ORF570.pdf},
year = {2014}
}
@inproceedings{Lewicki1999,
address = {Denver, CO, USA},
author = {Lewicki, Michael S and Sejnowski, Terrence J and Jolla, La},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Documents/Mendeley/Lewicki, Sejnowski, Jolla_1999_Coding time-varying signals using sparse, shift-invariant representations.pdf:pdf},
pages = {730--736},
title = {{Coding time-varying signals using sparse, shift-invariant representations}},
year = {1999}
}
@inproceedings{Zhang2010,
address = {San Francisco, CA, USA},
author = {Zhang, Qiang and Li, Baoxin},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Documents/Mendeley/Zhang, Li_2010_Discriminative K-SVD for Dictionary Learning in Face Recognition.pdf:pdf},
title = {{Discriminative K-SVD for Dictionary Learning in Face Recognition}},
year = {2010}
}
@inproceedings{Oculo14,
author = {Hoffmann, LC and Berry, SD},
booktitle = {National Academy of Sciences of the United States of America},
pages = {21371--21376},
title = {{Cerebellar theta oscillations are synchronized during hippocampal theta-contingent trace conditioning}},
year = {2009}
}
@inproceedings{Kim2010,
address = {Vancouver, Canada},
author = {Kim, Taehwan and Shakhnarovich, Gregory and Urtasun, Raquel},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Documents/Mendeley/Kim, Shakhnarovich, Urtasun_2010_Sparse Coding for Learning Interpretable Spatio-Temporal Primitives.pdf:pdf},
isbn = {9781617823800},
pages = {1117--1125},
title = {{Sparse Coding for Learning Interpretable Spatio-Temporal Primitives}},
year = {2010}
}
@article{Duchi2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
eprint = {arXiv:1103.4296v1},
file = {:home/tom/Documents/Mendeley//Duchi, Hazan, Singer_2011_Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
isbn = {9780982252925},
issn = {15324435},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://jmlr.org/papers/v12/duchi11a.html},
volume = {12},
year = {2011}
}
@inproceedings{hadsell2008deep,
author = {Hadsell, Raia and Erkan, Ayse and Sermanet, Pierre and Scoffier, Marco and Muller, Urs and LeCun, Yann},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
organization = {IEEE},
pages = {628--633},
title = {{Deep belief net learning in a long-range vision system for autonomous off-road driving}},
year = {2008}
}
@inproceedings{Chen2018,
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1808.10038},
author = {Chen, Xiaohan and Liu, Jialin and Wang, Zhangyang and Yin, Wotao},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1808.10038},
file = {:home/tom/Documents/Mendeley/Chen et al._2018_Theoretical Linear Convergence of Unfolded ISTA and its Practical Weights and Thresholds.pdf:pdf},
title = {{Theoretical Linear Convergence of Unfolded ISTA and its Practical Weights and Thresholds}},
url = {http://arxiv.org/abs/1808.10038},
year = {2018}
}
@inproceedings{Choromanska2015,
address = {San Diego, CA, USA},
archivePrefix = {arXiv},
arxivId = {1412.0233},
author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'{e}}rard Ben and LeCun, Yann},
booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
eprint = {1412.0233},
file = {:home/tom/Documents/Mendeley/Choromanska et al._2015_The Loss Surfaces of Multilayer Networks.pdf:pdf},
isbn = {1412.0233},
issn = {15337928},
keywords = {Neural Network,Optimization},
pages = {192----204},
title = {{The Loss Surfaces of Multilayer Networks}},
url = {http://arxiv.org/abs/1412.0233%5Cnhttp://www.arxiv.org/pdf/1412.0233.pdf},
year = {2015}
}
@phdthesis{Mbalo2016,
author = {Mbalo, NDIAYE},
title = {{Th{\`{e}}se de doctorat}},
year = {2016}
}
@inproceedings{rahimi2007random,
address = {Vancouver, Canada},
author = {Rahimi, Ali and Recht, Benjamin},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1177--1184},
title = {{Random features for large-scale kernel machines}},
year = {2007}
}
@article{Gabor1946,
author = {Gabor, Dennis},
doi = {10.1049/ji-3-2.1946.0074},
file = {:home/tom/Documents/Mendeley/Gabor_1946_Theory of Communication.pdf:pdf},
isbn = {6173845350},
issn = {09252312},
journal = {Journal of the Institution of Electrical Engineers},
number = {26},
pages = {429--457},
pmid = {17190757},
title = {{Theory of Communication}},
url = {citeulike-article-id:4452465},
volume = {93(26)},
year = {1946}
}
@article{Bruckstein2009,
author = {Bruckstein, Alfred M and Donoho, David L and Elad, Michael},
file = {:home/tom/Documents/Mendeley/Bruckstein, Donoho, Elad_2009_From Sparse Solutions of Systems of Equations to Sparse Modeling of Signals and Images.pdf:pdf},
journal = {Society for Industrial and Applied Mathematics},
keywords = {Sparse-Land,basis pursuit,compression,denoising,dictionary learning,inverse problems,linear system of equations,matching pursuit,mutual coherence,overcomplete,redundant,sparse coding,sparse represen- tation,underdetermined},
number = {1},
pages = {34--81},
title = {{From Sparse Solutions of Systems of Equations to Sparse Modeling of Signals and Images}},
volume = {51},
year = {2009}
}
@article{Cybenko1989,
author = {Cybenko, G.},
doi = {10.1007/BF02836480},
file = {:home/tom/Documents/Mendeley/Cybenko_1989_Approximation by Superpositions of a Sigmoidal Function.pdf:pdf},
isbn = {0780300564},
issn = {10009221},
journal = {Mathematics of Control, Signals, and Systems},
keywords = {approximation,completeness,neural networks},
number = {4},
pages = {303--314},
title = {{Approximation by Superpositions of a Sigmoidal Function}},
volume = {2},
year = {1989}
}
@inproceedings{Hu2010,
address = {Hong Kong, China},
author = {Hu, Rui and Barnard, Mark and Collomosse, John},
booktitle = {IEEE International Conference on Image Processing (ICIP)},
file = {:home/tom/Documents/Mendeley/Hu, Barnard, Collomosse_2010_Gradient Field Descriptor for Sketch Based Retrieval and Localization.pdf:pdf},
isbn = {9781424479948},
keywords = {able attention in recent,call for the retrieval,databases by visual example,has received consider-,of imagery based on,qve,the task of querying,visual appearance,with bag-of-visual-words ap-,years,yet many creative applications},
pages = {1025--1028},
title = {{Gradient Field Descriptor for Sketch Based Retrieval and Localization}},
year = {2010}
}
@article{Baldi1989,
author = {Baldi, Pierre and Hornik, Kurt},
doi = {10.1016/0893-6080(89)90014-2},
file = {:home/tom/Documents/Mendeley/Baldi, Hornik_1989_Neural networks and principal component analysis Learning from examples without local minima.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back propagation,Learning,Neural networks,Principal component analysis},
number = {1},
pages = {53--58},
title = {{Neural networks and principal component analysis: Learning from examples without local minima}},
volume = {2},
year = {1989}
}
@inproceedings{xin2016maximal,
author = {Xin, Bo and Wang, Yizhou and Gao, Wen and Wipf, David},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {4340----4348},
title = {{Maximal Sparsity with Deep Networks?}},
year = {2016}
}
@article{Giryes2016a,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.08291v5},
author = {Giryes, Raja and Sapiro, Guillermo and Bronstein, Alex M and Carolina, North},
eprint = {arXiv:1504.08291v5},
file = {:home/tom/Documents/Mendeley/Giryes et al._2016_Deep Neural Networks with Random Gaussian Weights A Universal Classification Strategy.pdf:pdf},
journal = {IEEE Transaction on Signal Processing},
number = {13},
pages = {3444--3457},
title = {{Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?}},
volume = {64},
year = {2016}
}
@article{Aharon2006,
author = {Aharon, Michal and Elad, Michael and Bruckstein, Alfred},
file = {:home/tom/Documents/Mendeley/Aharon, Elad, Bruckstein_2006_K-SVD An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation.pdf:pdf},
journal = {IEEE Transaction on Signal Processing},
number = {11},
pages = {4311--4322},
title = {{K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation}},
volume = {54},
year = {2006}
}
@inproceedings{Cadieu2008,
address = {Vancouver, Canada},
author = {Cadieu, Charles F. and Olshausen, Bruno A.},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Documents/Mendeley//Cadieu, Olshausen_2008_Learning Transformational Invariants from Natural Movies.pdf:pdf},
isbn = {9781605609492},
pages = {292--292},
title = {{Learning Transformational Invariants from Natural Movies}},
year = {2008}
}
@article{Oculo5,
author = {Donin, JF},
journal = {Canadian Journal of Ophthalmology},
number = {2},
pages = {212--215},
title = {{Acquired monocular nystagmus in children}},
volume = {Jul},
year = {1967}
}
@article{Jutten1991,
abstract = {The separation of independent sources from an array of sensors is a classical but difficult problem in signal processing. Based on some biological observations, an adaptive algorithm is proposed to separate simultaneously all the unknown independent sources. The adaptive rule, which constitutes an independence test using non-linear functions, is the main original point of this blind identification procedure. Moreover, a new concept, that of INdependent Components Analysis (INCA), more powerful than the classical Principal Components Analysis (in decision tasks) emerges from this work.},
author = {Jutten, C and Herault, J},
doi = {10.1016/0165-1684(91)90079-X},
file = {:home/tom/Documents/Mendeley/Jutten, Herault_1991_Blind seperation of sources, part I an adaptive algorithm based on neuromimetic architecture.pdf:pdf},
isbn = {0165-1684},
issn = {01651684},
journal = {Signal Processing},
keywords = {Separation of sources,high order moments,independent components,linear recursive adaptive filter.,neural networks,principal components},
number = {1},
pages = {1--10},
title = {{Blind seperation of sources, part I: an adaptive algorithm based on neuromimetic architecture}},
volume = {24},
year = {1991}
}
@article{Rey1991,
abstract = {A new method for nystagmus classification, using system identification techniques, is presented. We formulate a system whose input is head position and whose output is eye position. We approximate this system with an autoregressive with exogenous input (ARX) model which relates the input and output (transfer function) regardless of the temporal profile for the sensory stimulation. The system is then identified using a least squares criteria and three indicators are produced. From these a flag is produced that marks slow and fast phases as well as blinks and bad data segments. Tests with simulated and real data are presented and indicate that the segment classification is remarkably insensitive to recording noise and that it is more robust than previous techniques. Operator intervention is minimal. We expect the method to be applicable for all types of ocular nystagmus. Here, however, we illustrate our results only in the context of the vestibuloocular reflex (VOR). A discussion explains how this method can also be applied for optokinetic (OKN) or pursuit nystagmus.},
author = {Rey, C. G. and Galiana, Henrietta L.},
doi = {10.1109/10.76379},
file = {:home/tom/Documents/Mendeley/Rey, Galiana_1991_Parametric classification of segments in ocular nystagmus.pdf:pdf},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
number = {2},
pages = {142--148},
pmid = {2066123},
title = {{Parametric classification of segments in ocular nystagmus}},
volume = {38},
year = {1991}
}
@book{Hastie2015,
author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin J.},
file = {:home/tom/Documents/Mendeley//Hastie, Tibshirani, Wainwright_2015_Statistical Learning with Sparsity.pdf:pdf},
isbn = {9781498712170},
pages = {1--351},
publisher = {CRC Press},
title = {{Statistical Learning with Sparsity}},
year = {2015}
}
@article{Hiriart-Urruty1991,
author = {Hiriart-Urruty, J. B.},
doi = {10.1016/0022-247X(91)90187-5},
file = {:home/tom/Documents/Mendeley//Hiriart-Urruty_1991_How to regularize a difference of convex functions.pdf:pdf},
issn = {10960813},
journal = {Journal of Mathematical Analysis and Applications},
number = {1},
pages = {196--209},
title = {{How to regularize a difference of convex functions}},
volume = {162},
year = {1991}
}
@inproceedings{Zeiler2010,
address = {San Francisco, CA, USA},
author = {Zeiler, Matthew D. and Krishnan, Dilip and Taylor, Graham W. and Fergus, Rob},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Documents/Mendeley/Zeiler et al._2010_Deconvolutional Networks.pdf:pdf},
isbn = {9781424469857},
pages = {2528----2535},
publisher = {IEEE},
title = {{Deconvolutional Networks}},
year = {2010}
}
@article{Overschee1994,
author = {{Van Overschee}, Peter and {De Moor}, Bart},
file = {:home/tom/Documents/Mendeley/Van Overschee, De Moor_1994_N4SID Subspace Algorithms for the Stochastic Systemst.pdf:pdf},
journal = {Automatica},
keywords = {--system identification,abstract--recently a great deal,decomposition,difference equations,given,kalman filters,multivariable systems,of attention has been,or and singular value,state space methods,subspace state space system,to numerical algorithms for},
number = {1},
pages = {75--93},
title = {{N4SID: Subspace Algorithms for the Stochastic Systemst}},
volume = {30},
year = {1994}
}
@article{Vidaurre2018b,
abstract = {In this paper, we propose a method to track trial-specific neural dynamics of stimulus processing and decision making with high temporal precision. By applying this novel method to a perceptual template-matching task, we tracked representational brain states associated with the cascade of neural processing, from early sensory areas to higher-order areas that are involved in integration and decision-making. We address a major limitation of the traditional decoding approach: that it relies on consistent timing of these processes over trials. Using a temporally unconstrained decoding analysis approach, we found that the timing of the cognitive processes involved in perceptual judgements can vary considerably over trials. This revealed that the sequence of processing states was consistent for all subjects and trials, even when the timing of these states varied. Furthermore, we found that the specific timing of states on each trial was related to the quality of performance over trials.},
author = {Vidaurre, Diego and Myers, Nicholas E. and Stokes, Mark and Nobre, Anna C. and Woolrich, Mark W.},
doi = {10.1101/260943},
file = {:home/tom/Documents/Mendeley/Vidaurre et al._2018_Temporally unconstrained decoding reveals consistent but time-varying stages of stimulus processing.pdf:pdf},
isbn = {0003-021X},
journal = {bioRxiv},
pages = {260943},
title = {{Temporally unconstrained decoding reveals consistent but time-varying stages of stimulus processing}},
url = {https://www.biorxiv.org/content/early/2018/02/06/260943},
year = {2018}
}
@article{Oculo23,
author = {Mossman, SS and Bronstein, Alex M. and Gresty, M. and Kendall, B and Rudge, P},
journal = {Archive Ophthalmology},
number = {47},
pages = {357--359},
title = {{Convergence nystagmus associated with Arnold-Chiari malformation}},
volume = {MAr},
year = {1990}
}
@article{Mallat2016,
abstract = {Deep convolutional networks provide state of the art classifications and regressions results over many high-dimensional problems. We review their architecture, which scatters data with a cascade of linear filter weights and non-linearities. A mathematical framework is introduced to analyze their properties. Computations of invariants involve multiscale contractions, the linearization of hierarchical symmetries, and sparse separations. Applications are discussed.},
archivePrefix = {arXiv},
arxivId = {1601.04920},
author = {Mallat, St\'ephane},
doi = {10.1098/rsta.2015.0203},
eprint = {1601.04920},
file = {:home/tom/Documents/Mendeley/Mallat_2016_Understanding Deep Convolutional Networks.pdf:pdf},
isbn = {1581136625},
issn = {1364503X},
journal = {Philosophical Transaction of the Royal Society A},
number = {2065},
pmid = {26953183},
title = {{Understanding Deep Convolutional Networks}},
url = {http://arxiv.org/abs/1601.04920%0Ahttp://dx.doi.org/10.1098/rsta.2015.0203},
volume = {374},
year = {2016}
}
@article{Cason2011,
author = {Cason, T. P. and Absil, P. A. and {Van Dooren}, P.},
doi = {10.1007/978-94-007-0602-6_4},
file = {:home/tom/Documents/Mendeley/Cason, Absil, Van Dooren_2011_Comparing two matrices by means of isometric projections.pdf:pdf},
isbn = {9789400706019},
issn = {18761100},
journal = {Lecture Notes in Electrical Engineering},
pages = {77--93},
title = {{Comparing two matrices by means of isometric projections}},
volume = {80 LNEE},
year = {2011}
}
@inproceedings{Ge2016,
address = {Barcelona, Spain},
archivePrefix = {arXiv},
arxivId = {1605.07272},
author = {Ge, Rong and Lee, Jason and Ma, Tengyu},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {1605.07272},
file = {:home/tom/Documents/Mendeley/Ge, Lee, Ma_2016_Matrix Completion has No Spurious Local Minimum.pdf:pdf},
pages = {2973--2981},
title = {{Matrix Completion has No Spurious Local Minimum}},
year = {2016}
}
@article{kwapisz2011activity,
author = {Kwapisz, J and Weiss, G and Moore, S},
journal = {ACM SigKDD Explorations Newsletter},
number = {2},
pages = {74--82},
publisher = {ACM},
title = {{Activity recognition using cell phone accelerometers}},
volume = {12},
year = {2011}
}
@inproceedings{Dupre2018,
address = {Montreal, Canada},
author = {{Dupr{\'{e}} la Tour}, Tom and Moreau, Thomas and Jas, Mainak and Gramfort, Alexandre},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {:home/tom/Documents/Mendeley/Dupr{\'{e}} la Tour et al._2018_Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals.pdf:pdf},
pages = {3296--3306},
title = {{Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals}},
year = {2018}
}
@article{Ghil2002,
author = {Ghil, M and Allen, M R and Dettinger, M D and Ide, K and Kondrashov, D and Mann, M E and Robertson, a W and Saunders, A and Tian, Y and Varadi, F and Yiou, P},
doi = {10.1029/2001RG000092},
file = {:home/tom/Documents/Mendeley/Ghil et al._2002_Advanced spectral methods for climate time series.pdf:pdf},
isbn = {8755-1209},
issn = {8755-1209},
journal = {Reviews of Geophysics},
number = {1},
pages = {3.1--3.41},
title = {{Advanced spectral methods for climate time series}},
volume = {40},
year = {2002}
}
@article{Tibshirani1996,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tibshirani, Robert},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/Mendeley//Tibshirani_1996_Regression Shrinkage and Selection via the Lasso.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of the Royal Statistical Society: Series B (statistical methodology)},
keywords = {icle},
number = {1},
pages = {267----288},
pmid = {25246403},
title = {{Regression Shrinkage and Selection via the Lasso}},
volume = {58},
year = {1996}
}
@article{Schwartz-ziv2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.00810v3},
author = {Schwartz-ziv, Ravid and Tishby, Naftali},
eprint = {arXiv:1703.00810v3},
file = {:home/tom/Documents/Mendeley/Schwartz-ziv, Tishby_2017_Opening the black box of Deep Neural Networks via Information.pdf:pdf},
journal = {preprint ArXiv},
title = {{Opening the black box of Deep Neural Networks via Information}},
volume = {1703.00810},
year = {2017}
}
@article{Montavon2018,
author = {Montavon, Gr{\'{e}}goire and Samek, Wojciech and M{\"{u}}ller, Klaus-robert},
doi = {10.1016/j.dsp.2017.10.011},
file = {:home/tom/Documents/Mendeley/Montavon, Samek, M{\"{u}}ller_2018_Methods for interpreting and understanding deep neural networks.pdf:pdf},
issn = {1051-2004},
journal = {Digital Signal Processing},
keywords = {activation maximization,deep neural networks,sensitivity analysis},
pages = {1--15},
publisher = {Elsevier Inc.},
title = {{Methods for interpreting and understanding deep neural networks}},
url = {https://doi.org/10.1016/j.dsp.2017.10.011},
volume = {73},
year = {2018}
}
@inproceedings{Zhang2016,
abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
address = {Toulon, France},
archivePrefix = {arXiv},
arxivId = {1611.03530},
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
booktitle = {International Conference on Learning Representation (ICLR)},
doi = {10.1109/TKDE.2015.2507132},
eprint = {1611.03530},
file = {:home/tom/Documents/Mendeley/Zhang et al._2017_Understanding deep learning requires rethinking generalization.pdf:pdf},
isbn = {1506.02142},
issn = {10414347},
pmid = {88045},
title = {{Understanding deep learning requires rethinking generalization}},
url = {http://arxiv.org/abs/1611.03530},
year = {2017}
}
@article{weinberg2002using,
author = {Weinberg, H},
journal = {Analog Devices AN-602 application note},
title = {{Using the ADXL202 in pedometer and personal navigation applications}},
year = {2002}
}
@article{Bashan2008,
abstract = {We examine several recently suggested methods for the detection of long-range correlations in data series based on similar ideas as the well-established Detrended Fluctuation Analysis (DFA). In particular, we present a detailed comparison between the regular DFA and two recently suggested methods: the Centered Moving Average (CMA) Method and a Modified Detrended Fluctuation Analysis (MDFA). We find that CMA performs the same as DFA in long data with weak trends and is slightly superior to DFA in short data with weak trends. When comparing standard DFA to MDFA we observe that DFA performs slightly better in almost all examples we studied. We also discuss how several types of trends affect different types of DFA. For weak trends in the data, the new methods are comparable with DFA in these respects. However, if the functional form of the trend in data is not a-priori known, DFA remains the method of choice. Only a comparison of DFA results, using different detrending polynomials, yields full recognition of the trends. A comparison with independent methods is recommended for proving long-range correlations. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {0804.4081},
author = {Bashan, Amir and Bartsch, Ronny and Kantelhardt, Jan W. and Havlin, Shlomo},
doi = {10.1016/j.physa.2008.04.023},
eprint = {0804.4081},
file = {:home/tom/Documents/Mendeley/Bashan et al._2008_Comparison of detrending methods for fluctuation analysis.pdf:pdf},
isbn = {0022-1694},
issn = {03784371},
journal = {Physica A: Statistical Mechanics and its Applications},
keywords = {Crossovers,Detrended fluctuation analysis,Long-range correlations,Non-stationarities,Time series analysis},
number = {21},
pages = {5080--5090},
pmid = {22677174},
title = {{Comparison of detrending methods for fluctuation analysis}},
volume = {387},
year = {2008}
}
@inproceedings{Rolfe2013,
abstract = {We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters. From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.},
address = {Scottsdale, AZ, USA},
archivePrefix = {arXiv},
arxivId = {1301.3775},
author = {Rolfe, Jason Tyler and LeCun, Yan},
booktitle = {International Conference on Learning Representation (ICLR)},
eprint = {1301.3775},
file = {:home/tom/Documents/Mendeley//Rolfe, LeCun_2013_Discriminative Recurrent Sparse Auto-Encoders.pdf:pdf},
pages = {15},
title = {{Discriminative Recurrent Sparse Auto-Encoders}},
url = {http://arxiv.org/abs/1301.3775},
year = {2013}
}
@article{Nesterov2010,
author = {Nesterov, Yuri},
file = {:home/tom/Documents/Mendeley/Nesterov_2010_Efficiency of coordinate descent methods on huge-scale optimization problems.pdf:pdf},
journal = {SIAM Journal on Optimization},
keywords = {1 universit{\'{e}} catholique de,and,b-1348 louvain-la-neuve,be,belgium,convex optimization,coordinate relaxation,core and inma,e-mail,fast gradient,google problem,louvain,member of ecore,nesterov,schemes,the association between core,this author is also,uclouvain,worst-case efficiency estimates,yurii},
number = {2},
pages = {341--362},
title = {{Efficiency of coordinate descent methods on huge-scale optimization problems}},
volume = {22},
year = {2010}
}
@article{Janzamin2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.08473v3},
author = {Janzamin, Majid and Sedghi, Hanie and Anandkumar, Anima},
eprint = {arXiv:1506.08473v3},
file = {:home/tom/Documents/Mendeley/Janzamin, Sedghi, Anandkumar_2015_Beating the Perils of Non-Convexity Guaranteed Training of Neural Networks using Tensor Methods.pdf:pdf},
journal = {preprint ArXiv},
keywords = {method-of-moments,neural networks,risk bound,tensor decomposition},
title = {{Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods}},
volume = {150608473},
year = {2015}
}
@article{Hahnloser2000,
author = {Hahnloser, R H and Sarpeshkar, R and Mahowald, M a and Douglas, R J and Seung, H S},
doi = {10.1038/35016072},
file = {:home/tom/Documents/Mendeley/Hahnloser et al._2000_Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit.pdf:pdf},
isbn = {0028-0836 (Print)\r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
number = {6789},
pages = {947--951},
pmid = {10879535},
title = {{Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit.}},
volume = {405},
year = {2000}
}
@inproceedings{morgan1989generalization,
address = {Denver, United States},
author = {Morgan, Nelson and Bourlard, Herv{\'{e}}},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {630--637},
publisher = {International Computer Science Institute},
title = {{Generalization and parameter estimation in feedforward nets: Some experiments}},
year = {1990}
}
@inproceedings{You2016,
address = {Barcelona, Spain},
author = {You, Yang and Lian, Xiangru and Liu, Ji and Yu, Hsiang-Fu and Dhillon, Inderjit S. and Demmel, James and Hsieh, Cho-Jui},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Documents/Mendeley/You et al._2016_Asynchronous Parallel Greedy Coordinate Descent.pdf:pdf},
issn = {10495258},
pages = {4682--4690},
title = {{Asynchronous Parallel Greedy Coordinate Descent}},
year = {2016}
}
@article{Colagrossi2012,
author = {Colagrossi, Andrea and Bouscasse, B. and Antuono, M. and Marrone, S.},
doi = {10.1016/j.cpc.2012.02.032},
file = {:home/tom/Documents/Mendeley/Colagrossi et al._2012_Particle packing algorithm for SPH schemes.pdf:pdf},
isbn = {0010-4655},
issn = {00104655},
journal = {Computer Physics Communications},
keywords = {Lagrangian systems,Meshless methods,Particle initialization,Smoothed particle hydrodynamics},
number = {8},
pages = {1641--1653},
publisher = {Elsevier B.V.},
title = {{Particle packing algorithm for SPH schemes}},
url = {http://dx.doi.org/10.1016/j.cpc.2012.02.032},
volume = {183},
year = {2012}
}
@article{Schmidt2013,
archivePrefix = {arXiv},
arxivId = {1309.2388},
author = {Schmidt, Mark and Roux, Nicolas Le and Bach, Francis},
eprint = {1309.2388},
file = {:home/tom/Documents/Mendeley/Schmidt, Roux, Bach_2013_Minimizing Finite Sums with the Stochastic Average Gradient.pdf:pdf},
journal = {preprint ArXiv},
title = {{Minimizing Finite Sums with the Stochastic Average Gradient}},
url = {http://arxiv.org/abs/1309.2388},
volume = {1309.2388},
year = {2013}
}
@article{Duchi2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
eprint = {arXiv:1103.4296v1},
file = {:home/tom/Documents/Mendeley//Duchi, Hazan, Singer_2011_Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
journal = {Journal of Machine Learning Research (JMLR)},
pages = {2121--2159},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
volume = {12},
year = {2011}
}
@article{Paatero1994,
abstract = {A new variant 'PMF' of factor analysis is described. It is assumed that X is a matrix of observed data and sigma is the known matrix of standard deviations of elements of X. Both X and sigma are of dimensions n x m. The method solves the bilinear matrix problem X = GF + E where G is the unknown left hand factor matrix (scores) of dimensions n x p, F is the unknown right hand factor matrix (loadings) of dimensions p x m, and E is the matrix of residuals. The problem is solved in the weighted least squares sense: G and F are determined so that the Frobenius norm of E divided (element-by-element) by sigma is minimized. Furthermore, the solution is constrained so that all the elements of G and F are required to be non-negative. It is shown that the solutions by PMF are usually different from any solutions produced by the customary factor analysis (FA, i.e. principal component analysis (PCA) followed by rotations). Usually PMF produces a better fit to the data than FA. Also, the result of PF is guaranteed to be non-negative, while the result of FA often cannot be rotated so that all negative entries would be eliminated. Different possible application areas of the new method are briefly discussed. In environmental data, the error estimates of data can be widely varying and non-negativity is often an essential feature of the underlying models. Thus it is concluded that PMF is better suited than FA or PCA in many environmental applications. Examples of successful applications of PMF are shown in companion papers.},
author = {Paatero, Pentti and Tapper, Unto},
doi = {10.1002/env.3170050203},
file = {:home/tom/Documents/Mendeley/Paatero, Tapper_1994_Positive matrix factorization A non-negative factor model with optimal utilization of error estimates of data value.pdf:pdf},
isbn = {1180-4009},
issn = {1099095X},
journal = {Environmetrics},
keywords = {Alternating regression,Error estimates,Factor analysis,Principal component analysis,Repetitive measurements,Scaling,Weighted least squares},
number = {2},
pages = {111--126},
pmid = {39},
title = {{Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values}},
volume = {5},
year = {1994}
}
@article{kadri2015operator,
author = {Kadri, Hachem and DUFLOS, Emmanuel and Preux, Philippe and Stephane canu and Rakotomamonjy, Alain and Audiffren, Julien},
journal = {Journal of Machine Learning Research (JMLR)},
title = {{Operator-valued Kernels for Learning from Functional Response Data}},
year = {2015}
}
@article{montavon2011kernel,
author = {Montavon, Gr{\'{e}}goire and Braun, Mikio L and M{\"{u}}ller, Klaus-Robert},
file = {:home/tom/Documents/Mendeley//Montavon, Braun, M{\"{u}}ller_2011_Kernel Analysis of Deep Networks.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {deep networks,kernel principal component analysis,representations},
number = {Sep},
pages = {2563--2581},
title = {{Kernel analysis of deep networks}},
volume = {12},
year = {2011}
}
@inproceedings{Cuturi2017,
address = {Sidney, Australia},
archivePrefix = {arXiv},
arxivId = {1703.01541},
author = {Cuturi, Marco and Blondel, Mathieu},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1703.01541},
file = {:home/tom/Documents/Mendeley/Cuturi, Blondel_2017_Soft-DTW a Differentiable Loss Function for Time-Series.pdf:pdf},
pages = {894--903},
title = {{Soft-DTW: a Differentiable Loss Function for Time-Series}},
url = {http://arxiv.org/abs/1703.01541},
year = {2017}
}
@article{Tseng2009,
abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tseng, Paul and Yun, Sangwoon},
doi = {10.1007/s10957-008-9458-3},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/Mendeley/Tseng, Yun_2009_Block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization.pdf:pdf},
isbn = {9780874216561},
issn = {00223239},
journal = {Journal of Optimization Theory and Applications},
keywords = {Bilevel optimization,Complexity bound,Coordinate gradient descent,Global convergence,Linear constraints,Linear convergence rate,Nonsmooth optimization,Support vector machines,ℓ1-regularization},
number = {3},
pages = {513--535},
pmid = {15991970},
title = {{Block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization}},
volume = {140},
year = {2009}
}
@article{Oculo1,
author = {Arnoldi, KA and Tychsen, L.},
journal = {Journal of Pediatric Ophthalmology and Strabismus},
number = {32},
pages = {296--301},
title = {{Prevalence of intracranial lesions in children initially diagnosed with disconjugate nystagmus (spasmus nutans).}},
volume = {Sep-Oct},
year = {1995}
}
@inproceedings{Bagnall2006,
address = {Philadelphia, United States},
author = {Bagnall, Anthony and Ratanamahatana, Chotirat “Ann” and Keogh, Eamonn and Lonardi, Stefano and Janacek, Gareth},
booktitle = {SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1007/s10618-005-0028-0},
file = {:home/tom/Documents/Mendeley/Bagnall et al._2006_A Bit Level Representation for Time Series Data Mining with Shape Based Similarity.pdf:pdf},
isbn = {0000000000000},
issn = {1384-5810},
number = {1},
pages = {11--40},
publisher = {ACM},
title = {{A Bit Level Representation for Time Series Data Mining with Shape Based Similarity}},
url = {http://link.springer.com/10.1007/s10618-005-0028-0},
volume = {13},
year = {2006}
}
@article{ben2015comparison,
author = {{Ben Mansour}, K and Rezzoug, N and Gorce, P},
journal = {Computer methods in biomechanics and biomedical engineering},
pages = {1--2},
publisher = {Taylor \& Francis},
title = {{Comparison between several locations of gyroscope for gait events detection}},
year = {2015}
}
@inproceedings{Oquab2014,
address = {Colombus, OH, USA},
author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2014.222},
file = {:home/tom/Documents/Mendeley/Oquab et al._2014_Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
pages = {1717--1724},
title = {{Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks}},
year = {2014}
}
@article{Oculo33,
author = {Weissman, BM and Dell'Osso, LF and Abel, LA and Leigh, RJ},
journal = {Archive of Ophthalmology},
number = {105},
pages = {525--8},
title = {{Spasmus nutans. A quantitative prospective study}},
volume = {Apr},
year = {1987}
}
@article{Dasgupta2003,
author = {Dasgupta, Sanjoy and Gupta, Anupam},
doi = {10.1002/rsa.10073},
file = {:home/tom/Documents/Mendeley/Dasgupta, Gupta_2003_An Elementary Proof of a Theorem of Johnson and Lindenstrauss.pdf:pdf},
isbn = {1098-2418},
issn = {10429832},
journal = {Random Structures and Algorithms},
number = {1},
pages = {60--65},
title = {{An Elementary Proof of a Theorem of Johnson and Lindenstrauss}},
volume = {22},
year = {2003}
}
@article{yang2015randomized,
author = {Yang, Yun and Pilanci, Mert and Wainwright, Martin J.},
journal = {preprint ArXiv},
title = {{Randomized sketches for kernels: Fast and optimal non-parametric regression}},
volume = {1501.06195},
year = {2015}
}
@article{bengio2007scaling,
author = {Bengio, Yoshua and LeCun, Yann},
journal = {Large-scale kernel machines},
number = {5},
pages = {1--41},
title = {{Scaling learning algorithms towards AI}},
volume = {34},
year = {2007}
}
@inproceedings{Pham2013,
address = {Chicago, United States},
author = {Pham, Ninh and Pagh, Rasmus},
booktitle = {SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/2487575.2487591},
file = {:home/tom/Documents/Mendeley/Pham, Pagh_2013_Fast and scalable polynomial kernels via explicit feature maps.pdf:pdf},
isbn = {9781450321747},
issn = {9781450321747},
keywords = {count sketch,fft,polynomial kernel,svm,tensor product},
pages = {239},
publisher = {ACM},
title = {{Fast and scalable polynomial kernels via explicit feature maps}},
url = {http://dl.acm.org/citation.cfm?doid=2487575.2487591},
year = {2013}
}
@inproceedings{Moreau2015a,
author = {Moreau, Thomas and Oudre, Laurent and Vayatis, Nicolas},
booktitle = {Groupe de Recherche et d'Etudes en Traitement du Signal et des Images (GRETSI)},
title = {{Groupement automatique pour l ' analyse du spectre singulier}},
year = {2015}
}
@article{Vishwanathan2007,
author = {Vishwanathan, S. V. N. and Smola, Alexander J. and Vidal, Ren{\'{e}}},
file = {:home/tom/Documents/Mendeley/Vishwanathan, Smola, Vidal_2007_Binet-Cauchy Kernels on Dynamical Systems and its Application to the Analysis of Dynamic Scenes.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {arma models and dynamical,binet-cauchy theorem,sylvester,systems},
number = {1},
pages = {95--119},
title = {{Binet-Cauchy Kernels on Dynamical Systems and its Application to the Analysis of Dynamic Scenes}},
volume = {73},
year = {2007}
}
@article{Kane2010,
abstract = {We give two different and simple constructions for dimensionality reduction in $\ell_2$ via linear mappings that are sparse: only an $O(\varepsilon)$-fraction of entries in each column of our embedding matrices are non-zero to achieve distortion $1+\varepsilon$ with high probability, while still achieving the asymptotically optimal number of rows. These are the first constructions to provide subconstant sparsity for all values of parameters, improving upon previous works of Achlioptas (JCSS 2003) and Dasgupta, Kumar, and Sarl\'{o}s (STOC 2010). Such distributions can be used to speed up applications where $\ell_2$ dimensionality reduction is used.},
archivePrefix = {arXiv},
arxivId = {1012.1577},
author = {Kane, Daniel M. and Nelson, Jelani},
eprint = {1012.1577},
file = {:home/tom/Documents/Mendeley/Kane, Nelson_2014_Sparser Johnson-Lindenstrauss Transforms.pdf:pdf},
journal = {Journal of the Association for Computing Machinery (JACM)},
number = {1},
pages = {1--23},
title = {{Sparser Johnson-Lindenstrauss Transforms}},
url = {http://arxiv.org/abs/1012.1577},
volume = {61},
year = {2014}
}
@article{Frossard2011,
author = {Frossard, Pascal and Tosic, Ivana},
file = {:home/tom/Documents/Mendeley/Frossard, Tosic_2011_Dictionary Learning.pdf:pdf},
journal = {IEEE Signal Processing Magazine},
keywords = {dictionary learning,sparse coding},
number = {2},
pages = {27----38},
title = {{Dictionary Learning}},
volume = {28},
year = {2011}
}
@article{Gribonval2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.3790v3},
author = {Gribonval, R{\'{e}}mi and Jenatton, Rodolphe and Bach, Francis and Kleinsteuber, Martin and Seibert, Matthias},
doi = {10.1109/TIT.2015.2424238},
eprint = {arXiv:1312.3790v3},
file = {:home/tom/Documents/Mendeley/Gribonval et al._2015_Sample complexity of dictionary learning and other matrix factorizations.pdf:pdf},
isbn = {2011277906},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Dictionary learning,K-means clustering,non-negative matrix factorization,principal component analysis,sample complexity,sparse coding,structured learning},
number = {6},
pages = {3469--3486},
title = {{Sample complexity of dictionary learning and other matrix factorizations}},
volume = {61},
year = {2015}
}
@article{Oculo9,
author = {Gamlin, P and Mitchell, K},
journal = {Society of Neuroscience Abstr.},
pages = {346},
title = {{Reversible lesions of nucleus reticularis tegmenti pontis affect convergence and ocular accommodation}},
year = {1993}
}
@inproceedings{Mackey2008,
abstract = {In analogy to the PCA setting, the sparse PCA problem is often solved by iter- atively alternating between two subtasks: cardinality-constrained rank-one vari- ance maximization and matrix deflation. While the former has received a great deal of attention in the literature, the latter is seldom analyzed and is typically borrowed without justification from the PCA context. In this work, we demon- strate that the standard PCA deflation procedure is seldom appropriate for the sparse PCA setting. To rectify the situation, we first develop several deflation al- ternatives better suited to the cardinality-constrained context. We then reformulate the sparse PCAoptimization problemto explicitly reflect themaximumadditional variance objective on each round. The result is a generalized deflation procedure that typically outperforms more standard techniques on real-world datasets.},
address = {Vancouver, Canada},
annote = {* Regular deflation does not preserve orthogonality
* Propose new variants that make sure the orthogonality is preserved (schur complement and othonormal projections)},
author = {Mackey, Lester},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/tom/Documents/Mendeley/Mackey_2008_Deflation Methods for Sparse PCA.pdf:pdf},
isbn = {9781605609492},
pages = {1017--1024},
title = {{Deflation Methods for Sparse PCA.}},
url = {https://papers.nips.cc/paper/3575-deflation-methods-for-sparse-pca.pdf},
year = {2008}
}
@article{tsanas2010accurate,
author = {Tsanas, Athanasios and Little, Max A and McSharry, Patrick E and Ramig, Lorraine O},
journal = {IEEE Transactions on Biomedical Engineering},
number = {4},
pages = {884--893},
publisher = {IEEE},
title = {{Accurate telemonitoring of Parkinson's disease progression by noninvasive speech tests}},
volume = {57},
year = {2010}
}
@article{Zheng2018,
archivePrefix = {arXiv},
arxivId = {1807.05411},
author = {Zheng, Peng and Askham, Travis and Brunton, Steven L. and Kutz, J. Nathan and Aravkin, Aleksandr Y.},
eprint = {1807.05411},
file = {:home/tom/Documents/Mendeley/Zheng et al._2018_Sparse Relaxed Regularized Regression SR3.pdf:pdf},
journal = {preprint ArXiv},
title = {{Sparse Relaxed Regularized Regression: SR3}},
url = {http://arxiv.org/abs/1807.05411},
volume = {1807.05411},
year = {2018}
}
@book{Hertle2013,
author = {Hertle, Richard W and Dell'Osso, Louis F},
publisher = {Oxford University Press},
title = {{Nystagmus in infancy and childhood: current concepts in mechanisms, diagnoses, and management}},
year = {2013}
}
@article{Zhang2012,
abstract = {Given a sample covariance matrix, we examine the problem of maxi- mizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse principal component analysis and has a wide array of applications in ma- chine learning and engineering. Unfortunately, this problem is also combinatorially hard and we discuss convex relaxation techniques that efficiently produce good ap- proximate solutions. We then describe several algorithms solving these relaxations as well as greedy algorithms that iteratively improve the solution quality. Finally, we illustrate sparse PCA in several applications, ranging from senate voting and finance to news data.},
archivePrefix = {arXiv},
arxivId = {1011.3781},
author = {Zhang, Youwei and D'Aspremont, Alexandre and {El Ghaoui}, Laurent},
doi = {10.1007/978-1-4614-0769-0_31},
eprint = {1011.3781},
file = {:home/tom/Documents/Mendeley/Zhang, d'Aspremont, El Ghaoui_2012_Sparse PCA Convex relaxations, algorithms and applications.pdf:pdf},
isbn = {978-1-4614-0769-0},
issn = {08848289},
journal = {International Series in Operations Research and Management Science},
pages = {915--940},
title = {{Sparse PCA: Convex relaxations, algorithms and applications}},
volume = {166},
year = {2012}
}
@article{Lin2015,
abstract = {The alternating direction method of multipliers (ADMM) is widely used in solving structured convex optimization problems. Despite of its success in practice, the convergence properties of the standard ADMM for minimizing the sum of $N$ $(N\geq 3)$ convex functions with $N$ block variables linked by linear constraints, have remained unclear for a very long time. In this paper, we present convergence and convergence rate results for the standard ADMM applied to solve $N$-block $(N\geq 3)$ convex minimization problem, under the condition that one of these functions is convex (not necessarily strongly convex) and the other $N-1$ functions are strongly convex. Specifically, in that case the ADMM is proven to converge with rate $O(1/t)$ in a certain ergodic sense, and $o(1/t)$ in non-ergodic sense, where $t$ denotes the number of iterations.},
archivePrefix = {arXiv},
arxivId = {1408.4265},
author = {Lin, Tian Yi and Ma, Shi Qian and Zhang, Shu Zhong},
doi = {10.1007/s40305-015-0092-0},
eprint = {1408.4265},
file = {:home/tom/Documents/Mendeley/Lin, Ma, Zhang_2015_On the Sublinear Convergence Rate of Multi-block ADMM.pdf:pdf},
issn = {21946698},
journal = {Journal of the Operations Research Society of China},
keywords = {Alternating direction method of multipliers,Convex optimization,Sublinear convergence rate},
number = {3},
pages = {251--274},
title = {{On the Sublinear Convergence Rate of Multi-block ADMM}},
volume = {3},
year = {2015}
}
@article{Beck2009,
annote = {FISTA is a proximal algorithm (ISTA) with a nesterov momentum.
This paper give convergence rate for divers algorithms.},
author = {Beck, Amir and Teboulle, Marc},
file = {:home/tom/Documents/Mendeley//Beck, Teboulle_2009_A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems.pdf:pdf},
journal = {SIAM Journal on Imaging Sciences},
number = {1},
pages = {183--202},
pmid = {18005159},
title = {{A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems}},
volume = {2},
year = {2009}
}
@article{Anden2014,
archivePrefix = {arXiv},
arxivId = {1304.6763},
author = {And\'en, Joakim and Mallat, St\'ephane},
doi = {10.1109/TSP.2014.2326991},
eprint = {1304.6763},
file = {:home/tom/Documents/Mendeley/And'en, Mallat_2014_Deep Scattering Spectrum.pdf:pdf},
journal = {IEEE Transaction on Signal Processing},
number = {16},
pages = {4114--4128},
title = {{Deep Scattering Spectrum}},
url = {http://arxiv.org/abs/1304.6763%0Ahttp://dx.doi.org/10.1109/TSP.2014.2326991},
volume = {62},
year = {2014}
}
@article{Harris2006,
author = {Harris, Chris and Berry, David},
doi = {10.1080/08820530600613746},
file = {:home/tom/Documents/Mendeley/Harris, Berry_2006_A developmental model of infantile nystagmus.pdf:pdf},
isbn = {0882-0538},
issn = {08820538},
journal = {Seminars in Ophthalmology},
keywords = {Congenital nystagmus,Developmental plasticity,Infant eye movements,Optimal control,Oscular oscillations,Visual development},
number = {2},
pages = {63--69},
pmid = {16702071},
title = {{A developmental model of infantile nystagmus}},
volume = {21},
year = {2006}
}
@article{Hillar2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1106.3616v3},
author = {Hillar, Christopher and Sommer, Ft},
eprint = {arXiv:1106.3616v3},
file = {:home/tom/Documents/Mendeley/Hillar, Sommer_2011_When can dictionary learning uniquely recover sparse data from subsamples.pdf:pdf},
journal = {preprint ArXiv},
title = {{When can dictionary learning uniquely recover sparse data from subsamples?}},
url = {http://arxiv.org/abs/1106.3616},
volume = {1106.3616},
year = {2011}
}
@inproceedings{Lu2013,
address = {Portland, OR, USA},
author = {Lu, Cewu and Shi, Jiaping and Jia, Jiaya},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2013.60},
isbn = {978-0-7695-4989-7},
issn = {10636919},
keywords = {Dictionary Learning,Online Learning,Robust Statistics},
pages = {415--422},
title = {{Online robust dictionary learning}},
year = {2013}
}
@article{Hebiri2013,
archivePrefix = {arXiv},
arxivId = {1204.1605},
author = {Hebiri, Mohamed and Lederer, Johannes},
eprint = {1204.1605},
file = {:home/tom/Documents/Mendeley//Hebiri, Lederer_2013_How Correlations Influence Lasso Prediction.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {3},
pages = {1846--1854},
title = {{How Correlations Influence Lasso Prediction}},
volume = {59},
year = {2013}
}
@misc{Sifre2014,
abstract = {Image classification is the problem of assigning a label that best describes the content of unknown images, given a set of training images with known labels. This thesis introduces image classification algorithms based on the scattering transform, studies their properties and describes extensive classification experiments on challenging texture and object image datasets. Images are high dimensional signals for which generic machine learning algorithms fail when applied directly on the raw pixel space. Therefore, most successful approaches involve building a specific low dimensional representation on which the classification is performed. Traditionally, the representation was engineered to reduce the dimensionality of images by building invariance to geometric transformations while retaining discriminative features. More recently, deep convolutional networks have achieved state-of-the-art results on most image classification tasks. Such networks progressively build more invariant representations through a hierarchy of convolutional layers where all the weights are learned. This thesis proposes several scattering representations. Those scattering representa-tions have a structure similar to convolutional networks, but the weights of scattering are designed to provide mathematical guaranty of invariance to geometric transformations, sta-bility to deformations and energy preservation. In this thesis, we focus on affine and more specifically on rigid-motion transformations, which consist in translations and rotations, and which are common in real world images. Translation scattering is a cascade of two dimensional wavelet modulus operators which builds translation invariance. We propose a first separable rigid-motion separable scatter-ing, which applies a first scattering along the position variable to build translation in-variance, followed by a second scattering transform along the rotational orbits of the first scattering, to build invariance to rotations. As any separable representation, separable scattering has the advantage of simplicity but also loses some information about the joint distribution of positions and orientations in the intermediate layers of the representation. We define a joint rigid-motion scattering which does retain this information. The joint scattering consists in a cascade of wavelet modulus applied directly on the joint rigid-motion group. We introduce convolutions, wavelets, a wavelet transform and scattering on the rigid-motion group and propose fast implementations. Both separable and joint scattering are applied to texture image classi-fication with state-of-the-art results on most available texture datasets. Finally, we demonstrate the applicability of joint scattering and group convolutions on generic object image datasets. It is shown that convolutional networks performances are enhanced through the use of separable convolutions, similar to the rigid-motion con-volutions. Also, a non-invariant version of the rigid-motion scattering is demonstrated to achieve results similar to those obtained by the first layers of convolutional networks.},
author = {Sifre, Laurent},
file = {:home/tom/Documents/Mendeley/Sifre_2014_Rigid-Motion Scattering For Image Classification.pdf:pdf},
howpublished = {PhD Thesis},
pages = {128},
title = {{Rigid-Motion Scattering For Image Classification}},
year = {2014}
}
@inproceedings{faloutsos1994fast,
address = {Minneapolis, MN, USA},
author = {Faloutsos, C. and Ranganathan, M and Manolopoulos, Y},
booktitle = {ACM SIGMOD International Conference on Management of Data},
number = {2},
pages = {419--429},
title = {{Fast subsequence matching in time-series databases}},
volume = {23},
year = {1994}
}
@inproceedings{Alexandrov2005,
address = {St. Petersburg, Russia},
author = {Alexandrov, Theodore and Golyandina, Nina},
booktitle = {Workshop on Simulation},
file = {:home/tom/Documents/Mendeley/Alexandrov, Golyandina_2005_Automatic extraction and forecast of time series cyclic components within the framework of SSA.pdf:pdf},
pages = {45--50},
title = {{Automatic extraction and forecast of time series cyclic components within the framework of SSA}},
url = {http://www.math.uni-bremen.de/$\sim$theodore/mywiki/uploads/Main/AlexandrovGolyandina2005AutoSSA_cycles_WorkshopOnSimulation05.pdf},
year = {2005}
}
@article{naqvi2012step,
author = {Naqvi, N Z and Kumar, A and Chauhan, A and Sahni, K},
journal = {International Journal on Computer Science and Engineering (IJCSE)},
number = {5},
pages = {675--682},
title = {{Step Counting Using Smartphone-Based Accelerometer}},
volume = {4},
year = {2012}
}
@inproceedings{Lee2001,
abstract = {Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data. Two different multi- plicative algorithms for NMF are analyzed. They differ only slightly in the multiplicative factor used in the update rules. One algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized Kullback-Leibler divergence. The monotonic convergence of both algorithms can be proven using an auxiliary func- tion analogous to that used for proving convergence of the Expectation- Maximization algorithm. The algorithms can also be interpreted as diag- onally rescaled gradient descent, where the rescaling factor is optimally chosen to ensure convergence. Introduction},
address = {Vancouver, Canada},
author = {Lee, Dd and Seung, Hs},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1109/IJCNN.2008.4634046},
file = {:home/tom/Documents/Mendeley//Lee, Seung_2001_Algorithms for non-negative matrix factorization.pdf:pdf},
isbn = {9781424418206},
issn = {10987576},
pages = {556--562},
title = {{Algorithms for non-negative matrix factorization}},
url = {http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization},
year = {2001}
}
@book{Tseng2009a,
abstract = {We consider the problem of minimizing the sum of a smooth function and a separable convex function. This problem includes as special cases bound-constrained optimization and smooth optimization with ℓ1-regularization. We propose a (block) coordinate gradient descent method for solving this class of nonsmooth separable problems. We establish global convergence and, under a local Lipschitzian error bound assumption, linear convergence for this method. The local Lipschitzian error bound holds under assumptions analogous to those for constrained smooth optimization, e.g., the convex function is polyhedral and the smooth function is (nonconvex) quadratic or is the composition of a strongly convex function with a linear mapping. We report numerical experience with solving the ℓ1-regularization of unconstrained optimization problems from Mor{\'{e}} et al. in ACM Trans. Math. Softw. 7, 17–41, 1981 and from the CUTEr set (Gould and Orban in ACM Trans. Math. Softw. 29, 373–394, 2003). Comparison with L-BFGS-B and MINOS, applied to a reformulation of the ℓ1-regularized problem as a bound-constrained optimization problem, is also reported.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tseng, Paul and Yun, Sangwoon},
booktitle = {Mathematical Programming},
doi = {10.1007/s10107-007-0170-0},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/Mendeley/Tseng, Yun_2009_A coordinate gradient descent method for nonsmooth separable minimization.pdf:pdf},
isbn = {1010700701700},
issn = {00255610},
keywords = {Coordinate descent,Error bound,Global convergence,Linear convergence rate,Nonsmooth optimization},
number = {1-2},
pages = {387--423},
pmid = {15003161},
title = {{A coordinate gradient descent method for nonsmooth separable minimization}},
volume = {117},
year = {2009}
}
@article{Deng2015,
archivePrefix = {arXiv},
arxivId = {1208.3922},
author = {Deng, Wei and Yin, Wotao},
doi = {10.1007/s10915-015-0048-x},
eprint = {1208.3922},
file = {:home/tom/Documents/Mendeley/Deng, Yin_2015_On the Global and Linear Convergence of the Generalized Alternating Direction Method of Multipliers.pdf:pdf},
issn = {1573-7691},
journal = {Journal of Scientific Computing},
keywords = {alternating directions of multipliers,dual ascent,error bound,linear convergence,subject classifications},
number = {3},
pages = {889--916},
publisher = {Springer US},
title = {{On the Global and Linear Convergence of the Generalized Alternating Direction Method of Multipliers}},
url = {http://arxiv.org/abs/1208.3922},
volume = {77005},
year = {2015}
}
@article{Jain2000,
author = {Jain, A.K. and Duin, R. P W and Mao, Jianchang},
doi = {10.1109/34.824819},
file = {:home/tom/Documents/Mendeley/Jain, Duin, Mao_2000_Statistical pattern recognition a review.pdf:pdf},
isbn = {0162-8828 VO - 22},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {1},
pages = {4--37},
pmid = {17542025},
title = {{Statistical pattern recognition: a review}},
url = {http://ieeexplore.ieee.org/ielx5/34/17859/00824819.pdf?tp=&arnumber=824819&isnumber=17859%5Cnhttp://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824819},
volume = {22},
year = {2000}
}
@article{Gabay1976,
archivePrefix = {arXiv},
arxivId = {1104.0262},
author = {Gabay, Daniel and Mercier, Bertrand},
doi = {10.1016/0898-1221(76)90003-1},
eprint = {1104.0262},
file = {:home/tom/Documents/Mendeley/Gabay, Mercier_1976_A dual algorithm for the solution of nonlinear variational problems via finite element approximation.pdf:pdf},
isbn = {0278-0062 VO - 30},
issn = {08981221},
journal = {Computers \& Mathematics with Applications},
number = {1},
pages = {17--40},
pmid = {270766200012},
title = {{A dual algorithm for the solution of nonlinear variational problems via finite element approximation}},
url = {http://www.sciencedirect.com/science/article/pii/0898122176900031},
volume = {2},
year = {1976}
}
@article{Luo2016,
archivePrefix = {arXiv},
arxivId = {1602.00223},
author = {Luo, Luo and Chen, Zihao and Zhang, Zhihua and Li, Wu-Jun},
eprint = {1602.00223},
file = {:home/tom/Documents/Mendeley//Luo et al._2016_Variance-Reduced Second-Order Methods.pdf:pdf},
journal = {preprint ArXiv},
title = {{Variance-Reduced Second-Order Methods}},
volume = {1602.00223},
year = {2016}
}
@inproceedings{Shah2015,
address = {Montreal, Canada},
author = {Shah, Parikshit and Rao, Nikhil and Tang, Gongguo},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {2548--2556},
title = {{Sparse and Low-Rank Tensor Decomposition}},
year = {2015}
}
@article{pan1985real,
author = {Pan, J and Tompkins, W J},
journal = {IEEE Transactions on Biomedical Engineering},
number = {3},
pages = {230--236},
publisher = {IEEE},
title = {{A real-time {QRS} detection algorithm}},
volume = {32},
year = {1985}
}
@inproceedings{Yang2010,
address = {San Francisco, CA, USA},
author = {Yang, Jianchao and Yu, Kai and Huang, Thomas},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Documents/Mendeley/Yang, Yu, Huang_2010_Supervised Translation-Invariant Sparse Coding.pdf:pdf},
isbn = {9781424469857},
keywords = {Convolution,Neural net,dictionary learning,sparse coding},
pages = {3517--3524},
publisher = {IEEE},
title = {{Supervised Translation-Invariant Sparse Coding}},
year = {2010}
}
@inproceedings{Haeffele2017b,
address = {Quebec, Canada},
archivePrefix = {arXiv},
arxivId = {1706.01912},
author = {Haeffele, Benjamin D and Stahl, Richard and Vanmeerbeeck, Geert and Vidal, Ren{\'{e}}},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
doi = {10.1007/978-3-319-66185-8},
eprint = {1706.01912},
file = {:home/tom/Documents/Mendeley/Haeffele et al._2017_Efficient Reconstruction of Holographic Lens-Free Images by Sparse Phase Recovery.pdf:pdf},
isbn = {978-3-319-66184-1},
keywords = {holography,lens-free imaging,sparsity},
pages = {109--117},
title = {{Efficient Reconstruction of Holographic Lens-Free Images by Sparse Phase Recovery}},
url = {http://link.springer.com/10.1007/978-3-319-66185-8},
year = {2017}
}
@article{Oculo8,
author = {Galvez-Ruiz, A and Roig, C and Mu{\~{n}}oz, S and Arruga, J},
journal = {Neuro-Opthalmology},
pages = {276--279},
title = {{Convergent-divergent nystagmus as a manifestation of oculopalatal tremor.}},
volume = {35},
year = {2011}
}
@inproceedings{dahl2013improving,
address = {Vancouver, Canada},
author = {Dahl, George E and Sainath, Tara N and Hinton, Geoffrey E},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
organization = {IEEE},
pages = {8609--8613},
title = {{Improving deep neural networks for LVCSR using rectified linear units and dropout}},
year = {2013}
}
@article{Rockafellar1976,
abstract = {For the problem of minimizing a lower semicontinuous proper convex function f on a Hilbert space, the proximal point algorithm in exact form generates a sequence $\{ z^k \} $ by taking $z^{k + 1} $ to be the minimizes of $f(z) + ({1 / {2c_k }})\| {z - z^k } \|^2 $, where $c_k > 0$. This algorithm is of interest for several reasons, but especially because of its role in certain computational methods based on duality, such as the Hestenes-Powell method of multipliers in nonlinear programming. It is investigated here in a more general form where the requirement for exact minimization at each iteration is weakened, and the subdifferential $\partial f$ is replaced by an arbitrary maximal monotone operator T. Convergence is established under several criteria amenable to implementation. The rate of convergence is shown to be “typically” linear with an arbitrarily good modulus if $c_k $ stays large enough, in fact superlinear if $c_k \to \infty $. The case of $T = \partial f$ is treated in extra detail. Applicati...},
author = {Rockafellar, R. Tyrrell},
doi = {10.1137/0314056},
file = {:home/tom/Documents/Mendeley/Rockafellar_1976_Monotone Operators and the Proximal Point Algorithm.pdf:pdf},
isbn = {0233193960},
issn = {0363-0129},
journal = {SIAM Journal on Control and Optimization},
number = {5},
pages = {877--898},
title = {{Monotone Operators and the Proximal Point Algorithm}},
volume = {14},
year = {1976}
}
@article{Kurtek2012,
author = {Kurtek, Sebastian and Wu, Wei and Srivastava, Anuj},
doi = {10.1080/02664763.YYYY.XXXXXX},
file = {:home/tom/Documents/Mendeley/Kurtek, Wu, Srivastava_2013_Segmentation and Statistical Analysis of Biosignals with Application to Disease Classification.pdf:pdf},
journal = {Journal of Applied Statistics},
keywords = {classification,electrocardiogram,gait,infarction,myocardial,signal segmentation,statistical functional analysis},
number = {6},
pages = {1270----1288},
title = {{Segmentation and Statistical Analysis of Biosignals with Application to Disease Classification}},
volume = {40},
year = {2013}
}
@article{Friedman1937,
author = {Friedman, Milton},
doi = {10.2307/2279372},
file = {:home/tom/Documents/Mendeley/Friedman_1937_The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of Variance.pdf:pdf},
isbn = {01621459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
number = {200},
pages = {675--701},
pmid = {5377},
title = {{The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of Variance}},
url = {http://www.jstor.org/stable/2279372?origin=crossref},
volume = {32},
year = {1937}
}
@techreport{Author2016,
address = {Barcelona, Spain},
archivePrefix = {arXiv},
arxivId = {1608.04062},
author = {Author, Anonymous and Address, Affiliation},
eprint = {1608.04062},
file = {:home/tom/Documents/Mendeley/Author, Address_2016_Stacked Approximated Regression Machine A Simple Deep Learning Approach.pdf:pdf},
title = {{Stacked Approximated Regression Machine : A Simple Deep Learning Approach}},
year = {2016}
}
@inproceedings{Afsari2012,
address = {Providence, RI, USA},
author = {Afsari, B and Chaudhry, R and Ravichandran, A and Vidal, Ren{\'{e}}},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2012.6247929},
file = {:home/tom/Documents/Mendeley/Afsari et al._2012_Group action induced distances for averaging and clustering Linear Dynamical Systems with applications to the analysi.pdf:pdf},
isbn = {978-1-4673-1228-8},
issn = {1063-6919},
keywords = {LDS averaging algorithm,LDS statespace realization},
pages = {2208--2215},
title = {{Group action induced distances for averaging and clustering Linear Dynamical Systems with applications to the analysis of dynamic scenes}},
year = {2012}
}
@misc{Grisel2016,
author = {Moreau, Thomas and Grisel, Olivier},
howpublished = {https://github.com/tomMoral/loky},
title = {{Loky}},
url = {https://github.com/tomMoral/loky}
}
@inproceedings{Yang2017,
abstract = {Compressive Sensing (CS) is an effective approach for fast Magnetic Resonance Imaging (MRI). It aims at reconstructing MR image from a small number of under-sampled data in k-space, and accelerating the data acquisition in MRI. To improve the current MRI system in reconstruction accuracy and computational speed, in this paper, we propose a novel deep architecture, dubbed ADMM-Net. ADMM-Net is defined over a data flow graph, which is derived from the iterative pro-cedures in Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing a CS-based MRI model. In the training phase, all parameters of the net, e.g., image transforms, shrinkage functions, etc., are discriminatively trained end-to-end using L-BFGS algorithm. In the testing phase, it has computational overhead similar to ADMM but uses optimized parameters learned from the train-ing data for CS-based reconstruction task. Experiments on MRI image reconstruc-tion under different sampling ratios in k-space demonstrate that it significantly improves the baseline ADMM algorithm and achieves high reconstruction accura-cies with fast computational speed.},
archivePrefix = {arXiv},
arxivId = {1705.06869},
author = {Yang, Yan and Sun, Jian and Li, Huibin and Xu, Zongben},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {10.1145/2966986.2980084},
eprint = {1705.06869},
file = {:home/tom/Documents/Mendeley/Yang et al._2017_Deep ADMM-Net for Compressive Sensing MRI.pdf:pdf},
isbn = {9781450344661},
issn = {10495258},
pages = {10--18},
title = {{Deep ADMM-Net for Compressive Sensing MRI.}},
year = {2017}
}
@inproceedings{Steeg,
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.1222v1},
author = {Steeg, Greg Ver and Rey, Marina},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {arXiv:1406.1222v1},
file = {:home/tom/Documents/Mendeley/Steeg, Rey_2014_Discovering Structure in High-Dimensional Data Through Correlation Explanation.pdf:pdf},
pages = {577--585},
title = {{Discovering Structure in High-Dimensional Data Through Correlation Explanation}},
year = {2014}
}
@inproceedings{Bo2013,
address = {Portland, OR, USA},
author = {Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2013.91},
file = {:home/tom/Documents/Mendeley/Bo, Ren, Fox_2013_Multipath Sparse Coding Using Hierarchical Matching Pursuit.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {1063-6919},
pages = {660--667},
title = {{Multipath Sparse Coding Using Hierarchical Matching Pursuit}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6618935},
year = {2013}
}
