<!DOCTYPE html>
<html>
<head>
<title>Loky</title>
<meta charset="utf-8">
<link rel="stylesheet" type="text/css" href="../remark/slides.css">
<style>
    .left-column {
       width: 48%;
       float: left;
    }
    .reset-column {
       overflow: auto;
       width: 100%;
    }
    .right-column {
        width: 48%;
        float: right;
    }
    .footnote {
        position: absolute;
        bottom: 2em;
        margin: 0em 2em;
    }
    .grey {
        color: #bbbbbb;
    }
    .table img{
       width: 2em;
    }
     .state{
         color: #090;
        background-color: #f8f8f8;
     }

    .affiliations img {
        height: 60px;
        margin: 1em;
    }
</style>
</head>
  <body>
    <textarea id="source">
class: center, middle

# Sample slides

**Thomas Moreau** - Olivier Grisel

.affiliations[
  ![CMLA](images/logo_cmla.png)
  ![ENS Paris-Saclay](images/logo_ens.png)
  ![Heuritech](images/logo heuritech v2.png)
]

---
# Outline

<br/>


### Approximation

--

### Optimization

--

### Estimation


---
# 2 columns

.left-column[
Column1
Test
]

.right-column[.small[
Column2
Test
]]

.reset-column[
]
---
# Approximation with ReLU nets

.left-column[
```python

def relu(x):
    return np.maximum(x, 0)


def rect(x, a, b, h, eps=1e-7):
    return h / eps * (
           relu(x - a)
         - relu(x - (a + eps))
         - relu(x - b)
         + relu(x - (b + eps)))


x = np.linspace(-3, 3, 1000)
y = rect(x, 0, 1, 1.3)

plt.plot(x, y)
```
]

.right-column[
<img src="images/one-rectangle.svg" width="80%" />
]


.footnote.small[
[Quora: Is a single layered ReLU network still a universal
approximator?](https://www.quora.com/Is-a-single-layered-ReLu-network-still-a-universal-approximator),
Conner Davis]

---
# Approximation with ReLU nets

.left-column[
```python
import numpy as np
import matplotlib.pyplot as plt


def relu(x):
    return np.maximum(x, 0)


def rect(x, a, b, h, eps=1e-7):
    return h / eps * (
           relu(x - a)
         - relu(x - (a + eps))
         - relu(x - b)
         + relu(x - (b + eps)))


x = np.linspace(-3, 3, 1000)
*y = (  rect(x, -1, 0, 0.4)
*    + rect(x,  0, 1, 1.3)
*    + rect(x,  1, 2, 0.8))

plt.plot(x, y)
```
]

.right-column[
<img src="images/three-rectangles.svg" width="80%" />
]

.footnote.small[
[Quora: Is a single layered ReLU network still a universal
approximator?](https://www.quora.com/Is-a-single-layered-ReLu-network-still-a-universal-approximator),
Conner Davis
]

???
# Note in this slide for presenter mode
This is markdown

</textarea>
<style TYPE="text/css">
      code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>
<script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
      }
      });
      MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i = 0; i < all.length; i += 1) {
		     all[i].SourceElement().parentNode.className += ' has-jax';
		     }
		     });
</script>
<!--<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
<script type="text/javascript" src="/usr/share/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../remark/remark.min.js" type="text/javascript"> </script>
<script type="text/javascript">
      var slideshow = remark.create({
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
        //ratio: "4:3"
      });
</script>
</body>
</html>
